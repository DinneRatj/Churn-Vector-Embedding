{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a122163210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================================\n",
    "# [26 Mei 2020]:\n",
    "# loss function diganti dengan menggunakan implemenentasi Large Margin Cosine (LMC) Loss \n",
    "# di repo YirongMao berikut: https://github.com/YirongMao/softmax_variants\n",
    "#\n",
    "# LMC Loss adalah loss function yang secara teori akan diimplementasikan sejak awal. Paper LMC\n",
    "# Loss: https://arxiv.org/abs/1801.09414\n",
    "# \n",
    "# Cosine Embedding Loss yang sempat digunakan ternyata tidak benar-benar sama matematikanya\n",
    "# dengan LMC Loss, karena itu implementasinya diganti dengan implementasi dari YirongMao\n",
    "# =============================================================================================\n",
    "\n",
    "#Imporing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ==============================================================================================================\n",
    "# [26 Mei 2020]: ditambahkan agar angka random selalu sama ketika program dijalankan, sehingga hasil selalu sama\n",
    "# ==============================================================================================================\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "# =============================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account length</th>\n",
       "      <th>area code</th>\n",
       "      <th>phone number</th>\n",
       "      <th>international plan</th>\n",
       "      <th>voice mail plan</th>\n",
       "      <th>number vmail messages</th>\n",
       "      <th>total day minutes</th>\n",
       "      <th>total day calls</th>\n",
       "      <th>total day charge</th>\n",
       "      <th>total eve minutes</th>\n",
       "      <th>total eve calls</th>\n",
       "      <th>total eve charge</th>\n",
       "      <th>total night minutes</th>\n",
       "      <th>total night calls</th>\n",
       "      <th>total night charge</th>\n",
       "      <th>total intl minutes</th>\n",
       "      <th>total intl calls</th>\n",
       "      <th>total intl charge</th>\n",
       "      <th>customer service calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>382-4657</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>371-7191</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>358-1921</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>375-9999</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>330-6626</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3328</td>\n",
       "      <td>AZ</td>\n",
       "      <td>192</td>\n",
       "      <td>415</td>\n",
       "      <td>414-4276</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>36</td>\n",
       "      <td>156.2</td>\n",
       "      <td>77</td>\n",
       "      <td>26.55</td>\n",
       "      <td>215.5</td>\n",
       "      <td>126</td>\n",
       "      <td>18.32</td>\n",
       "      <td>279.1</td>\n",
       "      <td>83</td>\n",
       "      <td>12.56</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3329</td>\n",
       "      <td>WV</td>\n",
       "      <td>68</td>\n",
       "      <td>415</td>\n",
       "      <td>370-3271</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>231.1</td>\n",
       "      <td>57</td>\n",
       "      <td>39.29</td>\n",
       "      <td>153.4</td>\n",
       "      <td>55</td>\n",
       "      <td>13.04</td>\n",
       "      <td>191.3</td>\n",
       "      <td>123</td>\n",
       "      <td>8.61</td>\n",
       "      <td>9.6</td>\n",
       "      <td>4</td>\n",
       "      <td>2.59</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>RI</td>\n",
       "      <td>28</td>\n",
       "      <td>510</td>\n",
       "      <td>328-8230</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>180.8</td>\n",
       "      <td>109</td>\n",
       "      <td>30.74</td>\n",
       "      <td>288.8</td>\n",
       "      <td>58</td>\n",
       "      <td>24.55</td>\n",
       "      <td>191.9</td>\n",
       "      <td>91</td>\n",
       "      <td>8.64</td>\n",
       "      <td>14.1</td>\n",
       "      <td>6</td>\n",
       "      <td>3.81</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3331</td>\n",
       "      <td>CT</td>\n",
       "      <td>184</td>\n",
       "      <td>510</td>\n",
       "      <td>364-6381</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>213.8</td>\n",
       "      <td>105</td>\n",
       "      <td>36.35</td>\n",
       "      <td>159.6</td>\n",
       "      <td>84</td>\n",
       "      <td>13.57</td>\n",
       "      <td>139.2</td>\n",
       "      <td>137</td>\n",
       "      <td>6.26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3332</td>\n",
       "      <td>TN</td>\n",
       "      <td>74</td>\n",
       "      <td>415</td>\n",
       "      <td>400-4344</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>25</td>\n",
       "      <td>234.4</td>\n",
       "      <td>113</td>\n",
       "      <td>39.85</td>\n",
       "      <td>265.9</td>\n",
       "      <td>82</td>\n",
       "      <td>22.60</td>\n",
       "      <td>241.4</td>\n",
       "      <td>77</td>\n",
       "      <td>10.86</td>\n",
       "      <td>13.7</td>\n",
       "      <td>4</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3333 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     state  account length  area code phone number international plan  \\\n",
       "0       KS             128        415     382-4657                 no   \n",
       "1       OH             107        415     371-7191                 no   \n",
       "2       NJ             137        415     358-1921                 no   \n",
       "3       OH              84        408     375-9999                yes   \n",
       "4       OK              75        415     330-6626                yes   \n",
       "...    ...             ...        ...          ...                ...   \n",
       "3328    AZ             192        415     414-4276                 no   \n",
       "3329    WV              68        415     370-3271                 no   \n",
       "3330    RI              28        510     328-8230                 no   \n",
       "3331    CT             184        510     364-6381                yes   \n",
       "3332    TN              74        415     400-4344                 no   \n",
       "\n",
       "     voice mail plan  number vmail messages  total day minutes  \\\n",
       "0                yes                     25              265.1   \n",
       "1                yes                     26              161.6   \n",
       "2                 no                      0              243.4   \n",
       "3                 no                      0              299.4   \n",
       "4                 no                      0              166.7   \n",
       "...              ...                    ...                ...   \n",
       "3328             yes                     36              156.2   \n",
       "3329              no                      0              231.1   \n",
       "3330              no                      0              180.8   \n",
       "3331              no                      0              213.8   \n",
       "3332             yes                     25              234.4   \n",
       "\n",
       "      total day calls  total day charge  total eve minutes  total eve calls  \\\n",
       "0                 110             45.07              197.4               99   \n",
       "1                 123             27.47              195.5              103   \n",
       "2                 114             41.38              121.2              110   \n",
       "3                  71             50.90               61.9               88   \n",
       "4                 113             28.34              148.3              122   \n",
       "...               ...               ...                ...              ...   \n",
       "3328               77             26.55              215.5              126   \n",
       "3329               57             39.29              153.4               55   \n",
       "3330              109             30.74              288.8               58   \n",
       "3331              105             36.35              159.6               84   \n",
       "3332              113             39.85              265.9               82   \n",
       "\n",
       "      total eve charge  total night minutes  total night calls  \\\n",
       "0                16.78                244.7                 91   \n",
       "1                16.62                254.4                103   \n",
       "2                10.30                162.6                104   \n",
       "3                 5.26                196.9                 89   \n",
       "4                12.61                186.9                121   \n",
       "...                ...                  ...                ...   \n",
       "3328             18.32                279.1                 83   \n",
       "3329             13.04                191.3                123   \n",
       "3330             24.55                191.9                 91   \n",
       "3331             13.57                139.2                137   \n",
       "3332             22.60                241.4                 77   \n",
       "\n",
       "      total night charge  total intl minutes  total intl calls  \\\n",
       "0                  11.01                10.0                 3   \n",
       "1                  11.45                13.7                 3   \n",
       "2                   7.32                12.2                 5   \n",
       "3                   8.86                 6.6                 7   \n",
       "4                   8.41                10.1                 3   \n",
       "...                  ...                 ...               ...   \n",
       "3328               12.56                 9.9                 6   \n",
       "3329                8.61                 9.6                 4   \n",
       "3330                8.64                14.1                 6   \n",
       "3331                6.26                 5.0                10   \n",
       "3332               10.86                13.7                 4   \n",
       "\n",
       "      total intl charge  customer service calls  churn  \n",
       "0                  2.70                       1  False  \n",
       "1                  3.70                       1  False  \n",
       "2                  3.29                       0  False  \n",
       "3                  1.78                       2  False  \n",
       "4                  2.73                       3  False  \n",
       "...                 ...                     ...    ...  \n",
       "3328               2.67                       2  False  \n",
       "3329               2.59                       3  False  \n",
       "3330               3.81                       2  False  \n",
       "3331               1.35                       2  False  \n",
       "3332               3.70                       0  False  \n",
       "\n",
       "[3333 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading data\n",
    "learn_data = pd.read_csv(\"bigml_59c28831336c6604c800002a.csv\")\n",
    "pd.options.display.max_columns = None\n",
    "learn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining columns\n",
    "numerical_columns = ['area code', 'number vmail messages', 'total day minutes', 'total day calls',\n",
    "                     'total day charge', 'total eve minutes', 'total eve calls', 'total eve charge', 'total night minutes',\n",
    "                     'total night calls', 'total night charge', 'total intl minutes', 'total intl calls',\n",
    "                     'total intl charge', 'customer service calls']\n",
    "categorical_columns = ['state', 'international plan', 'voice mail plan']\n",
    "outputs = ['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing columns\n",
    "\n",
    "#Numerical\n",
    "numerical_data = np.stack([learn_data[col].values for col in numerical_columns], 1)\n",
    "numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
    "\n",
    "#Categorical\n",
    "for category in categorical_columns:\n",
    "    learn_data[category] = learn_data[category].astype('category')\n",
    "    \n",
    "st = learn_data['state'].cat.codes.values\n",
    "ip = learn_data['international plan'].cat.codes.values\n",
    "vm = learn_data['voice mail plan'].cat.codes.values\n",
    "\n",
    "categorical_data = np.stack([st, ip, vm], 1)\n",
    "categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
    "\n",
    "#Outputs\n",
    "learn_data[outputs] = learn_data[outputs].astype(int)\n",
    "\n",
    "outputs = torch.tensor(learn_data[outputs].values).flatten()\n",
    "outputs = outputs.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the data\n",
    "total_records = 3333\n",
    "train_records = int(total_records * .6)\n",
    "valid_records = int(total_records * .2)\n",
    "test_records = int(total_records * .2)\n",
    "numerical_train_data = numerical_data[:train_records]\n",
    "numerical_valid_data = numerical_data[train_records:train_records+valid_records]\n",
    "numerical_test_data = numerical_data[train_records+valid_records:total_records]\n",
    "categorical_train_data = categorical_data[:train_records]\n",
    "categorical_valid_data = categorical_data[train_records:train_records+valid_records]\n",
    "categorical_test_data = categorical_data[train_records+valid_records:total_records]\n",
    "train_outputs = outputs[:train_records]\n",
    "valid_outputs = outputs[train_records:train_records+valid_records]\n",
    "test_outputs = outputs[train_records+valid_records:total_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "import math\n",
    "\n",
    "#Creating the Neural Network\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(15, 100)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # =============================================================================================\n",
    "        # [26 Mei 2020]:\n",
    "        # weights1 dan 2 tidak ada di model ini, karena sudah ada sebagai atribut self.centers di  \n",
    "        # module LMC_Loss di model_utils.py.\n",
    "        # =============================================================================================\n",
    "        \n",
    "        self.layer1_1 = nn.Embedding(51, 5)\n",
    "        self.layer1_2 = nn.Embedding(2, 5)\n",
    "        self.layer1_3 = nn.Embedding(2, 5)\n",
    "        \n",
    "    def forward(self, x_numerical, x_categorical):\n",
    "        x1 = self.layer1(x_numerical)\n",
    "        \n",
    "        x1_embedding = self.layer1_1(x_categorical[:,0])\n",
    "        x2_embedding = self.layer1_2(x_categorical[:,1])\n",
    "        x3_embedding = self.layer1_3(x_categorical[:,2])\n",
    "        x_embedding = torch.cat([x1_embedding,x2_embedding,x3_embedding], 1)\n",
    "        \n",
    "        x1_1 = torch.cat([x1, x_embedding], 1)\n",
    "        \n",
    "        x1_act = self.act1(x1_1)\n",
    "        \n",
    "        return x1_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================================\n",
    "# [26 Mei 2020]: \n",
    "# menambahkan loss baru dengan library LMCLoss dari YirongMao. Cross entropy loss tetap ada, \n",
    "# karena implementasi LMCLoss YirongMao masih memanfaatkan Cross Entropy Loss asli Pytorch.\n",
    "# Cek cara implementasi di: https://github.com/YirongMao/softmax_variants/blob/master/train_mnist_LMCL.py\n",
    "#\n",
    "# margin (parameter m) menggunakan nilai 0.35, sesuai hasil penelitian dari paper asli LMCLoss\n",
    "# yang menemukan bahwa nilai optimalnya di 0.35 atau 4.\n",
    "# =============================================================================================\n",
    "\n",
    "import model_utils\n",
    "\n",
    "lmcl_loss = model_utils.LMCL_loss(num_classes=2, feat_dim=115, m=0.35)\n",
    "optimizer_lmcl = torch.optim.Adam(lmcl_loss.parameters(), lr=0.001)\n",
    "\n",
    "# =============================================================================================\n",
    "# [26 Mei 2020]: \n",
    "# cross entropy loss ditambahkan parameter weight, karena ternyata data imbalance (data not churn\n",
    "# jauh lebih banyak dari churn, dengan rasio sekitar 85:15). Data imbalance ini menyebabkan hasil \n",
    "# prediksi untuk kelas churn tidak baik. Solusinya adalah dengan menggunakan parameter weight di\n",
    "# cross entropy loss, sehingga penalty untuk salah di kelas churn jauh lebih besar daripada salah\n",
    "# di kelas not churn. Penalty diperbesar dengan rasio kebalikan dari rasio jumlah datanya (15:85)\n",
    "# =============================================================================================\n",
    "churn_percentage = 0.85\n",
    "#Defining loss function\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.Tensor([1-churn_percentage, churn_percentage]))\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:   0 loss: 2.60523057\n",
      "iteration:   1 loss: 2.47060966\n",
      "iteration:   2 loss: 2.52472544\n",
      "iteration:   3 loss: 2.48039532\n",
      "iteration:   4 loss: 2.52776027\n",
      "iteration:   5 loss: 2.70972228\n",
      "iteration:   6 loss: 2.52897978\n",
      "iteration:   7 loss: 2.64129210\n",
      "iteration:   8 loss: 2.55790305\n",
      "iteration:   9 loss: 2.61512589\n",
      "iteration:  10 loss: 2.32751536\n",
      "iteration:  11 loss: 2.57736921\n",
      "iteration:  12 loss: 2.56820083\n",
      "iteration:  13 loss: 2.33318591\n",
      "iteration:  14 loss: 2.49997926\n",
      "iteration:  15 loss: 2.54798794\n",
      "iteration:  16 loss: 2.26611829\n",
      "iteration:  17 loss: 2.27375126\n",
      "iteration:  18 loss: 2.44672489\n",
      "iteration:  19 loss: 2.68886924\n",
      "iteration:  20 loss: 2.17464137\n",
      "iteration:  21 loss: 2.53202033\n",
      "iteration:  22 loss: 2.12732148\n",
      "iteration:  23 loss: 2.70758080\n",
      "iteration:  24 loss: 2.56390667\n",
      "iteration:  25 loss: 2.71662116\n",
      "iteration:  26 loss: 1.93505013\n",
      "iteration:  27 loss: 2.55271220\n",
      "iteration:  28 loss: 2.35930777\n",
      "iteration:  29 loss: 2.51436520\n",
      "iteration:  30 loss: 2.89528012\n",
      "iteration:  31 loss: 2.76849484\n",
      "iteration:  32 loss: 1.88754570\n",
      "iteration:  33 loss: 2.68808150\n",
      "iteration:  34 loss: 2.67145443\n",
      "iteration:  35 loss: 2.35685563\n",
      "iteration:  36 loss: 2.66980052\n",
      "iteration:  37 loss: 2.66267157\n",
      "iteration:  38 loss: 1.82664168\n",
      "iteration:  39 loss: 2.85407448\n",
      "iteration:  40 loss: 2.34932327\n",
      "iteration:  41 loss: 2.67286491\n",
      "iteration:  42 loss: 1.64555120\n",
      "iteration:  43 loss: 2.74793506\n",
      "iteration:  44 loss: 1.67880738\n",
      "iteration:  45 loss: 2.73574185\n",
      "iteration:  46 loss: 2.82879972\n",
      "iteration:  47 loss: 2.17096376\n",
      "iteration:  48 loss: 1.65431380\n",
      "iteration:  49 loss: 2.95053029\n",
      "iteration:  50 loss: 2.90631747\n",
      "iteration:  51 loss: 2.41345477\n",
      "iteration:  52 loss: 2.30211759\n",
      "iteration:  53 loss: 1.52865434\n",
      "iteration:  54 loss: 2.97261977\n",
      "iteration:  55 loss: 2.97246337\n",
      "iteration:  56 loss: 2.35356736\n",
      "iteration:  57 loss: 2.32057881\n",
      "iteration:  58 loss: 2.81111670\n",
      "iteration:  59 loss: 1.47808623\n",
      "iteration:  60 loss: 2.82521105\n",
      "iteration:  61 loss: 2.78796196\n",
      "iteration:  62 loss: 2.81566668\n",
      "iteration:  63 loss: 1.48104739\n",
      "iteration:  64 loss: 2.29000425\n",
      "iteration:  65 loss: 2.25559354\n",
      "iteration:  66 loss: 2.16734743\n",
      "iteration:  67 loss: 2.17928839\n",
      "iteration:  68 loss: 1.46739435\n",
      "iteration:  69 loss: 1.51146817\n",
      "iteration:  70 loss: 1.40598154\n",
      "iteration:  71 loss: 2.30846262\n",
      "iteration:  72 loss: 2.30223465\n",
      "iteration:  73 loss: 3.18078256\n",
      "iteration:  74 loss: 1.37691426\n",
      "iteration:  75 loss: 1.29279089\n",
      "iteration:  76 loss: 2.29200459\n",
      "iteration:  77 loss: 2.95558739\n",
      "iteration:  78 loss: 2.30871701\n",
      "iteration:  79 loss: 2.88778067\n",
      "iteration:  80 loss: 1.24121249\n",
      "iteration:  81 loss: 2.35183811\n",
      "iteration:  82 loss: 1.10224509\n",
      "iteration:  83 loss: 3.06273508\n",
      "iteration:  84 loss: 2.32320428\n",
      "iteration:  85 loss: 2.86347675\n",
      "iteration:  86 loss: 1.02636814\n",
      "iteration:  87 loss: 2.38403368\n",
      "iteration:  88 loss: 1.06409585\n",
      "iteration:  89 loss: 2.92402005\n",
      "iteration:  90 loss: 3.48257589\n",
      "iteration:  91 loss: 2.87846470\n",
      "iteration:  92 loss: 0.91920799\n",
      "iteration:  93 loss: 2.33608174\n",
      "iteration:  94 loss: 2.98155713\n",
      "iteration:  95 loss: 0.96178126\n",
      "iteration:  96 loss: 2.96425009\n",
      "iteration:  97 loss: 3.48444891\n",
      "iteration:  98 loss: 2.90736914\n",
      "iteration:  99 loss: 0.93212515\n",
      "iteration: 100 loss: 2.29303789\n",
      "iteration: 101 loss: 2.29252100\n",
      "iteration: 102 loss: 2.30695915\n",
      "iteration: 103 loss: 3.08732724\n",
      "iteration: 104 loss: 0.94017261\n",
      "iteration: 105 loss: 0.90052134\n",
      "iteration: 106 loss: 0.87800521\n",
      "iteration: 107 loss: 3.10363007\n",
      "iteration: 108 loss: 0.93362474\n",
      "iteration: 109 loss: 0.89659303\n",
      "iteration: 110 loss: 3.11110806\n",
      "iteration: 111 loss: 2.36205626\n",
      "iteration: 112 loss: 3.08682680\n",
      "iteration: 113 loss: 3.13463306\n",
      "iteration: 114 loss: 2.18258882\n",
      "iteration: 115 loss: 3.12504005\n",
      "iteration: 116 loss: 0.83137846\n",
      "iteration: 117 loss: 0.83043313\n",
      "iteration: 118 loss: 2.33211231\n",
      "iteration: 119 loss: 2.99532771\n",
      "iteration: 120 loss: 2.22868323\n",
      "iteration: 121 loss: 0.82233799\n",
      "iteration: 122 loss: 0.81980759\n",
      "iteration: 123 loss: 3.07060242\n",
      "iteration: 124 loss: 2.36533213\n",
      "iteration: 125 loss: 2.34931111\n",
      "iteration: 126 loss: 3.20942450\n",
      "iteration: 127 loss: 3.52223444\n",
      "iteration: 128 loss: 3.02473474\n",
      "iteration: 129 loss: 2.32779813\n",
      "iteration: 130 loss: 3.06944776\n",
      "iteration: 131 loss: 2.32798409\n",
      "iteration: 132 loss: 3.65149879\n",
      "iteration: 133 loss: 3.57731342\n",
      "iteration: 134 loss: 3.44734120\n",
      "iteration: 135 loss: 3.04236674\n",
      "iteration: 136 loss: 2.25239635\n",
      "iteration: 137 loss: 3.00302005\n",
      "iteration: 138 loss: 0.90532821\n",
      "iteration: 139 loss: 2.29335666\n",
      "iteration: 140 loss: 3.02478027\n",
      "iteration: 141 loss: 0.97338027\n",
      "iteration: 142 loss: 2.32490230\n",
      "iteration: 143 loss: 2.25898361\n",
      "iteration: 144 loss: 2.26844144\n",
      "iteration: 145 loss: 2.32237124\n",
      "iteration: 146 loss: 2.32718611\n",
      "iteration: 147 loss: 2.20830560\n",
      "iteration: 148 loss: 2.30546069\n",
      "iteration: 149 loss: 2.43375611\n",
      "iteration: 150 loss: 0.99258226\n",
      "iteration: 151 loss: 2.24934816\n",
      "iteration: 152 loss: 2.32258296\n",
      "iteration: 153 loss: 4.20070601\n",
      "iteration: 154 loss: 0.97966534\n",
      "iteration: 155 loss: 2.22042418\n",
      "iteration: 156 loss: 0.96069121\n",
      "iteration: 157 loss: 0.94679159\n",
      "iteration: 158 loss: 2.29718900\n",
      "iteration: 159 loss: 2.18351769\n",
      "iteration: 160 loss: 2.16117311\n",
      "iteration: 161 loss: 2.97690678\n",
      "iteration: 162 loss: 0.92598027\n",
      "iteration: 163 loss: 3.01032877\n",
      "iteration: 164 loss: 2.33506608\n",
      "iteration: 165 loss: 3.08265352\n",
      "iteration: 166 loss: 2.26367784\n",
      "iteration: 167 loss: 3.00917578\n",
      "iteration: 168 loss: 0.91430265\n",
      "iteration: 169 loss: 3.38131762\n",
      "iteration: 170 loss: 3.61294436\n",
      "iteration: 171 loss: 3.03078222\n",
      "iteration: 172 loss: 0.93864918\n",
      "iteration: 173 loss: 3.36319232\n",
      "iteration: 174 loss: 2.30589747\n",
      "iteration: 175 loss: 3.42481327\n",
      "iteration: 176 loss: 3.33348131\n",
      "iteration: 177 loss: 2.20327377\n",
      "iteration: 178 loss: 2.21893334\n",
      "iteration: 179 loss: 2.25040793\n",
      "iteration: 180 loss: 2.37291121\n",
      "iteration: 181 loss: 1.01888716\n",
      "iteration: 182 loss: 1.05677140\n",
      "iteration: 183 loss: 2.11557531\n",
      "iteration: 184 loss: 3.54785681\n",
      "iteration: 185 loss: 3.35616684\n",
      "iteration: 186 loss: 3.75218439\n",
      "iteration: 187 loss: 2.26246953\n",
      "iteration: 188 loss: 3.67173290\n",
      "iteration: 189 loss: 3.26020527\n",
      "iteration: 190 loss: 3.06414580\n",
      "iteration: 191 loss: 2.89120722\n",
      "iteration: 192 loss: 1.19469154\n",
      "iteration: 193 loss: 3.36656237\n",
      "iteration: 194 loss: 1.29347241\n",
      "iteration: 195 loss: 2.85709357\n",
      "iteration: 196 loss: 2.35941267\n",
      "iteration: 197 loss: 3.19087815\n",
      "iteration: 198 loss: 2.26624084\n",
      "iteration: 199 loss: 1.34418535\n",
      "epoch:   1 mean loss training: 2.35576963\n",
      "epoch:   1 mean loss validation: 2.36369133\n",
      "iteration:   0 loss: 1.31439090\n",
      "iteration:   1 loss: 2.72207284\n",
      "iteration:   2 loss: 2.32874465\n",
      "iteration:   3 loss: 2.20407844\n",
      "iteration:   4 loss: 2.82734728\n",
      "iteration:   5 loss: 3.01355147\n",
      "iteration:   6 loss: 2.36437821\n",
      "iteration:   7 loss: 2.91734529\n",
      "iteration:   8 loss: 3.06581831\n",
      "iteration:   9 loss: 3.11530495\n",
      "iteration:  10 loss: 1.28288198\n",
      "iteration:  11 loss: 2.79808068\n",
      "iteration:  12 loss: 2.92470789\n",
      "iteration:  13 loss: 1.35471392\n",
      "iteration:  14 loss: 2.36088824\n",
      "iteration:  15 loss: 2.29033041\n",
      "iteration:  16 loss: 1.34206164\n",
      "iteration:  17 loss: 1.39160252\n",
      "iteration:  18 loss: 2.39539933\n",
      "iteration:  19 loss: 2.83452845\n",
      "iteration:  20 loss: 1.34200478\n",
      "iteration:  21 loss: 2.72750807\n",
      "iteration:  22 loss: 1.31598580\n",
      "iteration:  23 loss: 2.87856364\n",
      "iteration:  24 loss: 2.79425049\n",
      "iteration:  25 loss: 2.99424124\n",
      "iteration:  26 loss: 1.19432569\n",
      "iteration:  27 loss: 2.48416948\n",
      "iteration:  28 loss: 2.22199321\n",
      "iteration:  29 loss: 2.43716550\n",
      "iteration:  30 loss: 3.30650663\n",
      "iteration:  31 loss: 2.91350722\n",
      "iteration:  32 loss: 1.19087088\n",
      "iteration:  33 loss: 2.89781380\n",
      "iteration:  34 loss: 2.86809659\n",
      "iteration:  35 loss: 2.22405219\n",
      "iteration:  36 loss: 2.70788074\n",
      "iteration:  37 loss: 2.84476995\n",
      "iteration:  38 loss: 1.24899089\n",
      "iteration:  39 loss: 3.16573358\n",
      "iteration:  40 loss: 2.30161738\n",
      "iteration:  41 loss: 2.84206796\n",
      "iteration:  42 loss: 1.11970508\n",
      "iteration:  43 loss: 2.85841298\n",
      "iteration:  44 loss: 1.16540694\n",
      "iteration:  45 loss: 2.81531811\n",
      "iteration:  46 loss: 2.97737503\n",
      "iteration:  47 loss: 2.17517567\n",
      "iteration:  48 loss: 1.20441043\n",
      "iteration:  49 loss: 3.21292663\n",
      "iteration:  50 loss: 3.18350101\n",
      "iteration:  51 loss: 2.30314207\n",
      "iteration:  52 loss: 2.29463530\n",
      "iteration:  53 loss: 1.11297071\n",
      "iteration:  54 loss: 3.19425082\n",
      "iteration:  55 loss: 3.17281389\n",
      "iteration:  56 loss: 2.30700231\n",
      "iteration:  57 loss: 2.24187684\n",
      "iteration:  58 loss: 3.05134964\n",
      "iteration:  59 loss: 1.12698519\n",
      "iteration:  60 loss: 2.84448552\n",
      "iteration:  61 loss: 2.81897044\n",
      "iteration:  62 loss: 2.87092304\n",
      "iteration:  63 loss: 1.13711357\n",
      "iteration:  64 loss: 2.18505502\n",
      "iteration:  65 loss: 2.26421785\n",
      "iteration:  66 loss: 2.10616183\n",
      "iteration:  67 loss: 2.14590883\n",
      "iteration:  68 loss: 1.20640945\n",
      "iteration:  69 loss: 1.24067008\n",
      "iteration:  70 loss: 1.12170422\n",
      "iteration:  71 loss: 2.25229955\n",
      "iteration:  72 loss: 2.37163639\n",
      "iteration:  73 loss: 3.30571866\n",
      "iteration:  74 loss: 1.14057696\n",
      "iteration:  75 loss: 1.06739223\n",
      "iteration:  76 loss: 2.23178434\n",
      "iteration:  77 loss: 3.06963396\n",
      "iteration:  78 loss: 2.24969888\n",
      "iteration:  79 loss: 2.95912075\n",
      "iteration:  80 loss: 1.07486546\n",
      "iteration:  81 loss: 2.22806501\n",
      "iteration:  82 loss: 0.93011600\n",
      "iteration:  83 loss: 3.04283476\n",
      "iteration:  84 loss: 2.32539201\n",
      "iteration:  85 loss: 2.85074878\n",
      "iteration:  86 loss: 0.89153093\n",
      "iteration:  87 loss: 2.33645082\n",
      "iteration:  88 loss: 0.92887551\n",
      "iteration:  89 loss: 3.02952242\n",
      "iteration:  90 loss: 3.49173188\n",
      "iteration:  91 loss: 2.84740615\n",
      "iteration:  92 loss: 0.80388695\n",
      "iteration:  93 loss: 2.36516857\n",
      "iteration:  94 loss: 2.99895883\n",
      "iteration:  95 loss: 0.84710300\n",
      "iteration:  96 loss: 3.08973956\n",
      "iteration:  97 loss: 3.47485304\n",
      "iteration:  98 loss: 2.80718493\n",
      "iteration:  99 loss: 0.84473002\n",
      "iteration: 100 loss: 2.20199895\n",
      "iteration: 101 loss: 2.22144914\n",
      "iteration: 102 loss: 2.29839206\n",
      "iteration: 103 loss: 3.13189816\n",
      "iteration: 104 loss: 0.87396979\n",
      "iteration: 105 loss: 0.83292508\n",
      "iteration: 106 loss: 0.80842739\n",
      "iteration: 107 loss: 3.04374576\n",
      "iteration: 108 loss: 0.91739112\n",
      "iteration: 109 loss: 0.87088168\n",
      "iteration: 110 loss: 3.16425371\n",
      "iteration: 111 loss: 2.36714029\n",
      "iteration: 112 loss: 3.09173059\n",
      "iteration: 113 loss: 3.10975981\n",
      "iteration: 114 loss: 2.24290085\n",
      "iteration: 115 loss: 3.15070009\n",
      "iteration: 116 loss: 0.78635180\n",
      "iteration: 117 loss: 0.80051577\n",
      "iteration: 118 loss: 2.25144482\n",
      "iteration: 119 loss: 2.98268461\n",
      "iteration: 120 loss: 2.17090917\n",
      "iteration: 121 loss: 0.80815911\n",
      "iteration: 122 loss: 0.76472414\n",
      "iteration: 123 loss: 2.99300170\n",
      "iteration: 124 loss: 2.40062189\n",
      "iteration: 125 loss: 2.37475753\n",
      "iteration: 126 loss: 3.25423121\n",
      "iteration: 127 loss: 3.48571253\n",
      "iteration: 128 loss: 3.01022530\n",
      "iteration: 129 loss: 2.25986671\n",
      "iteration: 130 loss: 2.98576093\n",
      "iteration: 131 loss: 2.40807581\n",
      "iteration: 132 loss: 3.72696257\n",
      "iteration: 133 loss: 3.51751876\n",
      "iteration: 134 loss: 3.47008705\n",
      "iteration: 135 loss: 2.93801093\n",
      "iteration: 136 loss: 2.18932700\n",
      "iteration: 137 loss: 2.97559142\n",
      "iteration: 138 loss: 0.86863428\n",
      "iteration: 139 loss: 2.19120955\n",
      "iteration: 140 loss: 3.10277009\n",
      "iteration: 141 loss: 0.96030968\n",
      "iteration: 142 loss: 2.30565953\n",
      "iteration: 143 loss: 2.33449030\n",
      "iteration: 144 loss: 2.28515363\n",
      "iteration: 145 loss: 2.31931567\n",
      "iteration: 146 loss: 2.31334281\n",
      "iteration: 147 loss: 2.11586690\n",
      "iteration: 148 loss: 2.32384992\n",
      "iteration: 149 loss: 2.47522545\n",
      "iteration: 150 loss: 1.05089915\n",
      "iteration: 151 loss: 2.23367047\n",
      "iteration: 152 loss: 2.25793815\n",
      "iteration: 153 loss: 4.19261169\n",
      "iteration: 154 loss: 0.99452615\n",
      "iteration: 155 loss: 2.21824813\n",
      "iteration: 156 loss: 0.96162152\n",
      "iteration: 157 loss: 0.96448082\n",
      "iteration: 158 loss: 2.22739816\n",
      "iteration: 159 loss: 2.08731771\n",
      "iteration: 160 loss: 2.10773373\n",
      "iteration: 161 loss: 2.94122910\n",
      "iteration: 162 loss: 0.92444795\n",
      "iteration: 163 loss: 3.01873517\n",
      "iteration: 164 loss: 2.29332829\n",
      "iteration: 165 loss: 3.08315539\n",
      "iteration: 166 loss: 2.29156518\n",
      "iteration: 167 loss: 2.89582968\n",
      "iteration: 168 loss: 0.87018609\n",
      "iteration: 169 loss: 3.29936242\n",
      "iteration: 170 loss: 3.55382037\n",
      "iteration: 171 loss: 2.97442889\n",
      "iteration: 172 loss: 0.90056729\n",
      "iteration: 173 loss: 3.22371101\n",
      "iteration: 174 loss: 2.29429364\n",
      "iteration: 175 loss: 3.42171454\n",
      "iteration: 176 loss: 3.32502294\n",
      "iteration: 177 loss: 2.11729407\n",
      "iteration: 178 loss: 2.19960880\n",
      "iteration: 179 loss: 2.13920927\n",
      "iteration: 180 loss: 2.38963151\n",
      "iteration: 181 loss: 1.06225705\n",
      "iteration: 182 loss: 1.10532105\n",
      "iteration: 183 loss: 2.22614455\n",
      "iteration: 184 loss: 3.42886710\n",
      "iteration: 185 loss: 3.33007002\n",
      "iteration: 186 loss: 3.72871709\n",
      "iteration: 187 loss: 2.18201160\n",
      "iteration: 188 loss: 3.63743615\n",
      "iteration: 189 loss: 3.14893031\n",
      "iteration: 190 loss: 3.09321308\n",
      "iteration: 191 loss: 2.93778539\n",
      "iteration: 192 loss: 1.24284112\n",
      "iteration: 193 loss: 3.27919936\n",
      "iteration: 194 loss: 1.37694716\n",
      "iteration: 195 loss: 2.81927752\n",
      "iteration: 196 loss: 2.31222796\n",
      "iteration: 197 loss: 3.11402011\n",
      "iteration: 198 loss: 2.16299272\n",
      "iteration: 199 loss: 1.41577744\n",
      "epoch:   2 mean loss training: 2.30300879\n",
      "epoch:   2 mean loss validation: 2.33553243\n",
      "iteration:   0 loss: 1.42395091\n",
      "iteration:   1 loss: 2.61919904\n",
      "iteration:   2 loss: 2.37153125\n",
      "iteration:   3 loss: 2.12902880\n",
      "iteration:   4 loss: 2.80632710\n",
      "iteration:   5 loss: 3.03118038\n",
      "iteration:   6 loss: 2.40114784\n",
      "iteration:   7 loss: 2.92266440\n",
      "iteration:   8 loss: 2.99543929\n",
      "iteration:   9 loss: 3.02268195\n",
      "iteration:  10 loss: 1.32378852\n",
      "iteration:  11 loss: 2.72985864\n",
      "iteration:  12 loss: 3.00021386\n",
      "iteration:  13 loss: 1.40888524\n",
      "iteration:  14 loss: 2.38446307\n",
      "iteration:  15 loss: 2.19799256\n",
      "iteration:  16 loss: 1.37835550\n",
      "iteration:  17 loss: 1.46445394\n",
      "iteration:  18 loss: 2.50140619\n",
      "iteration:  19 loss: 2.72893572\n",
      "iteration:  20 loss: 1.36968565\n",
      "iteration:  21 loss: 2.67201638\n",
      "iteration:  22 loss: 1.33390725\n",
      "iteration:  23 loss: 2.82821488\n",
      "iteration:  24 loss: 2.77432752\n",
      "iteration:  25 loss: 3.05333233\n",
      "iteration:  26 loss: 1.17267823\n",
      "iteration:  27 loss: 2.52102733\n",
      "iteration:  28 loss: 2.16147041\n",
      "iteration:  29 loss: 2.45187163\n",
      "iteration:  30 loss: 3.20114303\n",
      "iteration:  31 loss: 2.85430217\n",
      "iteration:  32 loss: 1.12514043\n",
      "iteration:  33 loss: 2.88442349\n",
      "iteration:  34 loss: 2.85976267\n",
      "iteration:  35 loss: 2.14986658\n",
      "iteration:  36 loss: 2.54093099\n",
      "iteration:  37 loss: 2.82012177\n",
      "iteration:  38 loss: 1.23680174\n",
      "iteration:  39 loss: 3.12567663\n",
      "iteration:  40 loss: 2.32857609\n",
      "iteration:  41 loss: 2.82110476\n",
      "iteration:  42 loss: 1.08645618\n",
      "iteration:  43 loss: 2.80579138\n",
      "iteration:  44 loss: 1.12523770\n",
      "iteration:  45 loss: 2.72491717\n",
      "iteration:  46 loss: 2.98481488\n",
      "iteration:  47 loss: 2.24396992\n",
      "iteration:  48 loss: 1.19388092\n",
      "iteration:  49 loss: 3.19545484\n",
      "iteration:  50 loss: 3.17789459\n",
      "iteration:  51 loss: 2.25681138\n",
      "iteration:  52 loss: 2.33143163\n",
      "iteration:  53 loss: 1.08177125\n",
      "iteration:  54 loss: 3.16735506\n",
      "iteration:  55 loss: 3.11933827\n",
      "iteration:  56 loss: 2.29556870\n",
      "iteration:  57 loss: 2.19772530\n",
      "iteration:  58 loss: 3.03165221\n",
      "iteration:  59 loss: 1.11171031\n",
      "iteration:  60 loss: 2.74833822\n",
      "iteration:  61 loss: 2.71559596\n",
      "iteration:  62 loss: 2.79470563\n",
      "iteration:  63 loss: 1.10862744\n",
      "iteration:  64 loss: 2.10357714\n",
      "iteration:  65 loss: 2.29303646\n",
      "iteration:  66 loss: 2.03741097\n",
      "iteration:  67 loss: 2.09200048\n",
      "iteration:  68 loss: 1.21864641\n",
      "iteration:  69 loss: 1.25388849\n",
      "iteration:  70 loss: 1.08062470\n",
      "iteration:  71 loss: 2.22570801\n",
      "iteration:  72 loss: 2.45499158\n",
      "iteration:  73 loss: 3.28540993\n",
      "iteration:  74 loss: 1.11986363\n",
      "iteration:  75 loss: 1.03946888\n",
      "iteration:  76 loss: 2.19550061\n",
      "iteration:  77 loss: 3.13123250\n",
      "iteration:  78 loss: 2.18595457\n",
      "iteration:  79 loss: 2.96366096\n",
      "iteration:  80 loss: 1.07511079\n",
      "iteration:  81 loss: 2.11576128\n",
      "iteration:  82 loss: 0.87972665\n",
      "iteration:  83 loss: 2.98479342\n",
      "iteration:  84 loss: 2.32296348\n",
      "iteration:  85 loss: 2.80102396\n",
      "iteration:  86 loss: 0.85464358\n",
      "iteration:  87 loss: 2.30383182\n",
      "iteration:  88 loss: 0.89588195\n",
      "iteration:  89 loss: 3.08980727\n",
      "iteration:  90 loss: 3.44767976\n",
      "iteration:  91 loss: 2.77225876\n",
      "iteration:  92 loss: 0.74505955\n",
      "iteration:  93 loss: 2.40861273\n",
      "iteration:  94 loss: 2.99317741\n",
      "iteration:  95 loss: 0.79700673\n",
      "iteration:  96 loss: 3.16310644\n",
      "iteration:  97 loss: 3.44209385\n",
      "iteration:  98 loss: 2.69274974\n",
      "iteration:  99 loss: 0.79494387\n",
      "iteration: 100 loss: 2.12472177\n",
      "iteration: 101 loss: 2.16785645\n",
      "iteration: 102 loss: 2.28972483\n",
      "iteration: 103 loss: 3.17214108\n",
      "iteration: 104 loss: 0.83793259\n",
      "iteration: 105 loss: 0.78836703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 106 loss: 0.75848275\n",
      "iteration: 107 loss: 3.00261235\n",
      "iteration: 108 loss: 0.92329955\n",
      "iteration: 109 loss: 0.85958701\n",
      "iteration: 110 loss: 3.20596075\n",
      "iteration: 111 loss: 2.37515473\n",
      "iteration: 112 loss: 3.09356999\n",
      "iteration: 113 loss: 3.10028243\n",
      "iteration: 114 loss: 2.28431439\n",
      "iteration: 115 loss: 3.17316556\n",
      "iteration: 116 loss: 0.74771082\n",
      "iteration: 117 loss: 0.77953058\n",
      "iteration: 118 loss: 2.17716837\n",
      "iteration: 119 loss: 2.96915889\n",
      "iteration: 120 loss: 2.11236501\n",
      "iteration: 121 loss: 0.79881394\n",
      "iteration: 122 loss: 0.71733147\n",
      "iteration: 123 loss: 2.92310166\n",
      "iteration: 124 loss: 2.44257998\n",
      "iteration: 125 loss: 2.40062809\n",
      "iteration: 126 loss: 3.31511068\n",
      "iteration: 127 loss: 3.44221210\n",
      "iteration: 128 loss: 2.99205256\n",
      "iteration: 129 loss: 2.20118260\n",
      "iteration: 130 loss: 2.91410255\n",
      "iteration: 131 loss: 2.48948407\n",
      "iteration: 132 loss: 3.80709791\n",
      "iteration: 133 loss: 3.47607470\n",
      "iteration: 134 loss: 3.48671985\n",
      "iteration: 135 loss: 2.85806274\n",
      "iteration: 136 loss: 2.13574910\n",
      "iteration: 137 loss: 2.95894551\n",
      "iteration: 138 loss: 0.83583534\n",
      "iteration: 139 loss: 2.10887027\n",
      "iteration: 140 loss: 3.17462969\n",
      "iteration: 141 loss: 0.94096255\n",
      "iteration: 142 loss: 2.29646349\n",
      "iteration: 143 loss: 2.38264656\n",
      "iteration: 144 loss: 2.29920459\n",
      "iteration: 145 loss: 2.32158279\n",
      "iteration: 146 loss: 2.30502439\n",
      "iteration: 147 loss: 2.03256893\n",
      "iteration: 148 loss: 2.34010077\n",
      "iteration: 149 loss: 2.52041984\n",
      "iteration: 150 loss: 1.09281266\n",
      "iteration: 151 loss: 2.22160220\n",
      "iteration: 152 loss: 2.20459008\n",
      "iteration: 153 loss: 4.21096516\n",
      "iteration: 154 loss: 0.99826437\n",
      "iteration: 155 loss: 2.20279074\n",
      "iteration: 156 loss: 0.94529921\n",
      "iteration: 157 loss: 0.96125990\n",
      "iteration: 158 loss: 2.17886972\n",
      "iteration: 159 loss: 2.00298214\n",
      "iteration: 160 loss: 2.04788876\n",
      "iteration: 161 loss: 2.90989566\n",
      "iteration: 162 loss: 0.91747659\n",
      "iteration: 163 loss: 3.02889609\n",
      "iteration: 164 loss: 2.25770664\n",
      "iteration: 165 loss: 3.09259248\n",
      "iteration: 166 loss: 2.30945706\n",
      "iteration: 167 loss: 2.81096649\n",
      "iteration: 168 loss: 0.82731277\n",
      "iteration: 169 loss: 3.23729873\n",
      "iteration: 170 loss: 3.49450827\n",
      "iteration: 171 loss: 2.94518614\n",
      "iteration: 172 loss: 0.86240411\n",
      "iteration: 173 loss: 3.09525037\n",
      "iteration: 174 loss: 2.29489374\n",
      "iteration: 175 loss: 3.42489433\n",
      "iteration: 176 loss: 3.31450963\n",
      "iteration: 177 loss: 2.03748393\n",
      "iteration: 178 loss: 2.18161440\n",
      "iteration: 179 loss: 2.04135466\n",
      "iteration: 180 loss: 2.41906881\n",
      "iteration: 181 loss: 1.10150111\n",
      "iteration: 182 loss: 1.14857543\n",
      "iteration: 183 loss: 2.30598116\n",
      "iteration: 184 loss: 3.33162427\n",
      "iteration: 185 loss: 3.30821061\n",
      "iteration: 186 loss: 3.70023036\n",
      "iteration: 187 loss: 2.10330200\n",
      "iteration: 188 loss: 3.61476588\n",
      "iteration: 189 loss: 3.04287028\n",
      "iteration: 190 loss: 3.12653184\n",
      "iteration: 191 loss: 2.97022653\n",
      "iteration: 192 loss: 1.29376757\n",
      "iteration: 193 loss: 3.20284414\n",
      "iteration: 194 loss: 1.45665705\n",
      "iteration: 195 loss: 2.78341365\n",
      "iteration: 196 loss: 2.27641273\n",
      "iteration: 197 loss: 3.04827929\n",
      "iteration: 198 loss: 2.06501007\n",
      "iteration: 199 loss: 1.49132121\n",
      "epoch:   3 mean loss training: 2.28345203\n",
      "epoch:   3 mean loss validation: 2.31107306\n",
      "iteration:   0 loss: 1.52679515\n",
      "iteration:   1 loss: 2.53298950\n",
      "iteration:   2 loss: 2.41927218\n",
      "iteration:   3 loss: 2.06262994\n",
      "iteration:   4 loss: 2.78676844\n",
      "iteration:   5 loss: 3.05697107\n",
      "iteration:   6 loss: 2.42668557\n",
      "iteration:   7 loss: 2.93371344\n",
      "iteration:   8 loss: 2.92195463\n",
      "iteration:   9 loss: 2.93924904\n",
      "iteration:  10 loss: 1.35592091\n",
      "iteration:  11 loss: 2.67418599\n",
      "iteration:  12 loss: 3.06950307\n",
      "iteration:  13 loss: 1.45297587\n",
      "iteration:  14 loss: 2.41121888\n",
      "iteration:  15 loss: 2.11952329\n",
      "iteration:  16 loss: 1.40459073\n",
      "iteration:  17 loss: 1.53227115\n",
      "iteration:  18 loss: 2.59135103\n",
      "iteration:  19 loss: 2.64247656\n",
      "iteration:  20 loss: 1.39276648\n",
      "iteration:  21 loss: 2.60224915\n",
      "iteration:  22 loss: 1.34530365\n",
      "iteration:  23 loss: 2.78518343\n",
      "iteration:  24 loss: 2.76758099\n",
      "iteration:  25 loss: 3.12499523\n",
      "iteration:  26 loss: 1.14138246\n",
      "iteration:  27 loss: 2.56989098\n",
      "iteration:  28 loss: 2.10578394\n",
      "iteration:  29 loss: 2.47554755\n",
      "iteration:  30 loss: 3.10704970\n",
      "iteration:  31 loss: 2.81539178\n",
      "iteration:  32 loss: 1.06287873\n",
      "iteration:  33 loss: 2.86878753\n",
      "iteration:  34 loss: 2.86183643\n",
      "iteration:  35 loss: 2.08168125\n",
      "iteration:  36 loss: 2.39775538\n",
      "iteration:  37 loss: 2.80258131\n",
      "iteration:  38 loss: 1.21701515\n",
      "iteration:  39 loss: 3.09694433\n",
      "iteration:  40 loss: 2.34894586\n",
      "iteration:  41 loss: 2.80645442\n",
      "iteration:  42 loss: 1.04819357\n",
      "iteration:  43 loss: 2.76002574\n",
      "iteration:  44 loss: 1.08855188\n",
      "iteration:  45 loss: 2.63924980\n",
      "iteration:  46 loss: 3.00163698\n",
      "iteration:  47 loss: 2.29402137\n",
      "iteration:  48 loss: 1.17982233\n",
      "iteration:  49 loss: 3.18219090\n",
      "iteration:  50 loss: 3.17673063\n",
      "iteration:  51 loss: 2.21732116\n",
      "iteration:  52 loss: 2.36524987\n",
      "iteration:  53 loss: 1.04182518\n",
      "iteration:  54 loss: 3.14464450\n",
      "iteration:  55 loss: 3.07165337\n",
      "iteration:  56 loss: 2.29124069\n",
      "iteration:  57 loss: 2.16145134\n",
      "iteration:  58 loss: 3.01198030\n",
      "iteration:  59 loss: 1.09030771\n",
      "iteration:  60 loss: 2.66299319\n",
      "iteration:  61 loss: 2.63226604\n",
      "iteration:  62 loss: 2.73964810\n",
      "iteration:  63 loss: 1.07143235\n",
      "iteration:  64 loss: 2.02947831\n",
      "iteration:  65 loss: 2.31799841\n",
      "iteration:  66 loss: 1.96959662\n",
      "iteration:  67 loss: 2.04183912\n",
      "iteration:  68 loss: 1.23622668\n",
      "iteration:  69 loss: 1.26849985\n",
      "iteration:  70 loss: 1.04596698\n",
      "iteration:  71 loss: 2.19033170\n",
      "iteration:  72 loss: 2.52325654\n",
      "iteration:  73 loss: 3.26815867\n",
      "iteration:  74 loss: 1.10601926\n",
      "iteration:  75 loss: 1.01520228\n",
      "iteration:  76 loss: 2.15920734\n",
      "iteration:  77 loss: 3.18148017\n",
      "iteration:  78 loss: 2.12936544\n",
      "iteration:  79 loss: 2.97508693\n",
      "iteration:  80 loss: 1.08180773\n",
      "iteration:  81 loss: 2.01590610\n",
      "iteration:  82 loss: 0.83972651\n",
      "iteration:  83 loss: 2.92833853\n",
      "iteration:  84 loss: 2.32850313\n",
      "iteration:  85 loss: 2.73293781\n",
      "iteration:  86 loss: 0.82224202\n",
      "iteration:  87 loss: 2.28369975\n",
      "iteration:  88 loss: 0.87484151\n",
      "iteration:  89 loss: 3.14603782\n",
      "iteration:  90 loss: 3.39745188\n",
      "iteration:  91 loss: 2.68686938\n",
      "iteration:  92 loss: 0.69735694\n",
      "iteration:  93 loss: 2.44736528\n",
      "iteration:  94 loss: 2.97728515\n",
      "iteration:  95 loss: 0.75731301\n",
      "iteration:  96 loss: 3.24047732\n",
      "iteration:  97 loss: 3.39185452\n",
      "iteration:  98 loss: 2.56349158\n",
      "iteration:  99 loss: 0.76337934\n",
      "iteration: 100 loss: 2.03945351\n",
      "iteration: 101 loss: 2.10449505\n",
      "iteration: 102 loss: 2.28435349\n",
      "iteration: 103 loss: 3.20771408\n",
      "iteration: 104 loss: 0.82012987\n",
      "iteration: 105 loss: 0.76854539\n",
      "iteration: 106 loss: 0.72517729\n",
      "iteration: 107 loss: 2.94790196\n",
      "iteration: 108 loss: 0.95472145\n",
      "iteration: 109 loss: 0.86916077\n",
      "iteration: 110 loss: 3.24476647\n",
      "iteration: 111 loss: 2.38954473\n",
      "iteration: 112 loss: 3.09549904\n",
      "iteration: 113 loss: 3.08343196\n",
      "iteration: 114 loss: 2.31328630\n",
      "iteration: 115 loss: 3.18954659\n",
      "iteration: 116 loss: 0.72478348\n",
      "iteration: 117 loss: 0.77422333\n",
      "iteration: 118 loss: 2.09652185\n",
      "iteration: 119 loss: 2.93684578\n",
      "iteration: 120 loss: 2.04240465\n",
      "iteration: 121 loss: 0.80587626\n",
      "iteration: 122 loss: 0.68552810\n",
      "iteration: 123 loss: 2.83755541\n",
      "iteration: 124 loss: 2.49092817\n",
      "iteration: 125 loss: 2.43206167\n",
      "iteration: 126 loss: 3.37175751\n",
      "iteration: 127 loss: 3.37241244\n",
      "iteration: 128 loss: 2.95776963\n",
      "iteration: 129 loss: 2.13577509\n",
      "iteration: 130 loss: 2.82356882\n",
      "iteration: 131 loss: 2.57453728\n",
      "iteration: 132 loss: 3.87880659\n",
      "iteration: 133 loss: 3.41260648\n",
      "iteration: 134 loss: 3.47581267\n",
      "iteration: 135 loss: 2.75586128\n",
      "iteration: 136 loss: 2.07457018\n",
      "iteration: 137 loss: 2.92588329\n",
      "iteration: 138 loss: 0.82303905\n",
      "iteration: 139 loss: 2.01501060\n",
      "iteration: 140 loss: 3.23966813\n",
      "iteration: 141 loss: 0.94638664\n",
      "iteration: 142 loss: 2.28862119\n",
      "iteration: 143 loss: 2.43556952\n",
      "iteration: 144 loss: 2.31913996\n",
      "iteration: 145 loss: 2.33289742\n",
      "iteration: 146 loss: 2.30650043\n",
      "iteration: 147 loss: 1.93933570\n",
      "iteration: 148 loss: 2.36031199\n",
      "iteration: 149 loss: 2.58513260\n",
      "iteration: 150 loss: 1.16935563\n",
      "iteration: 151 loss: 2.20870233\n",
      "iteration: 152 loss: 2.14680767\n",
      "iteration: 153 loss: 4.20134163\n",
      "iteration: 154 loss: 1.02318418\n",
      "iteration: 155 loss: 2.18687773\n",
      "iteration: 156 loss: 0.94289416\n",
      "iteration: 157 loss: 0.97258478\n",
      "iteration: 158 loss: 2.12178564\n",
      "iteration: 159 loss: 1.89844561\n",
      "iteration: 160 loss: 1.97468555\n",
      "iteration: 161 loss: 2.86811709\n",
      "iteration: 162 loss: 0.92141372\n",
      "iteration: 163 loss: 3.03332186\n",
      "iteration: 164 loss: 2.22264910\n",
      "iteration: 165 loss: 3.09739256\n",
      "iteration: 166 loss: 2.33050823\n",
      "iteration: 167 loss: 2.71231508\n",
      "iteration: 168 loss: 0.78675938\n",
      "iteration: 169 loss: 3.15545917\n",
      "iteration: 170 loss: 3.41898298\n",
      "iteration: 171 loss: 2.91088748\n",
      "iteration: 172 loss: 0.82945061\n",
      "iteration: 173 loss: 2.93839431\n",
      "iteration: 174 loss: 2.30073738\n",
      "iteration: 175 loss: 3.42269564\n",
      "iteration: 176 loss: 3.28998256\n",
      "iteration: 177 loss: 1.94410062\n",
      "iteration: 178 loss: 2.15734220\n",
      "iteration: 179 loss: 1.92785132\n",
      "iteration: 180 loss: 2.45700955\n",
      "iteration: 181 loss: 1.15959132\n",
      "iteration: 182 loss: 1.21008337\n",
      "iteration: 183 loss: 2.39167547\n",
      "iteration: 184 loss: 3.20768261\n",
      "iteration: 185 loss: 3.27442718\n",
      "iteration: 186 loss: 3.64883041\n",
      "iteration: 187 loss: 2.01204610\n",
      "iteration: 188 loss: 3.58059192\n",
      "iteration: 189 loss: 2.90929937\n",
      "iteration: 190 loss: 3.17188859\n",
      "iteration: 191 loss: 2.99531722\n",
      "iteration: 192 loss: 1.37543499\n",
      "iteration: 193 loss: 3.09579659\n",
      "iteration: 194 loss: 1.57820237\n",
      "iteration: 195 loss: 2.73463845\n",
      "iteration: 196 loss: 2.23768187\n",
      "iteration: 197 loss: 2.95811319\n",
      "iteration: 198 loss: 1.95539749\n",
      "iteration: 199 loss: 1.60275698\n",
      "epoch:   4 mean loss training: 2.26416492\n",
      "epoch:   4 mean loss validation: 2.28546047\n",
      "iteration:   0 loss: 1.66644192\n",
      "iteration:   1 loss: 2.43269181\n",
      "iteration:   2 loss: 2.47676635\n",
      "iteration:   3 loss: 1.97584188\n",
      "iteration:   4 loss: 2.76454186\n",
      "iteration:   5 loss: 3.09514165\n",
      "iteration:   6 loss: 2.46062088\n",
      "iteration:   7 loss: 2.93920803\n",
      "iteration:   8 loss: 2.82865953\n",
      "iteration:   9 loss: 2.83762550\n",
      "iteration:  10 loss: 1.40305018\n",
      "iteration:  11 loss: 2.61198688\n",
      "iteration:  12 loss: 3.14751482\n",
      "iteration:  13 loss: 1.51279688\n",
      "iteration:  14 loss: 2.44147730\n",
      "iteration:  15 loss: 2.03349900\n",
      "iteration:  16 loss: 1.44130850\n",
      "iteration:  17 loss: 1.62020636\n",
      "iteration:  18 loss: 2.68924737\n",
      "iteration:  19 loss: 2.54669666\n",
      "iteration:  20 loss: 1.42358267\n",
      "iteration:  21 loss: 2.52116132\n",
      "iteration:  22 loss: 1.35987639\n",
      "iteration:  23 loss: 2.74006510\n",
      "iteration:  24 loss: 2.75637221\n",
      "iteration:  25 loss: 3.20605898\n",
      "iteration:  26 loss: 1.10505140\n",
      "iteration:  27 loss: 2.63060403\n",
      "iteration:  28 loss: 2.03600025\n",
      "iteration:  29 loss: 2.50333667\n",
      "iteration:  30 loss: 2.99837494\n",
      "iteration:  31 loss: 2.77736974\n",
      "iteration:  32 loss: 0.99631739\n",
      "iteration:  33 loss: 2.84242845\n",
      "iteration:  34 loss: 2.86230135\n",
      "iteration:  35 loss: 2.00255013\n",
      "iteration:  36 loss: 2.22842741\n",
      "iteration:  37 loss: 2.78608680\n",
      "iteration:  38 loss: 1.19759607\n",
      "iteration:  39 loss: 3.06469560\n",
      "iteration:  40 loss: 2.37368941\n",
      "iteration:  41 loss: 2.78431654\n",
      "iteration:  42 loss: 1.01084018\n",
      "iteration:  43 loss: 2.70440412\n",
      "iteration:  44 loss: 1.05339062\n",
      "iteration:  45 loss: 2.53294635\n",
      "iteration:  46 loss: 3.02570534\n",
      "iteration:  47 loss: 2.34966588\n",
      "iteration:  48 loss: 1.16859043\n",
      "iteration:  49 loss: 3.16743612\n",
      "iteration:  50 loss: 3.16933703\n",
      "iteration:  51 loss: 2.17841005\n",
      "iteration:  52 loss: 2.39711618\n",
      "iteration:  53 loss: 1.00341058\n",
      "iteration:  54 loss: 3.10853219\n",
      "iteration:  55 loss: 3.01087689\n",
      "iteration:  56 loss: 2.29190254\n",
      "iteration:  57 loss: 2.11922526\n",
      "iteration:  58 loss: 2.98380041\n",
      "iteration:  59 loss: 1.07882047\n",
      "iteration:  60 loss: 2.56436539\n",
      "iteration:  61 loss: 2.53291869\n",
      "iteration:  62 loss: 2.67512965\n",
      "iteration:  63 loss: 1.04051745\n",
      "iteration:  64 loss: 1.93942046\n",
      "iteration:  65 loss: 2.34741759\n",
      "iteration:  66 loss: 1.88721907\n",
      "iteration:  67 loss: 1.97867000\n",
      "iteration:  68 loss: 1.26986253\n",
      "iteration:  69 loss: 1.30041015\n",
      "iteration:  70 loss: 1.01895273\n",
      "iteration:  71 loss: 2.14860392\n",
      "iteration:  72 loss: 2.59345794\n",
      "iteration:  73 loss: 3.24344325\n",
      "iteration:  74 loss: 1.09655607\n",
      "iteration:  75 loss: 0.99231988\n",
      "iteration:  76 loss: 2.10919094\n",
      "iteration:  77 loss: 3.24201131\n",
      "iteration:  78 loss: 2.06300569\n",
      "iteration:  79 loss: 2.99240756\n",
      "iteration:  80 loss: 1.09183860\n",
      "iteration:  81 loss: 1.89753354\n",
      "iteration:  82 loss: 0.79683912\n",
      "iteration:  83 loss: 2.85463071\n",
      "iteration:  84 loss: 2.33949971\n",
      "iteration:  85 loss: 2.64698768\n",
      "iteration:  86 loss: 0.78299367\n",
      "iteration:  87 loss: 2.26082540\n",
      "iteration:  88 loss: 0.85090309\n",
      "iteration:  89 loss: 3.21100760\n",
      "iteration:  90 loss: 3.34000874\n",
      "iteration:  91 loss: 2.59027433\n",
      "iteration:  92 loss: 0.64778531\n",
      "iteration:  93 loss: 2.49656177\n",
      "iteration:  94 loss: 2.95575404\n",
      "iteration:  95 loss: 0.70996225\n",
      "iteration:  96 loss: 3.34157038\n",
      "iteration:  97 loss: 3.32936692\n",
      "iteration:  98 loss: 2.41064286\n",
      "iteration:  99 loss: 0.73269868\n",
      "iteration: 100 loss: 1.94135916\n",
      "iteration: 101 loss: 2.02664733\n",
      "iteration: 102 loss: 2.28533530\n",
      "iteration: 103 loss: 3.25027490\n",
      "iteration: 104 loss: 0.80074203\n",
      "iteration: 105 loss: 0.75567079\n",
      "iteration: 106 loss: 0.68995523\n",
      "iteration: 107 loss: 2.87938929\n",
      "iteration: 108 loss: 0.99982738\n",
      "iteration: 109 loss: 0.88632196\n",
      "iteration: 110 loss: 3.28915834\n",
      "iteration: 111 loss: 2.41375995\n",
      "iteration: 112 loss: 3.10068655\n",
      "iteration: 113 loss: 3.07081747\n",
      "iteration: 114 loss: 2.34596944\n",
      "iteration: 115 loss: 3.20878291\n",
      "iteration: 116 loss: 0.70468998\n",
      "iteration: 117 loss: 0.77146292\n",
      "iteration: 118 loss: 1.99140155\n",
      "iteration: 119 loss: 2.89584112\n",
      "iteration: 120 loss: 1.96257246\n",
      "iteration: 121 loss: 0.81337547\n",
      "iteration: 122 loss: 0.64919823\n",
      "iteration: 123 loss: 2.73382640\n",
      "iteration: 124 loss: 2.54083824\n",
      "iteration: 125 loss: 2.46744084\n",
      "iteration: 126 loss: 3.43789315\n",
      "iteration: 127 loss: 3.28354526\n",
      "iteration: 128 loss: 2.90842390\n",
      "iteration: 129 loss: 2.05485320\n",
      "iteration: 130 loss: 2.71672535\n",
      "iteration: 131 loss: 2.66471863\n",
      "iteration: 132 loss: 3.95594454\n",
      "iteration: 133 loss: 3.34156203\n",
      "iteration: 134 loss: 3.44695449\n",
      "iteration: 135 loss: 2.63645172\n",
      "iteration: 136 loss: 2.00233364\n",
      "iteration: 137 loss: 2.88372278\n",
      "iteration: 138 loss: 0.81202972\n",
      "iteration: 139 loss: 1.90544474\n",
      "iteration: 140 loss: 3.30579090\n",
      "iteration: 141 loss: 0.95260507\n",
      "iteration: 142 loss: 2.28433752\n",
      "iteration: 143 loss: 2.49854445\n",
      "iteration: 144 loss: 2.35014486\n",
      "iteration: 145 loss: 2.35502958\n",
      "iteration: 146 loss: 2.31740093\n",
      "iteration: 147 loss: 1.82456255\n",
      "iteration: 148 loss: 2.38586736\n",
      "iteration: 149 loss: 2.66502762\n",
      "iteration: 150 loss: 1.26706982\n",
      "iteration: 151 loss: 2.19391251\n",
      "iteration: 152 loss: 2.07682848\n",
      "iteration: 153 loss: 4.18534184\n",
      "iteration: 154 loss: 1.05239630\n",
      "iteration: 155 loss: 2.17109799\n",
      "iteration: 156 loss: 0.93790960\n",
      "iteration: 157 loss: 0.98172766\n",
      "iteration: 158 loss: 2.05253053\n",
      "iteration: 159 loss: 1.76497316\n",
      "iteration: 160 loss: 1.88134754\n",
      "iteration: 161 loss: 2.81474471\n",
      "iteration: 162 loss: 0.92818904\n",
      "iteration: 163 loss: 3.03756928\n",
      "iteration: 164 loss: 2.18882227\n",
      "iteration: 165 loss: 3.11369014\n",
      "iteration: 166 loss: 2.35560846\n",
      "iteration: 167 loss: 2.58962941\n",
      "iteration: 168 loss: 0.74614477\n",
      "iteration: 169 loss: 3.04870319\n",
      "iteration: 170 loss: 3.32428908\n",
      "iteration: 171 loss: 2.86601949\n",
      "iteration: 172 loss: 0.79707342\n",
      "iteration: 173 loss: 2.74539733\n",
      "iteration: 174 loss: 2.31308579\n",
      "iteration: 175 loss: 3.41185999\n",
      "iteration: 176 loss: 3.25414419\n",
      "iteration: 177 loss: 1.82634783\n",
      "iteration: 178 loss: 2.12838197\n",
      "iteration: 179 loss: 1.79059160\n",
      "iteration: 180 loss: 2.51014066\n",
      "iteration: 181 loss: 1.23914111\n",
      "iteration: 182 loss: 1.29431117\n",
      "iteration: 183 loss: 2.49083900\n",
      "iteration: 184 loss: 3.04590082\n",
      "iteration: 185 loss: 3.23193765\n",
      "iteration: 186 loss: 3.57622099\n",
      "iteration: 187 loss: 1.90324211\n",
      "iteration: 188 loss: 3.52968311\n",
      "iteration: 189 loss: 2.74708986\n",
      "iteration: 190 loss: 3.24196982\n",
      "iteration: 191 loss: 3.02009058\n",
      "iteration: 192 loss: 1.48428249\n",
      "iteration: 193 loss: 2.96211839\n",
      "iteration: 194 loss: 1.73379147\n",
      "iteration: 195 loss: 2.67644453\n",
      "iteration: 196 loss: 2.19369960\n",
      "iteration: 197 loss: 2.85277224\n",
      "iteration: 198 loss: 1.82551157\n",
      "iteration: 199 loss: 1.73789322\n",
      "epoch:   5 mean loss training: 2.24203658\n",
      "epoch:   5 mean loss validation: 2.25710464\n",
      "iteration:   0 loss: 1.83569586\n",
      "iteration:   1 loss: 2.31832719\n",
      "iteration:   2 loss: 2.53070426\n",
      "iteration:   3 loss: 1.86731112\n",
      "iteration:   4 loss: 2.75291586\n",
      "iteration:   5 loss: 3.15073848\n",
      "iteration:   6 loss: 2.50693870\n",
      "iteration:   7 loss: 2.94434118\n",
      "iteration:   8 loss: 2.72572041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:   9 loss: 2.73154616\n",
      "iteration:  10 loss: 1.44299436\n",
      "iteration:  11 loss: 2.55140090\n",
      "iteration:  12 loss: 3.24384522\n",
      "iteration:  13 loss: 1.56351531\n",
      "iteration:  14 loss: 2.47942924\n",
      "iteration:  15 loss: 1.91866958\n",
      "iteration:  16 loss: 1.46031046\n",
      "iteration:  17 loss: 1.69802821\n",
      "iteration:  18 loss: 2.79891062\n",
      "iteration:  19 loss: 2.43714762\n",
      "iteration:  20 loss: 1.43546164\n",
      "iteration:  21 loss: 2.43769908\n",
      "iteration:  22 loss: 1.34788179\n",
      "iteration:  23 loss: 2.69091749\n",
      "iteration:  24 loss: 2.75314641\n",
      "iteration:  25 loss: 3.29813409\n",
      "iteration:  26 loss: 1.04436350\n",
      "iteration:  27 loss: 2.68928266\n",
      "iteration:  28 loss: 1.95224345\n",
      "iteration:  29 loss: 2.52795362\n",
      "iteration:  30 loss: 2.89237022\n",
      "iteration:  31 loss: 2.74656487\n",
      "iteration:  32 loss: 0.90386540\n",
      "iteration:  33 loss: 2.82343864\n",
      "iteration:  34 loss: 2.86143064\n",
      "iteration:  35 loss: 1.90798318\n",
      "iteration:  36 loss: 2.03317904\n",
      "iteration:  37 loss: 2.77434993\n",
      "iteration:  38 loss: 1.15715039\n",
      "iteration:  39 loss: 3.04681253\n",
      "iteration:  40 loss: 2.39051986\n",
      "iteration:  41 loss: 2.75995922\n",
      "iteration:  42 loss: 0.95434290\n",
      "iteration:  43 loss: 2.64467740\n",
      "iteration:  44 loss: 1.00202549\n",
      "iteration:  45 loss: 2.40732002\n",
      "iteration:  46 loss: 3.06151366\n",
      "iteration:  47 loss: 2.40454626\n",
      "iteration:  48 loss: 1.14496183\n",
      "iteration:  49 loss: 3.15029669\n",
      "iteration:  50 loss: 3.15893722\n",
      "iteration:  51 loss: 2.13708568\n",
      "iteration:  52 loss: 2.42776203\n",
      "iteration:  53 loss: 0.96172184\n",
      "iteration:  54 loss: 3.05767989\n",
      "iteration:  55 loss: 2.93458676\n",
      "iteration:  56 loss: 2.29830837\n",
      "iteration:  57 loss: 2.06786704\n",
      "iteration:  58 loss: 2.93334913\n",
      "iteration:  59 loss: 1.07553756\n",
      "iteration:  60 loss: 2.43730664\n",
      "iteration:  61 loss: 2.40449739\n",
      "iteration:  62 loss: 2.59354305\n",
      "iteration:  63 loss: 1.01836526\n",
      "iteration:  64 loss: 1.82669616\n",
      "iteration:  65 loss: 2.38643432\n",
      "iteration:  66 loss: 1.78367007\n",
      "iteration:  67 loss: 1.89651763\n",
      "iteration:  68 loss: 1.33723414\n",
      "iteration:  69 loss: 1.36833405\n",
      "iteration:  70 loss: 1.00777352\n",
      "iteration:  71 loss: 2.09758210\n",
      "iteration:  72 loss: 2.65896893\n",
      "iteration:  73 loss: 3.20335865\n",
      "iteration:  74 loss: 1.10146570\n",
      "iteration:  75 loss: 0.97858542\n",
      "iteration:  76 loss: 2.04720831\n",
      "iteration:  77 loss: 3.32013273\n",
      "iteration:  78 loss: 1.98086536\n",
      "iteration:  79 loss: 3.01569557\n",
      "iteration:  80 loss: 1.11831701\n",
      "iteration:  81 loss: 1.75059414\n",
      "iteration:  82 loss: 0.75249815\n",
      "iteration:  83 loss: 2.77723551\n",
      "iteration:  84 loss: 2.35258126\n",
      "iteration:  85 loss: 2.53569221\n",
      "iteration:  86 loss: 0.73966461\n",
      "iteration:  87 loss: 2.23526812\n",
      "iteration:  88 loss: 0.82726073\n",
      "iteration:  89 loss: 3.28816819\n",
      "iteration:  90 loss: 3.27831268\n",
      "iteration:  91 loss: 2.45648456\n",
      "iteration:  92 loss: 0.59597772\n",
      "iteration:  93 loss: 2.56156850\n",
      "iteration:  94 loss: 2.92362332\n",
      "iteration:  95 loss: 0.65787840\n",
      "iteration:  96 loss: 3.45776224\n",
      "iteration:  97 loss: 3.24431968\n",
      "iteration:  98 loss: 2.21087098\n",
      "iteration:  99 loss: 0.70173156\n",
      "iteration: 100 loss: 1.82144117\n",
      "iteration: 101 loss: 1.93237734\n",
      "iteration: 102 loss: 2.28890467\n",
      "iteration: 103 loss: 3.29851675\n",
      "iteration: 104 loss: 0.78557193\n",
      "iteration: 105 loss: 0.75071549\n",
      "iteration: 106 loss: 0.65457612\n",
      "iteration: 107 loss: 2.79075480\n",
      "iteration: 108 loss: 1.07493186\n",
      "iteration: 109 loss: 0.91759104\n",
      "iteration: 110 loss: 3.34414673\n",
      "iteration: 111 loss: 2.44842148\n",
      "iteration: 112 loss: 3.11233687\n",
      "iteration: 113 loss: 3.05593419\n",
      "iteration: 114 loss: 2.37910509\n",
      "iteration: 115 loss: 3.24083972\n",
      "iteration: 116 loss: 0.68741947\n",
      "iteration: 117 loss: 0.77035248\n",
      "iteration: 118 loss: 1.86995196\n",
      "iteration: 119 loss: 2.84137583\n",
      "iteration: 120 loss: 1.85236585\n",
      "iteration: 121 loss: 0.82788599\n",
      "iteration: 122 loss: 0.61319828\n",
      "iteration: 123 loss: 2.60666800\n",
      "iteration: 124 loss: 2.60207224\n",
      "iteration: 125 loss: 2.51023149\n",
      "iteration: 126 loss: 3.51621985\n",
      "iteration: 127 loss: 3.15823555\n",
      "iteration: 128 loss: 2.82931638\n",
      "iteration: 129 loss: 1.95425141\n",
      "iteration: 130 loss: 2.57262278\n",
      "iteration: 131 loss: 2.76450753\n",
      "iteration: 132 loss: 4.04181242\n",
      "iteration: 133 loss: 3.24886584\n",
      "iteration: 134 loss: 3.38570166\n",
      "iteration: 135 loss: 2.46920586\n",
      "iteration: 136 loss: 1.90885210\n",
      "iteration: 137 loss: 2.82115173\n",
      "iteration: 138 loss: 0.81299859\n",
      "iteration: 139 loss: 1.76466012\n",
      "iteration: 140 loss: 3.37471724\n",
      "iteration: 141 loss: 0.97799987\n",
      "iteration: 142 loss: 2.28336763\n",
      "iteration: 143 loss: 2.58789825\n",
      "iteration: 144 loss: 2.39217138\n",
      "iteration: 145 loss: 2.38480616\n",
      "iteration: 146 loss: 2.34101248\n",
      "iteration: 147 loss: 1.68311870\n",
      "iteration: 148 loss: 2.42580080\n",
      "iteration: 149 loss: 2.78229237\n",
      "iteration: 150 loss: 1.39722741\n",
      "iteration: 151 loss: 2.17536616\n",
      "iteration: 152 loss: 1.99788284\n",
      "iteration: 153 loss: 4.15052605\n",
      "iteration: 154 loss: 1.08775282\n",
      "iteration: 155 loss: 2.15584087\n",
      "iteration: 156 loss: 0.92791057\n",
      "iteration: 157 loss: 0.98583716\n",
      "iteration: 158 loss: 1.96137750\n",
      "iteration: 159 loss: 1.58451366\n",
      "iteration: 160 loss: 1.76009548\n",
      "iteration: 161 loss: 2.75015664\n",
      "iteration: 162 loss: 0.92928988\n",
      "iteration: 163 loss: 3.05078530\n",
      "iteration: 164 loss: 2.16013598\n",
      "iteration: 165 loss: 3.13180327\n",
      "iteration: 166 loss: 2.38652968\n",
      "iteration: 167 loss: 2.44457936\n",
      "iteration: 168 loss: 0.69707072\n",
      "iteration: 169 loss: 2.91601658\n",
      "iteration: 170 loss: 3.21521020\n",
      "iteration: 171 loss: 2.80157733\n",
      "iteration: 172 loss: 0.75698555\n",
      "iteration: 173 loss: 2.49934340\n",
      "iteration: 174 loss: 2.34086394\n",
      "iteration: 175 loss: 3.39319372\n",
      "iteration: 176 loss: 3.21663785\n",
      "iteration: 177 loss: 1.67244756\n",
      "iteration: 178 loss: 2.10044646\n",
      "iteration: 179 loss: 1.61104822\n",
      "iteration: 180 loss: 2.58829546\n",
      "iteration: 181 loss: 1.32597339\n",
      "iteration: 182 loss: 1.39207602\n",
      "iteration: 183 loss: 2.59594965\n",
      "iteration: 184 loss: 2.85509014\n",
      "iteration: 185 loss: 3.19949865\n",
      "iteration: 186 loss: 3.50523686\n",
      "iteration: 187 loss: 1.75023556\n",
      "iteration: 188 loss: 3.48648095\n",
      "iteration: 189 loss: 2.55508089\n",
      "iteration: 190 loss: 3.36035275\n",
      "iteration: 191 loss: 3.06889129\n",
      "iteration: 192 loss: 1.60040820\n",
      "iteration: 193 loss: 2.80782700\n",
      "iteration: 194 loss: 1.89682877\n",
      "iteration: 195 loss: 2.62569761\n",
      "iteration: 196 loss: 2.14630055\n",
      "iteration: 197 loss: 2.74540877\n",
      "iteration: 198 loss: 1.65241957\n",
      "iteration: 199 loss: 1.87032628\n",
      "epoch:   6 mean loss training: 2.21515942\n",
      "epoch:   6 mean loss validation: 2.21789145\n",
      "iteration:   0 loss: 2.00505042\n",
      "iteration:   1 loss: 2.20544195\n",
      "iteration:   2 loss: 2.55671072\n",
      "iteration:   3 loss: 1.71138549\n",
      "iteration:   4 loss: 2.77339530\n",
      "iteration:   5 loss: 3.25472784\n",
      "iteration:   6 loss: 2.54869151\n",
      "iteration:   7 loss: 2.90620279\n",
      "iteration:   8 loss: 2.63919306\n",
      "iteration:   9 loss: 2.64120793\n",
      "iteration:  10 loss: 1.43307209\n",
      "iteration:  11 loss: 2.50106239\n",
      "iteration:  12 loss: 3.37403393\n",
      "iteration:  13 loss: 1.55660367\n",
      "iteration:  14 loss: 2.51624846\n",
      "iteration:  15 loss: 1.75348401\n",
      "iteration:  16 loss: 1.42443633\n",
      "iteration:  17 loss: 1.72821152\n",
      "iteration:  18 loss: 2.87048912\n",
      "iteration:  19 loss: 2.33902025\n",
      "iteration:  20 loss: 1.38994896\n",
      "iteration:  21 loss: 2.34837222\n",
      "iteration:  22 loss: 1.26919413\n",
      "iteration:  23 loss: 2.64109445\n",
      "iteration:  24 loss: 2.77384019\n",
      "iteration:  25 loss: 3.40637684\n",
      "iteration:  26 loss: 0.94182163\n",
      "iteration:  27 loss: 2.75638270\n",
      "iteration:  28 loss: 1.84373891\n",
      "iteration:  29 loss: 2.57434773\n",
      "iteration:  30 loss: 2.79400182\n",
      "iteration:  31 loss: 2.73852873\n",
      "iteration:  32 loss: 0.78603625\n",
      "iteration:  33 loss: 2.80005360\n",
      "iteration:  34 loss: 2.84828448\n",
      "iteration:  35 loss: 1.79592931\n",
      "iteration:  36 loss: 1.78692043\n",
      "iteration:  37 loss: 2.76654196\n",
      "iteration:  38 loss: 1.08905542\n",
      "iteration:  39 loss: 3.04724193\n",
      "iteration:  40 loss: 2.40703845\n",
      "iteration:  41 loss: 2.70820594\n",
      "iteration:  42 loss: 0.89021993\n",
      "iteration:  43 loss: 2.56655169\n",
      "iteration:  44 loss: 0.95148021\n",
      "iteration:  45 loss: 2.23689127\n",
      "iteration:  46 loss: 3.12444782\n",
      "iteration:  47 loss: 2.45041704\n",
      "iteration:  48 loss: 1.12451780\n",
      "iteration:  49 loss: 3.12824464\n",
      "iteration:  50 loss: 3.14566207\n",
      "iteration:  51 loss: 2.09915590\n",
      "iteration:  52 loss: 2.44181657\n",
      "iteration:  53 loss: 0.93550301\n",
      "iteration:  54 loss: 2.96507645\n",
      "iteration:  55 loss: 2.81808805\n",
      "iteration:  56 loss: 2.33142543\n",
      "iteration:  57 loss: 2.00789070\n",
      "iteration:  58 loss: 2.85403991\n",
      "iteration:  59 loss: 1.11632526\n",
      "iteration:  60 loss: 2.26934218\n",
      "iteration:  61 loss: 2.23007202\n",
      "iteration:  62 loss: 2.47835469\n",
      "iteration:  63 loss: 1.02358973\n",
      "iteration:  64 loss: 1.67725587\n",
      "iteration:  65 loss: 2.43039298\n",
      "iteration:  66 loss: 1.65504479\n",
      "iteration:  67 loss: 1.78745914\n",
      "iteration:  68 loss: 1.45899367\n",
      "iteration:  69 loss: 1.48632026\n",
      "iteration:  70 loss: 1.01375318\n",
      "iteration:  71 loss: 2.02937436\n",
      "iteration:  72 loss: 2.71862054\n",
      "iteration:  73 loss: 3.16311789\n",
      "iteration:  74 loss: 1.10455287\n",
      "iteration:  75 loss: 0.96195632\n",
      "iteration:  76 loss: 1.95195651\n",
      "iteration:  77 loss: 3.43410325\n",
      "iteration:  78 loss: 1.87016571\n",
      "iteration:  79 loss: 3.07310939\n",
      "iteration:  80 loss: 1.14130735\n",
      "iteration:  81 loss: 1.55687630\n",
      "iteration:  82 loss: 0.67874551\n",
      "iteration:  83 loss: 2.67891002\n",
      "iteration:  84 loss: 2.38018465\n",
      "iteration:  85 loss: 2.39493108\n",
      "iteration:  86 loss: 0.65968424\n",
      "iteration:  87 loss: 2.21246266\n",
      "iteration:  88 loss: 0.77978444\n",
      "iteration:  89 loss: 3.39576054\n",
      "iteration:  90 loss: 3.21527505\n",
      "iteration:  91 loss: 2.29642677\n",
      "iteration:  92 loss: 0.53155631\n",
      "iteration:  93 loss: 2.64273477\n",
      "iteration:  94 loss: 2.87855148\n",
      "iteration:  95 loss: 0.58058363\n",
      "iteration:  96 loss: 3.62659597\n",
      "iteration:  97 loss: 3.12288141\n",
      "iteration:  98 loss: 1.97033393\n",
      "iteration:  99 loss: 0.66167867\n",
      "iteration: 100 loss: 1.65762699\n",
      "iteration: 101 loss: 1.79681480\n",
      "iteration: 102 loss: 2.31355834\n",
      "iteration: 103 loss: 3.37870598\n",
      "iteration: 104 loss: 0.76120102\n",
      "iteration: 105 loss: 0.76052189\n",
      "iteration: 106 loss: 0.61098647\n",
      "iteration: 107 loss: 2.66149831\n",
      "iteration: 108 loss: 1.19580626\n",
      "iteration: 109 loss: 0.97606677\n",
      "iteration: 110 loss: 3.41307044\n",
      "iteration: 111 loss: 2.51138210\n",
      "iteration: 112 loss: 3.13719106\n",
      "iteration: 113 loss: 3.04555702\n",
      "iteration: 114 loss: 2.42698526\n",
      "iteration: 115 loss: 3.28566504\n",
      "iteration: 116 loss: 0.68504888\n",
      "iteration: 117 loss: 0.77905548\n",
      "iteration: 118 loss: 1.69080400\n",
      "iteration: 119 loss: 2.76469803\n",
      "iteration: 120 loss: 1.71099699\n",
      "iteration: 121 loss: 0.84997427\n",
      "iteration: 122 loss: 0.57758486\n",
      "iteration: 123 loss: 2.41513228\n",
      "iteration: 124 loss: 2.65280771\n",
      "iteration: 125 loss: 2.55358577\n",
      "iteration: 126 loss: 3.59349012\n",
      "iteration: 127 loss: 2.96478605\n",
      "iteration: 128 loss: 2.69665813\n",
      "iteration: 129 loss: 1.80862808\n",
      "iteration: 130 loss: 2.36822486\n",
      "iteration: 131 loss: 2.87433958\n",
      "iteration: 132 loss: 4.10952663\n",
      "iteration: 133 loss: 3.11308122\n",
      "iteration: 134 loss: 3.28284431\n",
      "iteration: 135 loss: 2.23501849\n",
      "iteration: 136 loss: 1.78388643\n",
      "iteration: 137 loss: 2.72180605\n",
      "iteration: 138 loss: 0.82769132\n",
      "iteration: 139 loss: 1.58080983\n",
      "iteration: 140 loss: 3.43146801\n",
      "iteration: 141 loss: 1.01033974\n",
      "iteration: 142 loss: 2.30303717\n",
      "iteration: 143 loss: 2.70835853\n",
      "iteration: 144 loss: 2.47194576\n",
      "iteration: 145 loss: 2.43583775\n",
      "iteration: 146 loss: 2.38921547\n",
      "iteration: 147 loss: 1.50528800\n",
      "iteration: 148 loss: 2.48755860\n",
      "iteration: 149 loss: 2.93617058\n",
      "iteration: 150 loss: 1.55648005\n",
      "iteration: 151 loss: 2.15100813\n",
      "iteration: 152 loss: 1.90554047\n",
      "iteration: 153 loss: 4.11932230\n",
      "iteration: 154 loss: 1.12098944\n",
      "iteration: 155 loss: 2.16239190\n",
      "iteration: 156 loss: 0.89407402\n",
      "iteration: 157 loss: 0.96346599\n",
      "iteration: 158 loss: 1.84773588\n",
      "iteration: 159 loss: 1.33445108\n",
      "iteration: 160 loss: 1.60176194\n",
      "iteration: 161 loss: 2.65910578\n",
      "iteration: 162 loss: 0.90312892\n",
      "iteration: 163 loss: 3.09024477\n",
      "iteration: 164 loss: 2.13032675\n",
      "iteration: 165 loss: 3.18615818\n",
      "iteration: 166 loss: 2.41842437\n",
      "iteration: 167 loss: 2.25967336\n",
      "iteration: 168 loss: 0.64180416\n",
      "iteration: 169 loss: 2.73509979\n",
      "iteration: 170 loss: 3.08339310\n",
      "iteration: 171 loss: 2.70137191\n",
      "iteration: 172 loss: 0.70770389\n",
      "iteration: 173 loss: 2.16198087\n",
      "iteration: 174 loss: 2.39905334\n",
      "iteration: 175 loss: 3.35580325\n",
      "iteration: 176 loss: 3.16116333\n",
      "iteration: 177 loss: 1.46212769\n",
      "iteration: 178 loss: 2.07014012\n",
      "iteration: 179 loss: 1.37786329\n",
      "iteration: 180 loss: 2.70836210\n",
      "iteration: 181 loss: 1.45353150\n",
      "iteration: 182 loss: 1.53574014\n",
      "iteration: 183 loss: 2.70487261\n",
      "iteration: 184 loss: 2.58202481\n",
      "iteration: 185 loss: 3.15257931\n",
      "iteration: 186 loss: 3.37925267\n",
      "iteration: 187 loss: 1.53568053\n",
      "iteration: 188 loss: 3.41957355\n",
      "iteration: 189 loss: 2.31142449\n",
      "iteration: 190 loss: 3.53281188\n",
      "iteration: 191 loss: 3.12620997\n",
      "iteration: 192 loss: 1.74573755\n",
      "iteration: 193 loss: 2.61360288\n",
      "iteration: 194 loss: 2.07903051\n",
      "iteration: 195 loss: 2.58233809\n",
      "iteration: 196 loss: 2.08890963\n",
      "iteration: 197 loss: 2.62662363\n",
      "iteration: 198 loss: 1.41466248\n",
      "iteration: 199 loss: 1.98202372\n",
      "epoch:   7 mean loss training: 2.17825627\n",
      "epoch:   7 mean loss validation: 2.16741753\n",
      "iteration:   0 loss: 2.17049623\n",
      "iteration:   1 loss: 2.10791135\n",
      "iteration:   2 loss: 2.56861949\n",
      "iteration:   3 loss: 1.48588502\n",
      "iteration:   4 loss: 2.82296300\n",
      "iteration:   5 loss: 3.41300201\n",
      "iteration:   6 loss: 2.60166407\n",
      "iteration:   7 loss: 2.83267450\n",
      "iteration:   8 loss: 2.57858419\n",
      "iteration:   9 loss: 2.57732177\n",
      "iteration:  10 loss: 1.37202454\n",
      "iteration:  11 loss: 2.46368504\n",
      "iteration:  12 loss: 3.48924422\n",
      "iteration:  13 loss: 1.48262525\n",
      "iteration:  14 loss: 2.54646468\n",
      "iteration:  15 loss: 1.55621409\n",
      "iteration:  16 loss: 1.33579683\n",
      "iteration:  17 loss: 1.70460176\n",
      "iteration:  18 loss: 2.91782427\n",
      "iteration:  19 loss: 2.28197622\n",
      "iteration:  20 loss: 1.27478218\n",
      "iteration:  21 loss: 2.26271582\n",
      "iteration:  22 loss: 1.11288869\n",
      "iteration:  23 loss: 2.56801200\n",
      "iteration:  24 loss: 2.78823566\n",
      "iteration:  25 loss: 3.49733448\n",
      "iteration:  26 loss: 0.83627689\n",
      "iteration:  27 loss: 2.80681920\n",
      "iteration:  28 loss: 1.72030163\n",
      "iteration:  29 loss: 2.58475590\n",
      "iteration:  30 loss: 2.71630096\n",
      "iteration:  31 loss: 2.76029110\n",
      "iteration:  32 loss: 0.66830122\n",
      "iteration:  33 loss: 2.75089550\n",
      "iteration:  34 loss: 2.84869409\n",
      "iteration:  35 loss: 1.67091954\n",
      "iteration:  36 loss: 1.47221923\n",
      "iteration:  37 loss: 2.75800467\n",
      "iteration:  38 loss: 1.02819002\n",
      "iteration:  39 loss: 3.04270196\n",
      "iteration:  40 loss: 2.42193437\n",
      "iteration:  41 loss: 2.64445257\n",
      "iteration:  42 loss: 0.82694399\n",
      "iteration:  43 loss: 2.46837807\n",
      "iteration:  44 loss: 0.91378695\n",
      "iteration:  45 loss: 1.97711611\n",
      "iteration:  46 loss: 3.20833850\n",
      "iteration:  47 loss: 2.47857380\n",
      "iteration:  48 loss: 1.13356745\n",
      "iteration:  49 loss: 3.07428789\n",
      "iteration:  50 loss: 3.11622262\n",
      "iteration:  51 loss: 2.07777953\n",
      "iteration:  52 loss: 2.47117686\n",
      "iteration:  53 loss: 0.92726779\n",
      "iteration:  54 loss: 2.80044055\n",
      "iteration:  55 loss: 2.62265873\n",
      "iteration:  56 loss: 2.39047122\n",
      "iteration:  57 loss: 1.92755783\n",
      "iteration:  58 loss: 2.69739938\n",
      "iteration:  59 loss: 1.21140850\n",
      "iteration:  60 loss: 2.04054189\n",
      "iteration:  61 loss: 2.00019956\n",
      "iteration:  62 loss: 2.31972241\n",
      "iteration:  63 loss: 1.06289113\n",
      "iteration:  64 loss: 1.50530112\n",
      "iteration:  65 loss: 2.49624658\n",
      "iteration:  66 loss: 1.52444458\n",
      "iteration:  67 loss: 1.65498400\n",
      "iteration:  68 loss: 1.63132346\n",
      "iteration:  69 loss: 1.64408040\n",
      "iteration:  70 loss: 1.02516758\n",
      "iteration:  71 loss: 1.92902493\n",
      "iteration:  72 loss: 2.74586940\n",
      "iteration:  73 loss: 3.15215969\n",
      "iteration:  74 loss: 1.06649113\n",
      "iteration:  75 loss: 0.90666956\n",
      "iteration:  76 loss: 1.80805957\n",
      "iteration:  77 loss: 3.58813381\n",
      "iteration:  78 loss: 1.72889924\n",
      "iteration:  79 loss: 3.19586587\n",
      "iteration:  80 loss: 1.11202538\n",
      "iteration:  81 loss: 1.31756544\n",
      "iteration:  82 loss: 0.55944711\n",
      "iteration:  83 loss: 2.57763052\n",
      "iteration:  84 loss: 2.41917777\n",
      "iteration:  85 loss: 2.24595666\n",
      "iteration:  86 loss: 0.52722925\n",
      "iteration:  87 loss: 2.21623206\n",
      "iteration:  88 loss: 0.68350416\n",
      "iteration:  89 loss: 3.52722597\n",
      "iteration:  90 loss: 3.17199016\n",
      "iteration:  91 loss: 2.11379051\n",
      "iteration:  92 loss: 0.46543396\n",
      "iteration:  93 loss: 2.72300291\n",
      "iteration:  94 loss: 2.79398942\n",
      "iteration:  95 loss: 0.48374560\n",
      "iteration:  96 loss: 3.79820848\n",
      "iteration:  97 loss: 2.93384576\n",
      "iteration:  98 loss: 1.68081856\n",
      "iteration:  99 loss: 0.63066363\n",
      "iteration: 100 loss: 1.44467449\n",
      "iteration: 101 loss: 1.60100102\n",
      "iteration: 102 loss: 2.37079191\n",
      "iteration: 103 loss: 3.47177553\n",
      "iteration: 104 loss: 0.74799711\n",
      "iteration: 105 loss: 0.81003368\n",
      "iteration: 106 loss: 0.57989931\n",
      "iteration: 107 loss: 2.47358370\n",
      "iteration: 108 loss: 1.41308367\n",
      "iteration: 109 loss: 1.09152257\n",
      "iteration: 110 loss: 3.47959352\n",
      "iteration: 111 loss: 2.58726239\n",
      "iteration: 112 loss: 3.17896771\n",
      "iteration: 113 loss: 3.04542804\n",
      "iteration: 114 loss: 2.46541357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 115 loss: 3.33705425\n",
      "iteration: 116 loss: 0.71966517\n",
      "iteration: 117 loss: 0.79429841\n",
      "iteration: 118 loss: 1.44808793\n",
      "iteration: 119 loss: 2.63524175\n",
      "iteration: 120 loss: 1.51068783\n",
      "iteration: 121 loss: 0.88293701\n",
      "iteration: 122 loss: 0.55222553\n",
      "iteration: 123 loss: 2.17423987\n",
      "iteration: 124 loss: 2.68163013\n",
      "iteration: 125 loss: 2.58036160\n",
      "iteration: 126 loss: 3.66957545\n",
      "iteration: 127 loss: 2.67350316\n",
      "iteration: 128 loss: 2.48799229\n",
      "iteration: 129 loss: 1.60284233\n",
      "iteration: 130 loss: 2.12208247\n",
      "iteration: 131 loss: 2.96816373\n",
      "iteration: 132 loss: 4.16298151\n",
      "iteration: 133 loss: 2.96793699\n",
      "iteration: 134 loss: 3.13483000\n",
      "iteration: 135 loss: 1.91462624\n",
      "iteration: 136 loss: 1.61653149\n",
      "iteration: 137 loss: 2.56916714\n",
      "iteration: 138 loss: 0.84706384\n",
      "iteration: 139 loss: 1.34501612\n",
      "iteration: 140 loss: 3.49026012\n",
      "iteration: 141 loss: 1.03695703\n",
      "iteration: 142 loss: 2.36408091\n",
      "iteration: 143 loss: 2.82345986\n",
      "iteration: 144 loss: 2.59221530\n",
      "iteration: 145 loss: 2.52605247\n",
      "iteration: 146 loss: 2.49088407\n",
      "iteration: 147 loss: 1.33683693\n",
      "iteration: 148 loss: 2.55287719\n",
      "iteration: 149 loss: 3.11063170\n",
      "iteration: 150 loss: 1.68319488\n",
      "iteration: 151 loss: 2.10843801\n",
      "iteration: 152 loss: 1.84268129\n",
      "iteration: 153 loss: 4.13216591\n",
      "iteration: 154 loss: 1.10176253\n",
      "iteration: 155 loss: 2.21245909\n",
      "iteration: 156 loss: 0.79427612\n",
      "iteration: 157 loss: 0.88270098\n",
      "iteration: 158 loss: 1.69124413\n",
      "iteration: 159 loss: 1.02287555\n",
      "iteration: 160 loss: 1.40526831\n",
      "iteration: 161 loss: 2.52210784\n",
      "iteration: 162 loss: 0.79589236\n",
      "iteration: 163 loss: 3.15113616\n",
      "iteration: 164 loss: 2.14766479\n",
      "iteration: 165 loss: 3.28055716\n",
      "iteration: 166 loss: 2.42626619\n",
      "iteration: 167 loss: 2.06392837\n",
      "iteration: 168 loss: 0.57240158\n",
      "iteration: 169 loss: 2.47956920\n",
      "iteration: 170 loss: 2.93475294\n",
      "iteration: 171 loss: 2.58010554\n",
      "iteration: 172 loss: 0.64543587\n",
      "iteration: 173 loss: 1.71255827\n",
      "iteration: 174 loss: 2.51858377\n",
      "iteration: 175 loss: 3.32471275\n",
      "iteration: 176 loss: 3.06193972\n",
      "iteration: 177 loss: 1.18201458\n",
      "iteration: 178 loss: 2.04330254\n",
      "iteration: 179 loss: 1.11671650\n",
      "iteration: 180 loss: 2.89121079\n",
      "iteration: 181 loss: 1.60691559\n",
      "iteration: 182 loss: 1.68946671\n",
      "iteration: 183 loss: 2.84572577\n",
      "iteration: 184 loss: 2.24772429\n",
      "iteration: 185 loss: 3.12536883\n",
      "iteration: 186 loss: 3.23420763\n",
      "iteration: 187 loss: 1.23904860\n",
      "iteration: 188 loss: 3.36371994\n",
      "iteration: 189 loss: 2.04529023\n",
      "iteration: 190 loss: 3.73852634\n",
      "iteration: 191 loss: 3.20409346\n",
      "iteration: 192 loss: 1.80801833\n",
      "iteration: 193 loss: 2.45240784\n",
      "iteration: 194 loss: 2.12996793\n",
      "iteration: 195 loss: 2.57541466\n",
      "iteration: 196 loss: 2.02934027\n",
      "iteration: 197 loss: 2.56811595\n",
      "iteration: 198 loss: 1.12028694\n",
      "iteration: 199 loss: 1.89659226\n",
      "epoch:   8 mean loss training: 2.12803888\n",
      "epoch:   8 mean loss validation: 2.08800411\n",
      "iteration:   0 loss: 2.16907358\n",
      "iteration:   1 loss: 2.03193784\n",
      "iteration:   2 loss: 2.51785326\n",
      "iteration:   3 loss: 1.17282128\n",
      "iteration:   4 loss: 2.98203063\n",
      "iteration:   5 loss: 3.58033299\n",
      "iteration:   6 loss: 2.69039536\n",
      "iteration:   7 loss: 2.67495036\n",
      "iteration:   8 loss: 2.59755802\n",
      "iteration:   9 loss: 2.62340331\n",
      "iteration:  10 loss: 1.22175348\n",
      "iteration:  11 loss: 2.40589690\n",
      "iteration:  12 loss: 3.58061457\n",
      "iteration:  13 loss: 1.29755676\n",
      "iteration:  14 loss: 2.56935978\n",
      "iteration:  15 loss: 1.27226651\n",
      "iteration:  16 loss: 1.17347622\n",
      "iteration:  17 loss: 1.58576643\n",
      "iteration:  18 loss: 2.90190005\n",
      "iteration:  19 loss: 2.26700473\n",
      "iteration:  20 loss: 1.09966969\n",
      "iteration:  21 loss: 2.27652979\n",
      "iteration:  22 loss: 0.91860038\n",
      "iteration:  23 loss: 2.48626351\n",
      "iteration:  24 loss: 2.77414870\n",
      "iteration:  25 loss: 3.54216790\n",
      "iteration:  26 loss: 0.75437599\n",
      "iteration:  27 loss: 2.79981422\n",
      "iteration:  28 loss: 1.58589208\n",
      "iteration:  29 loss: 2.58198380\n",
      "iteration:  30 loss: 2.57797003\n",
      "iteration:  31 loss: 2.77705550\n",
      "iteration:  32 loss: 0.56487066\n",
      "iteration:  33 loss: 2.59810948\n",
      "iteration:  34 loss: 2.84145141\n",
      "iteration:  35 loss: 1.50985503\n",
      "iteration:  36 loss: 0.99775410\n",
      "iteration:  37 loss: 2.73025608\n",
      "iteration:  38 loss: 1.02041161\n",
      "iteration:  39 loss: 3.00047708\n",
      "iteration:  40 loss: 2.44043326\n",
      "iteration:  41 loss: 2.52190852\n",
      "iteration:  42 loss: 0.82949615\n",
      "iteration:  43 loss: 2.31171346\n",
      "iteration:  44 loss: 0.91469675\n",
      "iteration:  45 loss: 1.46720433\n",
      "iteration:  46 loss: 3.28792834\n",
      "iteration:  47 loss: 2.54563880\n",
      "iteration:  48 loss: 1.21300876\n",
      "iteration:  49 loss: 2.96169233\n",
      "iteration:  50 loss: 3.00510526\n",
      "iteration:  51 loss: 2.01151109\n",
      "iteration:  52 loss: 2.53116417\n",
      "iteration:  53 loss: 0.96276760\n",
      "iteration:  54 loss: 2.48742437\n",
      "iteration:  55 loss: 2.32208633\n",
      "iteration:  56 loss: 2.47335601\n",
      "iteration:  57 loss: 1.77664232\n",
      "iteration:  58 loss: 2.41659546\n",
      "iteration:  59 loss: 1.34389019\n",
      "iteration:  60 loss: 1.86471355\n",
      "iteration:  61 loss: 1.75257301\n",
      "iteration:  62 loss: 2.15961409\n",
      "iteration:  63 loss: 1.13159502\n",
      "iteration:  64 loss: 1.38330865\n",
      "iteration:  65 loss: 2.61122465\n",
      "iteration:  66 loss: 1.49879134\n",
      "iteration:  67 loss: 1.52319539\n",
      "iteration:  68 loss: 1.78353059\n",
      "iteration:  69 loss: 1.74838221\n",
      "iteration:  70 loss: 0.97726274\n",
      "iteration:  71 loss: 1.82445872\n",
      "iteration:  72 loss: 2.70941162\n",
      "iteration:  73 loss: 3.23896432\n",
      "iteration:  74 loss: 0.87618303\n",
      "iteration:  75 loss: 0.72037703\n",
      "iteration:  76 loss: 1.55685198\n",
      "iteration:  77 loss: 3.79252720\n",
      "iteration:  78 loss: 1.55188096\n",
      "iteration:  79 loss: 3.42715144\n",
      "iteration:  80 loss: 0.98044437\n",
      "iteration:  81 loss: 1.04658663\n",
      "iteration:  82 loss: 0.39218986\n",
      "iteration:  83 loss: 2.46734428\n",
      "iteration:  84 loss: 2.50551009\n",
      "iteration:  85 loss: 2.12482905\n",
      "iteration:  86 loss: 0.36161602\n",
      "iteration:  87 loss: 2.28612876\n",
      "iteration:  88 loss: 0.54462260\n",
      "iteration:  89 loss: 3.66887188\n",
      "iteration:  90 loss: 3.12361217\n",
      "iteration:  91 loss: 1.82367337\n",
      "iteration:  92 loss: 0.40907562\n",
      "iteration:  93 loss: 2.78835297\n",
      "iteration:  94 loss: 2.61025882\n",
      "iteration:  95 loss: 0.39884776\n",
      "iteration:  96 loss: 3.99287248\n",
      "iteration:  97 loss: 2.54616046\n",
      "iteration:  98 loss: 1.35836494\n",
      "iteration:  99 loss: 0.63205481\n",
      "iteration: 100 loss: 1.12389743\n",
      "iteration: 101 loss: 1.23066187\n",
      "iteration: 102 loss: 2.47597933\n",
      "iteration: 103 loss: 3.56096935\n",
      "iteration: 104 loss: 0.72260404\n",
      "iteration: 105 loss: 0.89964890\n",
      "iteration: 106 loss: 0.55284053\n",
      "iteration: 107 loss: 2.25045586\n",
      "iteration: 108 loss: 1.76541185\n",
      "iteration: 109 loss: 1.26812613\n",
      "iteration: 110 loss: 3.56625772\n",
      "iteration: 111 loss: 2.67030740\n",
      "iteration: 112 loss: 3.23565125\n",
      "iteration: 113 loss: 3.07646775\n",
      "iteration: 114 loss: 2.55242276\n",
      "iteration: 115 loss: 3.40656996\n",
      "iteration: 116 loss: 0.78036338\n",
      "iteration: 117 loss: 0.75832772\n",
      "iteration: 118 loss: 1.15310681\n",
      "iteration: 119 loss: 2.47816420\n",
      "iteration: 120 loss: 1.28611875\n",
      "iteration: 121 loss: 0.86726439\n",
      "iteration: 122 loss: 0.48567370\n",
      "iteration: 123 loss: 1.91313112\n",
      "iteration: 124 loss: 2.68307304\n",
      "iteration: 125 loss: 2.60282803\n",
      "iteration: 126 loss: 3.70351124\n",
      "iteration: 127 loss: 2.35912800\n",
      "iteration: 128 loss: 2.27164125\n",
      "iteration: 129 loss: 1.30714643\n",
      "iteration: 130 loss: 1.87883210\n",
      "iteration: 131 loss: 2.98108172\n",
      "iteration: 132 loss: 4.22574472\n",
      "iteration: 133 loss: 2.93248773\n",
      "iteration: 134 loss: 2.97625279\n",
      "iteration: 135 loss: 1.51085722\n",
      "iteration: 136 loss: 1.36447453\n",
      "iteration: 137 loss: 2.36777544\n",
      "iteration: 138 loss: 0.76713616\n",
      "iteration: 139 loss: 1.07123387\n",
      "iteration: 140 loss: 3.50675941\n",
      "iteration: 141 loss: 0.92641848\n",
      "iteration: 142 loss: 2.43738484\n",
      "iteration: 143 loss: 2.86633205\n",
      "iteration: 144 loss: 2.74681234\n",
      "iteration: 145 loss: 2.58272696\n",
      "iteration: 146 loss: 2.57337475\n",
      "iteration: 147 loss: 1.19471347\n",
      "iteration: 148 loss: 2.61495876\n",
      "iteration: 149 loss: 3.20900607\n",
      "iteration: 150 loss: 1.65447569\n",
      "iteration: 151 loss: 1.98295546\n",
      "iteration: 152 loss: 1.81650984\n",
      "iteration: 153 loss: 4.18130159\n",
      "iteration: 154 loss: 1.01281655\n",
      "iteration: 155 loss: 2.37519050\n",
      "iteration: 156 loss: 0.65318024\n",
      "iteration: 157 loss: 0.75998890\n",
      "iteration: 158 loss: 1.43598020\n",
      "iteration: 159 loss: 0.61364001\n",
      "iteration: 160 loss: 1.36661696\n",
      "iteration: 161 loss: 2.36048174\n",
      "iteration: 162 loss: 0.64338994\n",
      "iteration: 163 loss: 3.24826503\n",
      "iteration: 164 loss: 2.21297622\n",
      "iteration: 165 loss: 3.38659406\n",
      "iteration: 166 loss: 2.48024178\n",
      "iteration: 167 loss: 1.83296621\n",
      "iteration: 168 loss: 0.51282102\n",
      "iteration: 169 loss: 2.11152601\n",
      "iteration: 170 loss: 2.86767650\n",
      "iteration: 171 loss: 2.42329741\n",
      "iteration: 172 loss: 0.57683605\n",
      "iteration: 173 loss: 1.12865031\n",
      "iteration: 174 loss: 2.70406437\n",
      "iteration: 175 loss: 3.40972948\n",
      "iteration: 176 loss: 3.02804184\n",
      "iteration: 177 loss: 0.95262355\n",
      "iteration: 178 loss: 2.03477454\n",
      "iteration: 179 loss: 0.90488511\n",
      "iteration: 180 loss: 3.09265900\n",
      "iteration: 181 loss: 1.58339190\n",
      "iteration: 182 loss: 1.62698030\n",
      "iteration: 183 loss: 2.92193675\n",
      "iteration: 184 loss: 2.02169132\n",
      "iteration: 185 loss: 3.13265848\n",
      "iteration: 186 loss: 3.17361236\n",
      "iteration: 187 loss: 0.88127196\n",
      "iteration: 188 loss: 3.38890147\n",
      "iteration: 189 loss: 1.93649805\n",
      "iteration: 190 loss: 3.87587571\n",
      "iteration: 191 loss: 3.29872417\n",
      "iteration: 192 loss: 1.55814600\n",
      "iteration: 193 loss: 2.41934490\n",
      "iteration: 194 loss: 1.81548774\n",
      "iteration: 195 loss: 2.72408915\n",
      "iteration: 196 loss: 2.01091599\n",
      "iteration: 197 loss: 2.65668607\n",
      "iteration: 198 loss: 0.80519807\n",
      "iteration: 199 loss: 1.48223710\n",
      "epoch:   9 mean loss training: 2.05796099\n",
      "epoch:   9 mean loss validation: 1.97181773\n",
      "iteration:   0 loss: 1.79171860\n",
      "iteration:   1 loss: 1.97305441\n",
      "iteration:   2 loss: 2.38798141\n",
      "iteration:   3 loss: 0.85303420\n",
      "iteration:   4 loss: 3.21543050\n",
      "iteration:   5 loss: 3.65921474\n",
      "iteration:   6 loss: 2.72908783\n",
      "iteration:   7 loss: 2.42541885\n",
      "iteration:   8 loss: 2.70531750\n",
      "iteration:   9 loss: 2.83484602\n",
      "iteration:  10 loss: 0.99573010\n",
      "iteration:  11 loss: 2.30084467\n",
      "iteration:  12 loss: 3.60406494\n",
      "iteration:  13 loss: 1.02653754\n",
      "iteration:  14 loss: 2.58437061\n",
      "iteration:  15 loss: 0.94277900\n",
      "iteration:  16 loss: 0.91083866\n",
      "iteration:  17 loss: 1.31723452\n",
      "iteration:  18 loss: 2.74029875\n",
      "iteration:  19 loss: 2.33229399\n",
      "iteration:  20 loss: 0.83021080\n",
      "iteration:  21 loss: 2.38428593\n",
      "iteration:  22 loss: 0.68496931\n",
      "iteration:  23 loss: 2.39583731\n",
      "iteration:  24 loss: 2.81095839\n",
      "iteration:  25 loss: 3.55957699\n",
      "iteration:  26 loss: 0.68397450\n",
      "iteration:  27 loss: 2.78209186\n",
      "iteration:  28 loss: 1.34800470\n",
      "iteration:  29 loss: 2.58193183\n",
      "iteration:  30 loss: 2.39744663\n",
      "iteration:  31 loss: 2.70676875\n",
      "iteration:  32 loss: 0.49067545\n",
      "iteration:  33 loss: 2.25161386\n",
      "iteration:  34 loss: 2.80411482\n",
      "iteration:  35 loss: 1.18042994\n",
      "iteration:  36 loss: 0.55691040\n",
      "iteration:  37 loss: 2.76886678\n",
      "iteration:  38 loss: 1.07825673\n",
      "iteration:  39 loss: 2.82252955\n",
      "iteration:  40 loss: 2.49038148\n",
      "iteration:  41 loss: 2.49250412\n",
      "iteration:  42 loss: 0.88522130\n",
      "iteration:  43 loss: 1.99870765\n",
      "iteration:  44 loss: 0.91540432\n",
      "iteration:  45 loss: 0.77512908\n",
      "iteration:  46 loss: 3.40871835\n",
      "iteration:  47 loss: 2.61562085\n",
      "iteration:  48 loss: 1.27321768\n",
      "iteration:  49 loss: 2.92196560\n",
      "iteration:  50 loss: 2.90181422\n",
      "iteration:  51 loss: 1.88997746\n",
      "iteration:  52 loss: 2.60375404\n",
      "iteration:  53 loss: 0.90130472\n",
      "iteration:  54 loss: 2.14432192\n",
      "iteration:  55 loss: 2.10663414\n",
      "iteration:  56 loss: 2.57293344\n",
      "iteration:  57 loss: 1.52611232\n",
      "iteration:  58 loss: 2.28167820\n",
      "iteration:  59 loss: 1.31328750\n",
      "iteration:  60 loss: 1.86567879\n",
      "iteration:  61 loss: 1.73544371\n",
      "iteration:  62 loss: 2.22221565\n",
      "iteration:  63 loss: 0.99480504\n",
      "iteration:  64 loss: 1.28629816\n",
      "iteration:  65 loss: 2.71586180\n",
      "iteration:  66 loss: 1.49043214\n",
      "iteration:  67 loss: 1.35581207\n",
      "iteration:  68 loss: 1.66038430\n",
      "iteration:  69 loss: 1.53959763\n",
      "iteration:  70 loss: 0.79092580\n",
      "iteration:  71 loss: 1.76956880\n",
      "iteration:  72 loss: 2.54957676\n",
      "iteration:  73 loss: 3.46654201\n",
      "iteration:  74 loss: 0.58940619\n",
      "iteration:  75 loss: 0.51447296\n",
      "iteration:  76 loss: 1.34725380\n",
      "iteration:  77 loss: 3.93147302\n",
      "iteration:  78 loss: 1.45389473\n",
      "iteration:  79 loss: 3.64792132\n",
      "iteration:  80 loss: 0.88111728\n",
      "iteration:  81 loss: 0.84933561\n",
      "iteration:  82 loss: 0.30583042\n",
      "iteration:  83 loss: 2.38526845\n",
      "iteration:  84 loss: 2.68694258\n",
      "iteration:  85 loss: 1.84954011\n",
      "iteration:  86 loss: 0.27726749\n",
      "iteration:  87 loss: 2.47524071\n",
      "iteration:  88 loss: 0.46719682\n",
      "iteration:  89 loss: 3.72395468\n",
      "iteration:  90 loss: 3.01998901\n",
      "iteration:  91 loss: 1.48476517\n",
      "iteration:  92 loss: 0.37734920\n",
      "iteration:  93 loss: 2.80653334\n",
      "iteration:  94 loss: 2.31802249\n",
      "iteration:  95 loss: 0.32770261\n",
      "iteration:  96 loss: 4.11787987\n",
      "iteration:  97 loss: 2.11695313\n",
      "iteration:  98 loss: 1.09627938\n",
      "iteration:  99 loss: 0.68095475\n",
      "iteration: 100 loss: 0.89247626\n",
      "iteration: 101 loss: 0.89084560\n",
      "iteration: 102 loss: 2.62359428\n",
      "iteration: 103 loss: 3.54268622\n",
      "iteration: 104 loss: 0.62456757\n",
      "iteration: 105 loss: 0.97606319\n",
      "iteration: 106 loss: 0.51599687\n",
      "iteration: 107 loss: 2.14958119\n",
      "iteration: 108 loss: 1.96938288\n",
      "iteration: 109 loss: 1.36232710\n",
      "iteration: 110 loss: 3.58171821\n",
      "iteration: 111 loss: 2.62237382\n",
      "iteration: 112 loss: 3.35798192\n",
      "iteration: 113 loss: 3.20728993\n",
      "iteration: 114 loss: 2.51871538\n",
      "iteration: 115 loss: 3.45886922\n",
      "iteration: 116 loss: 0.84105760\n",
      "iteration: 117 loss: 0.62422025\n",
      "iteration: 118 loss: 0.99221152\n",
      "iteration: 119 loss: 2.35097265\n",
      "iteration: 120 loss: 1.25628638\n",
      "iteration: 121 loss: 0.79782611\n",
      "iteration: 122 loss: 0.43369478\n",
      "iteration: 123 loss: 1.83569193\n",
      "iteration: 124 loss: 2.63321066\n",
      "iteration: 125 loss: 2.57044649\n",
      "iteration: 126 loss: 3.67307782\n",
      "iteration: 127 loss: 2.23823190\n",
      "iteration: 128 loss: 2.13747764\n",
      "iteration: 129 loss: 1.08907342\n",
      "iteration: 130 loss: 1.86320174\n",
      "iteration: 131 loss: 2.82511544\n",
      "iteration: 132 loss: 4.17572498\n",
      "iteration: 133 loss: 2.98668981\n",
      "iteration: 134 loss: 2.87052441\n",
      "iteration: 135 loss: 1.33215928\n",
      "iteration: 136 loss: 1.21356463\n",
      "iteration: 137 loss: 2.29096842\n",
      "iteration: 138 loss: 0.66420490\n",
      "iteration: 139 loss: 0.86891425\n",
      "iteration: 140 loss: 3.45839500\n",
      "iteration: 141 loss: 0.70693517\n",
      "iteration: 142 loss: 2.56048632\n",
      "iteration: 143 loss: 2.70455861\n",
      "iteration: 144 loss: 2.84575200\n",
      "iteration: 145 loss: 2.59904504\n",
      "iteration: 146 loss: 2.59986639\n",
      "iteration: 147 loss: 1.11324632\n",
      "iteration: 148 loss: 2.64287400\n",
      "iteration: 149 loss: 3.11954498\n",
      "iteration: 150 loss: 1.42570662\n",
      "iteration: 151 loss: 1.90191483\n",
      "iteration: 152 loss: 1.93294668\n",
      "iteration: 153 loss: 4.24165010\n",
      "iteration: 154 loss: 0.91130811\n",
      "iteration: 155 loss: 2.69190931\n",
      "iteration: 156 loss: 0.54138225\n",
      "iteration: 157 loss: 0.64758956\n",
      "iteration: 158 loss: 1.25948572\n",
      "iteration: 159 loss: 0.42209139\n",
      "iteration: 160 loss: 1.37244904\n",
      "iteration: 161 loss: 2.14194584\n",
      "iteration: 162 loss: 0.55348140\n",
      "iteration: 163 loss: 3.21795082\n",
      "iteration: 164 loss: 2.38344908\n",
      "iteration: 165 loss: 3.39838815\n",
      "iteration: 166 loss: 2.50875521\n",
      "iteration: 167 loss: 1.60939109\n",
      "iteration: 168 loss: 0.53445524\n",
      "iteration: 169 loss: 1.86565292\n",
      "iteration: 170 loss: 2.85507584\n",
      "iteration: 171 loss: 2.25862885\n",
      "iteration: 172 loss: 0.56221020\n",
      "iteration: 173 loss: 0.87339282\n",
      "iteration: 174 loss: 2.87750483\n",
      "iteration: 175 loss: 3.53018594\n",
      "iteration: 176 loss: 2.94310617\n",
      "iteration: 177 loss: 0.89143282\n",
      "iteration: 178 loss: 2.23812580\n",
      "iteration: 179 loss: 0.79997087\n",
      "iteration: 180 loss: 3.11302829\n",
      "iteration: 181 loss: 1.38988924\n",
      "iteration: 182 loss: 1.42078328\n",
      "iteration: 183 loss: 2.75612497\n",
      "iteration: 184 loss: 2.04059505\n",
      "iteration: 185 loss: 2.97679949\n",
      "iteration: 186 loss: 3.08971214\n",
      "iteration: 187 loss: 0.69350767\n",
      "iteration: 188 loss: 3.28558493\n",
      "iteration: 189 loss: 2.04316545\n",
      "iteration: 190 loss: 3.77833557\n",
      "iteration: 191 loss: 3.15004706\n",
      "iteration: 192 loss: 1.36655581\n",
      "iteration: 193 loss: 2.32651067\n",
      "iteration: 194 loss: 1.54377794\n",
      "iteration: 195 loss: 2.82352924\n",
      "iteration: 196 loss: 2.09092689\n",
      "iteration: 197 loss: 2.67933679\n",
      "iteration: 198 loss: 0.73373860\n",
      "iteration: 199 loss: 1.32745147\n",
      "epoch:  10 mean loss training: 1.98437285\n",
      "epoch:  10 mean loss validation: 1.92071033\n",
      "iteration:   0 loss: 1.61812890\n",
      "iteration:   1 loss: 1.94256794\n",
      "iteration:   2 loss: 2.30976772\n",
      "iteration:   3 loss: 0.71674114\n",
      "iteration:   4 loss: 3.23404884\n",
      "iteration:   5 loss: 3.58461523\n",
      "iteration:   6 loss: 2.66977167\n",
      "iteration:   7 loss: 2.20580482\n",
      "iteration:   8 loss: 2.68103123\n",
      "iteration:   9 loss: 2.86248040\n",
      "iteration:  10 loss: 0.92875582\n",
      "iteration:  11 loss: 2.20799756\n",
      "iteration:  12 loss: 3.45744276\n",
      "iteration:  13 loss: 0.93155140\n",
      "iteration:  14 loss: 2.59643316\n",
      "iteration:  15 loss: 0.88769031\n",
      "iteration:  16 loss: 0.83236992\n",
      "iteration:  17 loss: 1.19231808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  18 loss: 2.55666256\n",
      "iteration:  19 loss: 2.37512851\n",
      "iteration:  20 loss: 0.71453100\n",
      "iteration:  21 loss: 2.33226728\n",
      "iteration:  22 loss: 0.60531825\n",
      "iteration:  23 loss: 2.21135068\n",
      "iteration:  24 loss: 2.78800750\n",
      "iteration:  25 loss: 3.47629571\n",
      "iteration:  26 loss: 0.72602946\n",
      "iteration:  27 loss: 2.77541661\n",
      "iteration:  28 loss: 1.01220322\n",
      "iteration:  29 loss: 2.55399275\n",
      "iteration:  30 loss: 2.14178944\n",
      "iteration:  31 loss: 2.53604412\n",
      "iteration:  32 loss: 0.48640549\n",
      "iteration:  33 loss: 1.97484338\n",
      "iteration:  34 loss: 2.76422977\n",
      "iteration:  35 loss: 0.83387548\n",
      "iteration:  36 loss: 0.46980631\n",
      "iteration:  37 loss: 2.83288097\n",
      "iteration:  38 loss: 1.18241704\n",
      "iteration:  39 loss: 2.77381063\n",
      "iteration:  40 loss: 2.46923399\n",
      "iteration:  41 loss: 2.57504511\n",
      "iteration:  42 loss: 0.91193718\n",
      "iteration:  43 loss: 1.72804976\n",
      "iteration:  44 loss: 0.86356366\n",
      "iteration:  45 loss: 0.68260831\n",
      "iteration:  46 loss: 3.48382258\n",
      "iteration:  47 loss: 2.59661198\n",
      "iteration:  48 loss: 1.23271906\n",
      "iteration:  49 loss: 3.04356909\n",
      "iteration:  50 loss: 2.78353333\n",
      "iteration:  51 loss: 1.89162397\n",
      "iteration:  52 loss: 2.60833144\n",
      "iteration:  53 loss: 0.86424452\n",
      "iteration:  54 loss: 1.92456686\n",
      "iteration:  55 loss: 2.19475746\n",
      "iteration:  56 loss: 2.40821719\n",
      "iteration:  57 loss: 1.40845966\n",
      "iteration:  58 loss: 2.19061923\n",
      "iteration:  59 loss: 1.23859930\n",
      "iteration:  60 loss: 1.88786709\n",
      "iteration:  61 loss: 1.76753998\n",
      "iteration:  62 loss: 2.31928778\n",
      "iteration:  63 loss: 0.94357234\n",
      "iteration:  64 loss: 1.25674212\n",
      "iteration:  65 loss: 2.76015854\n",
      "iteration:  66 loss: 1.49150419\n",
      "iteration:  67 loss: 1.31582141\n",
      "iteration:  68 loss: 1.53246796\n",
      "iteration:  69 loss: 1.37862968\n",
      "iteration:  70 loss: 0.72031903\n",
      "iteration:  71 loss: 1.99068320\n",
      "iteration:  72 loss: 2.41024399\n",
      "iteration:  73 loss: 3.51395130\n",
      "iteration:  74 loss: 0.56265944\n",
      "iteration:  75 loss: 0.44220963\n",
      "iteration:  76 loss: 1.32669699\n",
      "iteration:  77 loss: 3.86444569\n",
      "iteration:  78 loss: 1.37317014\n",
      "iteration:  79 loss: 3.61378527\n",
      "iteration:  80 loss: 1.03526902\n",
      "iteration:  81 loss: 0.71272951\n",
      "iteration:  82 loss: 0.31619805\n",
      "iteration:  83 loss: 2.27858782\n",
      "iteration:  84 loss: 2.76623893\n",
      "iteration:  85 loss: 1.53102612\n",
      "iteration:  86 loss: 0.30289966\n",
      "iteration:  87 loss: 2.58553505\n",
      "iteration:  88 loss: 0.47097179\n",
      "iteration:  89 loss: 3.63085747\n",
      "iteration:  90 loss: 2.87811375\n",
      "iteration:  91 loss: 1.24720633\n",
      "iteration:  92 loss: 0.39639905\n",
      "iteration:  93 loss: 2.80387259\n",
      "iteration:  94 loss: 2.23490667\n",
      "iteration:  95 loss: 0.30989707\n",
      "iteration:  96 loss: 4.06032372\n",
      "iteration:  97 loss: 1.99403226\n",
      "iteration:  98 loss: 0.97184378\n",
      "iteration:  99 loss: 0.68859202\n",
      "iteration: 100 loss: 0.88009602\n",
      "iteration: 101 loss: 0.78424168\n",
      "iteration: 102 loss: 2.79458547\n",
      "iteration: 103 loss: 3.44708920\n",
      "iteration: 104 loss: 0.54774880\n",
      "iteration: 105 loss: 0.99148577\n",
      "iteration: 106 loss: 0.48384586\n",
      "iteration: 107 loss: 2.15227437\n",
      "iteration: 108 loss: 1.99837339\n",
      "iteration: 109 loss: 1.41952813\n",
      "iteration: 110 loss: 3.54147172\n",
      "iteration: 111 loss: 2.60823607\n",
      "iteration: 112 loss: 3.36183906\n",
      "iteration: 113 loss: 3.19318533\n",
      "iteration: 114 loss: 2.41169786\n",
      "iteration: 115 loss: 3.46225858\n",
      "iteration: 116 loss: 0.86402398\n",
      "iteration: 117 loss: 0.57071280\n",
      "iteration: 118 loss: 1.09038222\n",
      "iteration: 119 loss: 2.18156958\n",
      "iteration: 120 loss: 1.19883394\n",
      "iteration: 121 loss: 0.77681166\n",
      "iteration: 122 loss: 0.42883113\n",
      "iteration: 123 loss: 1.86412013\n",
      "iteration: 124 loss: 2.53006268\n",
      "iteration: 125 loss: 2.50581765\n",
      "iteration: 126 loss: 3.61272740\n",
      "iteration: 127 loss: 2.02759337\n",
      "iteration: 128 loss: 2.03148437\n",
      "iteration: 129 loss: 1.01326442\n",
      "iteration: 130 loss: 1.90042555\n",
      "iteration: 131 loss: 2.69164515\n",
      "iteration: 132 loss: 4.01161003\n",
      "iteration: 133 loss: 2.96408343\n",
      "iteration: 134 loss: 2.75934935\n",
      "iteration: 135 loss: 1.21569216\n",
      "iteration: 136 loss: 1.07647967\n",
      "iteration: 137 loss: 2.18069506\n",
      "iteration: 138 loss: 0.68830371\n",
      "iteration: 139 loss: 0.83702022\n",
      "iteration: 140 loss: 3.38205743\n",
      "iteration: 141 loss: 0.68363798\n",
      "iteration: 142 loss: 2.54005075\n",
      "iteration: 143 loss: 2.59655285\n",
      "iteration: 144 loss: 2.79630709\n",
      "iteration: 145 loss: 2.52146840\n",
      "iteration: 146 loss: 2.51188993\n",
      "iteration: 147 loss: 1.17211378\n",
      "iteration: 148 loss: 2.52588725\n",
      "iteration: 149 loss: 3.00489092\n",
      "iteration: 150 loss: 1.39284468\n",
      "iteration: 151 loss: 1.79412782\n",
      "iteration: 152 loss: 2.10190487\n",
      "iteration: 153 loss: 4.10072041\n",
      "iteration: 154 loss: 0.96930689\n",
      "iteration: 155 loss: 2.70446754\n",
      "iteration: 156 loss: 0.63106215\n",
      "iteration: 157 loss: 0.73938584\n",
      "iteration: 158 loss: 1.09859061\n",
      "iteration: 159 loss: 0.48935387\n",
      "iteration: 160 loss: 1.56560922\n",
      "iteration: 161 loss: 2.02151680\n",
      "iteration: 162 loss: 0.56088531\n",
      "iteration: 163 loss: 3.13739800\n",
      "iteration: 164 loss: 2.65490007\n",
      "iteration: 165 loss: 3.35376549\n",
      "iteration: 166 loss: 2.36359882\n",
      "iteration: 167 loss: 1.64801800\n",
      "iteration: 168 loss: 0.63875663\n",
      "iteration: 169 loss: 1.81017768\n",
      "iteration: 170 loss: 2.83550096\n",
      "iteration: 171 loss: 2.19767570\n",
      "iteration: 172 loss: 0.59894317\n",
      "iteration: 173 loss: 0.89558125\n",
      "iteration: 174 loss: 3.12710047\n",
      "iteration: 175 loss: 3.65057039\n",
      "iteration: 176 loss: 2.83886719\n",
      "iteration: 177 loss: 0.93580872\n",
      "iteration: 178 loss: 2.35417390\n",
      "iteration: 179 loss: 0.68789089\n",
      "iteration: 180 loss: 2.94838548\n",
      "iteration: 181 loss: 1.09821928\n",
      "iteration: 182 loss: 1.06938827\n",
      "iteration: 183 loss: 2.58013916\n",
      "iteration: 184 loss: 2.08976626\n",
      "iteration: 185 loss: 3.02125788\n",
      "iteration: 186 loss: 3.04859972\n",
      "iteration: 187 loss: 0.62736094\n",
      "iteration: 188 loss: 3.31297946\n",
      "iteration: 189 loss: 2.39898205\n",
      "iteration: 190 loss: 3.63603783\n",
      "iteration: 191 loss: 3.10201335\n",
      "iteration: 192 loss: 1.19842923\n",
      "iteration: 193 loss: 2.26739120\n",
      "iteration: 194 loss: 1.40375853\n",
      "iteration: 195 loss: 2.93197513\n",
      "iteration: 196 loss: 2.26089191\n",
      "iteration: 197 loss: 2.70447183\n",
      "iteration: 198 loss: 0.75999564\n",
      "iteration: 199 loss: 1.30471373\n",
      "epoch:  11 mean loss training: 1.94133902\n",
      "epoch:  11 mean loss validation: 1.89434862\n",
      "iteration:   0 loss: 1.53782904\n",
      "iteration:   1 loss: 1.91796660\n",
      "iteration:   2 loss: 2.24163890\n",
      "iteration:   3 loss: 0.64820063\n",
      "iteration:   4 loss: 3.21661949\n",
      "iteration:   5 loss: 3.49163532\n",
      "iteration:   6 loss: 2.66859031\n",
      "iteration:   7 loss: 2.09718585\n",
      "iteration:   8 loss: 2.66393042\n",
      "iteration:   9 loss: 2.96197367\n",
      "iteration:  10 loss: 0.93791938\n",
      "iteration:  11 loss: 2.11837339\n",
      "iteration:  12 loss: 3.32500315\n",
      "iteration:  13 loss: 0.90097755\n",
      "iteration:  14 loss: 2.56591558\n",
      "iteration:  15 loss: 0.89094704\n",
      "iteration:  16 loss: 0.83777237\n",
      "iteration:  17 loss: 1.19593632\n",
      "iteration:  18 loss: 2.49502587\n",
      "iteration:  19 loss: 2.33674955\n",
      "iteration:  20 loss: 0.77403080\n",
      "iteration:  21 loss: 2.33291197\n",
      "iteration:  22 loss: 0.70215666\n",
      "iteration:  23 loss: 2.10848188\n",
      "iteration:  24 loss: 2.56194234\n",
      "iteration:  25 loss: 3.36507654\n",
      "iteration:  26 loss: 0.88827825\n",
      "iteration:  27 loss: 2.72551441\n",
      "iteration:  28 loss: 0.88457340\n",
      "iteration:  29 loss: 2.49617696\n",
      "iteration:  30 loss: 1.97583163\n",
      "iteration:  31 loss: 2.37303734\n",
      "iteration:  32 loss: 0.55891401\n",
      "iteration:  33 loss: 1.88117814\n",
      "iteration:  34 loss: 2.68105292\n",
      "iteration:  35 loss: 0.78128928\n",
      "iteration:  36 loss: 0.48333538\n",
      "iteration:  37 loss: 2.68644977\n",
      "iteration:  38 loss: 1.22072875\n",
      "iteration:  39 loss: 2.66282582\n",
      "iteration:  40 loss: 2.45555329\n",
      "iteration:  41 loss: 2.59182763\n",
      "iteration:  42 loss: 0.91061896\n",
      "iteration:  43 loss: 1.95688045\n",
      "iteration:  44 loss: 0.78853297\n",
      "iteration:  45 loss: 0.64776134\n",
      "iteration:  46 loss: 3.55205536\n",
      "iteration:  47 loss: 2.46083307\n",
      "iteration:  48 loss: 1.07342899\n",
      "iteration:  49 loss: 3.06092715\n",
      "iteration:  50 loss: 2.74827695\n",
      "iteration:  51 loss: 1.97478747\n",
      "iteration:  52 loss: 2.51950026\n",
      "iteration:  53 loss: 0.81826699\n",
      "iteration:  54 loss: 1.83384991\n",
      "iteration:  55 loss: 2.16576815\n",
      "iteration:  56 loss: 2.43906856\n",
      "iteration:  57 loss: 1.39767313\n",
      "iteration:  58 loss: 2.09497213\n",
      "iteration:  59 loss: 1.24708295\n",
      "iteration:  60 loss: 1.98337233\n",
      "iteration:  61 loss: 1.93802035\n",
      "iteration:  62 loss: 2.37592363\n",
      "iteration:  63 loss: 0.93930870\n",
      "iteration:  64 loss: 1.16049278\n",
      "iteration:  65 loss: 2.74934149\n",
      "iteration:  66 loss: 1.47094607\n",
      "iteration:  67 loss: 1.27885890\n",
      "iteration:  68 loss: 1.52104497\n",
      "iteration:  69 loss: 1.38174021\n",
      "iteration:  70 loss: 0.74137747\n",
      "iteration:  71 loss: 2.01246691\n",
      "iteration:  72 loss: 2.35130143\n",
      "iteration:  73 loss: 3.42601943\n",
      "iteration:  74 loss: 0.61147386\n",
      "iteration:  75 loss: 0.49444243\n",
      "iteration:  76 loss: 1.29050863\n",
      "iteration:  77 loss: 3.75986195\n",
      "iteration:  78 loss: 1.29345226\n",
      "iteration:  79 loss: 3.54222250\n",
      "iteration:  80 loss: 1.13805842\n",
      "iteration:  81 loss: 0.58366036\n",
      "iteration:  82 loss: 0.35447878\n",
      "iteration:  83 loss: 2.16465759\n",
      "iteration:  84 loss: 2.70475364\n",
      "iteration:  85 loss: 1.46974087\n",
      "iteration:  86 loss: 0.33726853\n",
      "iteration:  87 loss: 2.63399458\n",
      "iteration:  88 loss: 0.47252092\n",
      "iteration:  89 loss: 3.54775715\n",
      "iteration:  90 loss: 2.83342195\n",
      "iteration:  91 loss: 1.34995306\n",
      "iteration:  92 loss: 0.41018036\n",
      "iteration:  93 loss: 2.75221157\n",
      "iteration:  94 loss: 2.21314645\n",
      "iteration:  95 loss: 0.31786433\n",
      "iteration:  96 loss: 4.01553822\n",
      "iteration:  97 loss: 1.95153260\n",
      "iteration:  98 loss: 1.02900767\n",
      "iteration:  99 loss: 0.67883354\n",
      "iteration: 100 loss: 0.82062143\n",
      "iteration: 101 loss: 0.65506488\n",
      "iteration: 102 loss: 2.82560635\n",
      "iteration: 103 loss: 3.39851093\n",
      "iteration: 104 loss: 0.48426011\n",
      "iteration: 105 loss: 0.92962188\n",
      "iteration: 106 loss: 0.41423428\n",
      "iteration: 107 loss: 2.15317988\n",
      "iteration: 108 loss: 1.98588622\n",
      "iteration: 109 loss: 1.39809275\n",
      "iteration: 110 loss: 3.51088619\n",
      "iteration: 111 loss: 2.63117290\n",
      "iteration: 112 loss: 3.39971614\n",
      "iteration: 113 loss: 3.13401937\n",
      "iteration: 114 loss: 2.38973713\n",
      "iteration: 115 loss: 3.47850132\n",
      "iteration: 116 loss: 0.88166469\n",
      "iteration: 117 loss: 0.54716665\n",
      "iteration: 118 loss: 0.98132497\n",
      "iteration: 119 loss: 2.15286732\n",
      "iteration: 120 loss: 1.01174152\n",
      "iteration: 121 loss: 0.74099934\n",
      "iteration: 122 loss: 0.40992656\n",
      "iteration: 123 loss: 1.83279908\n",
      "iteration: 124 loss: 2.45415831\n",
      "iteration: 125 loss: 2.45148492\n",
      "iteration: 126 loss: 3.60857725\n",
      "iteration: 127 loss: 1.78937232\n",
      "iteration: 128 loss: 2.00500607\n",
      "iteration: 129 loss: 1.00415862\n",
      "iteration: 130 loss: 2.00114179\n",
      "iteration: 131 loss: 2.65147257\n",
      "iteration: 132 loss: 3.94929790\n",
      "iteration: 133 loss: 3.00500298\n",
      "iteration: 134 loss: 2.69152880\n",
      "iteration: 135 loss: 1.14847112\n",
      "iteration: 136 loss: 1.05251670\n",
      "iteration: 137 loss: 2.10164857\n",
      "iteration: 138 loss: 0.64618099\n",
      "iteration: 139 loss: 0.84177101\n",
      "iteration: 140 loss: 3.35300803\n",
      "iteration: 141 loss: 0.62753999\n",
      "iteration: 142 loss: 2.53830528\n",
      "iteration: 143 loss: 2.50844789\n",
      "iteration: 144 loss: 2.82398629\n",
      "iteration: 145 loss: 2.50517893\n",
      "iteration: 146 loss: 2.51820374\n",
      "iteration: 147 loss: 1.13898563\n",
      "iteration: 148 loss: 2.43842435\n",
      "iteration: 149 loss: 2.99585199\n",
      "iteration: 150 loss: 1.24707472\n",
      "iteration: 151 loss: 1.56000543\n",
      "iteration: 152 loss: 2.34247303\n",
      "iteration: 153 loss: 4.21167088\n",
      "iteration: 154 loss: 0.90435117\n",
      "iteration: 155 loss: 2.80880523\n",
      "iteration: 156 loss: 0.59300452\n",
      "iteration: 157 loss: 0.70451885\n",
      "iteration: 158 loss: 0.99624199\n",
      "iteration: 159 loss: 0.49611297\n",
      "iteration: 160 loss: 1.42347848\n",
      "iteration: 161 loss: 2.01262379\n",
      "iteration: 162 loss: 0.53466094\n",
      "iteration: 163 loss: 3.14261913\n",
      "iteration: 164 loss: 2.61492062\n",
      "iteration: 165 loss: 3.32109261\n",
      "iteration: 166 loss: 2.30935693\n",
      "iteration: 167 loss: 1.62594450\n",
      "iteration: 168 loss: 0.63970888\n",
      "iteration: 169 loss: 1.66914153\n",
      "iteration: 170 loss: 2.72258091\n",
      "iteration: 171 loss: 2.11494756\n",
      "iteration: 172 loss: 0.61402541\n",
      "iteration: 173 loss: 0.76365632\n",
      "iteration: 174 loss: 3.03863382\n",
      "iteration: 175 loss: 3.51588416\n",
      "iteration: 176 loss: 2.78207517\n",
      "iteration: 177 loss: 0.99275357\n",
      "iteration: 178 loss: 2.36523485\n",
      "iteration: 179 loss: 0.69508040\n",
      "iteration: 180 loss: 2.92112136\n",
      "iteration: 181 loss: 1.12944376\n",
      "iteration: 182 loss: 1.09453142\n",
      "iteration: 183 loss: 2.56069040\n",
      "iteration: 184 loss: 2.05949950\n",
      "iteration: 185 loss: 2.89045382\n",
      "iteration: 186 loss: 2.95383883\n",
      "iteration: 187 loss: 0.62036455\n",
      "iteration: 188 loss: 3.28470087\n",
      "iteration: 189 loss: 2.45266819\n",
      "iteration: 190 loss: 3.57964182\n",
      "iteration: 191 loss: 3.02325630\n",
      "iteration: 192 loss: 1.09419572\n",
      "iteration: 193 loss: 2.24418902\n",
      "iteration: 194 loss: 1.25472331\n",
      "iteration: 195 loss: 3.02102590\n",
      "iteration: 196 loss: 2.32418585\n",
      "iteration: 197 loss: 2.82248330\n",
      "iteration: 198 loss: 0.77516043\n",
      "iteration: 199 loss: 1.24548817\n",
      "epoch:  12 mean loss training: 1.91172028\n",
      "epoch:  12 mean loss validation: 1.87630391\n",
      "iteration:   0 loss: 1.43935466\n",
      "iteration:   1 loss: 1.88429034\n",
      "iteration:   2 loss: 2.21230078\n",
      "iteration:   3 loss: 0.58933634\n",
      "iteration:   4 loss: 3.16094804\n",
      "iteration:   5 loss: 3.43567276\n",
      "iteration:   6 loss: 2.66125464\n",
      "iteration:   7 loss: 2.01752400\n",
      "iteration:   8 loss: 2.72311854\n",
      "iteration:   9 loss: 2.88321424\n",
      "iteration:  10 loss: 0.86648279\n",
      "iteration:  11 loss: 2.12688923\n",
      "iteration:  12 loss: 3.24689269\n",
      "iteration:  13 loss: 0.84725559\n",
      "iteration:  14 loss: 2.58193016\n",
      "iteration:  15 loss: 0.91572911\n",
      "iteration:  16 loss: 0.78560615\n",
      "iteration:  17 loss: 1.22306621\n",
      "iteration:  18 loss: 2.43827438\n",
      "iteration:  19 loss: 2.32461190\n",
      "iteration:  20 loss: 0.74959958\n",
      "iteration:  21 loss: 2.38078809\n",
      "iteration:  22 loss: 0.72734076\n",
      "iteration:  23 loss: 1.93820798\n",
      "iteration:  24 loss: 2.41451478\n",
      "iteration:  25 loss: 3.29498339\n",
      "iteration:  26 loss: 0.91876823\n",
      "iteration:  27 loss: 2.68456960\n",
      "iteration:  28 loss: 0.66971761\n",
      "iteration:  29 loss: 2.44687891\n",
      "iteration:  30 loss: 1.97499645\n",
      "iteration:  31 loss: 2.28734016\n",
      "iteration:  32 loss: 0.53648812\n",
      "iteration:  33 loss: 1.87503183\n",
      "iteration:  34 loss: 2.71265340\n",
      "iteration:  35 loss: 0.84183222\n",
      "iteration:  36 loss: 0.50768387\n",
      "iteration:  37 loss: 2.73672223\n",
      "iteration:  38 loss: 1.06170475\n",
      "iteration:  39 loss: 2.75811601\n",
      "iteration:  40 loss: 2.49062324\n",
      "iteration:  41 loss: 2.50066924\n",
      "iteration:  42 loss: 1.01183069\n",
      "iteration:  43 loss: 2.15787840\n",
      "iteration:  44 loss: 0.73979461\n",
      "iteration:  45 loss: 0.66242093\n",
      "iteration:  46 loss: 3.48757577\n",
      "iteration:  47 loss: 2.46759486\n",
      "iteration:  48 loss: 1.06610215\n",
      "iteration:  49 loss: 3.18080902\n",
      "iteration:  50 loss: 2.77873039\n",
      "iteration:  51 loss: 2.03289533\n",
      "iteration:  52 loss: 2.42410493\n",
      "iteration:  53 loss: 0.82085454\n",
      "iteration:  54 loss: 1.79300678\n",
      "iteration:  55 loss: 2.18454194\n",
      "iteration:  56 loss: 2.51526427\n",
      "iteration:  57 loss: 1.31324804\n",
      "iteration:  58 loss: 2.02578783\n",
      "iteration:  59 loss: 1.25148427\n",
      "iteration:  60 loss: 1.96655500\n",
      "iteration:  61 loss: 1.96170723\n",
      "iteration:  62 loss: 2.28520823\n",
      "iteration:  63 loss: 0.90417212\n",
      "iteration:  64 loss: 1.05511618\n",
      "iteration:  65 loss: 2.64068651\n",
      "iteration:  66 loss: 1.39492416\n",
      "iteration:  67 loss: 1.13765538\n",
      "iteration:  68 loss: 1.47508347\n",
      "iteration:  69 loss: 1.26140058\n",
      "iteration:  70 loss: 0.74302256\n",
      "iteration:  71 loss: 2.06979752\n",
      "iteration:  72 loss: 2.25586748\n",
      "iteration:  73 loss: 3.32312512\n",
      "iteration:  74 loss: 0.60110354\n",
      "iteration:  75 loss: 0.51261395\n",
      "iteration:  76 loss: 1.21018708\n",
      "iteration:  77 loss: 3.55458975\n",
      "iteration:  78 loss: 1.27273130\n",
      "iteration:  79 loss: 3.46770906\n",
      "iteration:  80 loss: 1.21518433\n",
      "iteration:  81 loss: 0.46530831\n",
      "iteration:  82 loss: 0.38950771\n",
      "iteration:  83 loss: 2.06693697\n",
      "iteration:  84 loss: 2.74153519\n",
      "iteration:  85 loss: 1.43506444\n",
      "iteration:  86 loss: 0.39747328\n",
      "iteration:  87 loss: 2.69314790\n",
      "iteration:  88 loss: 0.50388831\n",
      "iteration:  89 loss: 3.46046352\n",
      "iteration:  90 loss: 2.87734413\n",
      "iteration:  91 loss: 1.48864126\n",
      "iteration:  92 loss: 0.44210294\n",
      "iteration:  93 loss: 2.67694283\n",
      "iteration:  94 loss: 2.16670823\n",
      "iteration:  95 loss: 0.32195717\n",
      "iteration:  96 loss: 3.97622633\n",
      "iteration:  97 loss: 1.89075291\n",
      "iteration:  98 loss: 1.12583053\n",
      "iteration:  99 loss: 0.63525188\n",
      "iteration: 100 loss: 0.81314319\n",
      "iteration: 101 loss: 0.64273614\n",
      "iteration: 102 loss: 2.84202242\n",
      "iteration: 103 loss: 3.38840580\n",
      "iteration: 104 loss: 0.42269877\n",
      "iteration: 105 loss: 0.86975580\n",
      "iteration: 106 loss: 0.38321200\n",
      "iteration: 107 loss: 2.18067908\n",
      "iteration: 108 loss: 1.92517078\n",
      "iteration: 109 loss: 1.32290590\n",
      "iteration: 110 loss: 3.50452852\n",
      "iteration: 111 loss: 2.63682652\n",
      "iteration: 112 loss: 3.48806810\n",
      "iteration: 113 loss: 3.13324738\n",
      "iteration: 114 loss: 2.34523177\n",
      "iteration: 115 loss: 3.53844500\n",
      "iteration: 116 loss: 0.84601218\n",
      "iteration: 117 loss: 0.44858322\n",
      "iteration: 118 loss: 0.95703208\n",
      "iteration: 119 loss: 2.13123560\n",
      "iteration: 120 loss: 0.99411726\n",
      "iteration: 121 loss: 0.69665456\n",
      "iteration: 122 loss: 0.40267006\n",
      "iteration: 123 loss: 1.93537104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 124 loss: 2.38289142\n",
      "iteration: 125 loss: 2.40920615\n",
      "iteration: 126 loss: 3.60368037\n",
      "iteration: 127 loss: 1.74724996\n",
      "iteration: 128 loss: 2.01326418\n",
      "iteration: 129 loss: 1.03291559\n",
      "iteration: 130 loss: 2.05192614\n",
      "iteration: 131 loss: 2.53277111\n",
      "iteration: 132 loss: 3.82461524\n",
      "iteration: 133 loss: 2.96407461\n",
      "iteration: 134 loss: 2.61765528\n",
      "iteration: 135 loss: 1.11966348\n",
      "iteration: 136 loss: 1.00390828\n",
      "iteration: 137 loss: 2.01215744\n",
      "iteration: 138 loss: 0.64185351\n",
      "iteration: 139 loss: 0.85710901\n",
      "iteration: 140 loss: 3.29442930\n",
      "iteration: 141 loss: 0.62780732\n",
      "iteration: 142 loss: 2.53761888\n",
      "iteration: 143 loss: 2.44981766\n",
      "iteration: 144 loss: 2.81604338\n",
      "iteration: 145 loss: 2.48656154\n",
      "iteration: 146 loss: 2.49364257\n",
      "iteration: 147 loss: 1.09514236\n",
      "iteration: 148 loss: 2.43311071\n",
      "iteration: 149 loss: 2.93180656\n",
      "iteration: 150 loss: 1.23712194\n",
      "iteration: 151 loss: 1.42204928\n",
      "iteration: 152 loss: 2.47508788\n",
      "iteration: 153 loss: 4.13757229\n",
      "iteration: 154 loss: 0.99970144\n",
      "iteration: 155 loss: 2.68240142\n",
      "iteration: 156 loss: 0.65404141\n",
      "iteration: 157 loss: 0.71394235\n",
      "iteration: 158 loss: 1.04105294\n",
      "iteration: 159 loss: 0.53265578\n",
      "iteration: 160 loss: 1.25868714\n",
      "iteration: 161 loss: 1.94085085\n",
      "iteration: 162 loss: 0.58880788\n",
      "iteration: 163 loss: 2.99888015\n",
      "iteration: 164 loss: 2.62270260\n",
      "iteration: 165 loss: 3.28669453\n",
      "iteration: 166 loss: 2.21581960\n",
      "iteration: 167 loss: 1.65486145\n",
      "iteration: 168 loss: 0.71658564\n",
      "iteration: 169 loss: 1.80309236\n",
      "iteration: 170 loss: 2.63651776\n",
      "iteration: 171 loss: 2.15614605\n",
      "iteration: 172 loss: 0.70040715\n",
      "iteration: 173 loss: 0.86342573\n",
      "iteration: 174 loss: 3.02755952\n",
      "iteration: 175 loss: 3.33773041\n",
      "iteration: 176 loss: 2.72055984\n",
      "iteration: 177 loss: 1.01796484\n",
      "iteration: 178 loss: 2.35398030\n",
      "iteration: 179 loss: 0.74473667\n",
      "iteration: 180 loss: 2.92306924\n",
      "iteration: 181 loss: 1.11989367\n",
      "iteration: 182 loss: 1.15110838\n",
      "iteration: 183 loss: 2.62310743\n",
      "iteration: 184 loss: 2.08006239\n",
      "iteration: 185 loss: 2.81898046\n",
      "iteration: 186 loss: 2.95255566\n",
      "iteration: 187 loss: 0.66787434\n",
      "iteration: 188 loss: 3.29876852\n",
      "iteration: 189 loss: 2.48824143\n",
      "iteration: 190 loss: 3.63805842\n",
      "iteration: 191 loss: 2.96661901\n",
      "iteration: 192 loss: 1.05992019\n",
      "iteration: 193 loss: 2.18633580\n",
      "iteration: 194 loss: 1.41434288\n",
      "iteration: 195 loss: 2.94639897\n",
      "iteration: 196 loss: 2.35842299\n",
      "iteration: 197 loss: 2.74913883\n",
      "iteration: 198 loss: 0.81520391\n",
      "iteration: 199 loss: 1.33827806\n",
      "epoch:  13 mean loss training: 1.89409089\n",
      "epoch:  13 mean loss validation: 1.87905419\n",
      "iteration:   0 loss: 1.56304669\n",
      "iteration:   1 loss: 1.86711681\n",
      "iteration:   2 loss: 2.16850352\n",
      "iteration:   3 loss: 0.60857224\n",
      "iteration:   4 loss: 3.13831067\n",
      "iteration:   5 loss: 3.43744326\n",
      "iteration:   6 loss: 2.70010591\n",
      "iteration:   7 loss: 1.98861361\n",
      "iteration:   8 loss: 2.65291810\n",
      "iteration:   9 loss: 2.81628466\n",
      "iteration:  10 loss: 0.91726309\n",
      "iteration:  11 loss: 2.16012239\n",
      "iteration:  12 loss: 3.14192438\n",
      "iteration:  13 loss: 0.86993891\n",
      "iteration:  14 loss: 2.57672334\n",
      "iteration:  15 loss: 0.94018990\n",
      "iteration:  16 loss: 0.80674630\n",
      "iteration:  17 loss: 1.24806392\n",
      "iteration:  18 loss: 2.35468817\n",
      "iteration:  19 loss: 2.39132714\n",
      "iteration:  20 loss: 0.73765069\n",
      "iteration:  21 loss: 2.34256530\n",
      "iteration:  22 loss: 0.71713984\n",
      "iteration:  23 loss: 1.89929378\n",
      "iteration:  24 loss: 2.18495154\n",
      "iteration:  25 loss: 3.23108339\n",
      "iteration:  26 loss: 0.98999757\n",
      "iteration:  27 loss: 2.67589879\n",
      "iteration:  28 loss: 0.66294521\n",
      "iteration:  29 loss: 2.39894009\n",
      "iteration:  30 loss: 1.81731296\n",
      "iteration:  31 loss: 2.35109496\n",
      "iteration:  32 loss: 0.55741858\n",
      "iteration:  33 loss: 1.82543230\n",
      "iteration:  34 loss: 2.65546679\n",
      "iteration:  35 loss: 0.79771078\n",
      "iteration:  36 loss: 0.48631716\n",
      "iteration:  37 loss: 2.66626644\n",
      "iteration:  38 loss: 1.05993712\n",
      "iteration:  39 loss: 2.77974343\n",
      "iteration:  40 loss: 2.40252757\n",
      "iteration:  41 loss: 2.44946551\n",
      "iteration:  42 loss: 1.01178873\n",
      "iteration:  43 loss: 2.29993463\n",
      "iteration:  44 loss: 0.66972876\n",
      "iteration:  45 loss: 0.62236267\n",
      "iteration:  46 loss: 3.48294616\n",
      "iteration:  47 loss: 2.40118551\n",
      "iteration:  48 loss: 1.00421369\n",
      "iteration:  49 loss: 3.17292500\n",
      "iteration:  50 loss: 2.69543052\n",
      "iteration:  51 loss: 2.24414635\n",
      "iteration:  52 loss: 2.33308125\n",
      "iteration:  53 loss: 0.75576341\n",
      "iteration:  54 loss: 1.80277658\n",
      "iteration:  55 loss: 2.15940785\n",
      "iteration:  56 loss: 2.43316579\n",
      "iteration:  57 loss: 1.09664750\n",
      "iteration:  58 loss: 2.05734015\n",
      "iteration:  59 loss: 1.26941168\n",
      "iteration:  60 loss: 1.97120678\n",
      "iteration:  61 loss: 1.99708688\n",
      "iteration:  62 loss: 2.20623255\n",
      "iteration:  63 loss: 0.88092285\n",
      "iteration:  64 loss: 0.99544662\n",
      "iteration:  65 loss: 2.51833320\n",
      "iteration:  66 loss: 1.22520328\n",
      "iteration:  67 loss: 1.08690786\n",
      "iteration:  68 loss: 1.44703746\n",
      "iteration:  69 loss: 1.32955837\n",
      "iteration:  70 loss: 0.76457167\n",
      "iteration:  71 loss: 2.22515559\n",
      "iteration:  72 loss: 2.19299269\n",
      "iteration:  73 loss: 3.16685081\n",
      "iteration:  74 loss: 0.70594555\n",
      "iteration:  75 loss: 0.51318711\n",
      "iteration:  76 loss: 1.10470402\n",
      "iteration:  77 loss: 3.52520323\n",
      "iteration:  78 loss: 1.33257079\n",
      "iteration:  79 loss: 3.44203544\n",
      "iteration:  80 loss: 1.17909598\n",
      "iteration:  81 loss: 0.52002496\n",
      "iteration:  82 loss: 0.40573898\n",
      "iteration:  83 loss: 2.12584782\n",
      "iteration:  84 loss: 2.62949681\n",
      "iteration:  85 loss: 1.58182728\n",
      "iteration:  86 loss: 0.40507248\n",
      "iteration:  87 loss: 2.54667735\n",
      "iteration:  88 loss: 0.50925010\n",
      "iteration:  89 loss: 3.35108256\n",
      "iteration:  90 loss: 2.85130143\n",
      "iteration:  91 loss: 1.20925498\n",
      "iteration:  92 loss: 0.45965281\n",
      "iteration:  93 loss: 2.60989499\n",
      "iteration:  94 loss: 2.22883010\n",
      "iteration:  95 loss: 0.36361900\n",
      "iteration:  96 loss: 3.94530439\n",
      "iteration:  97 loss: 1.86874354\n",
      "iteration:  98 loss: 0.99898893\n",
      "iteration:  99 loss: 0.68102074\n",
      "iteration: 100 loss: 0.74370563\n",
      "iteration: 101 loss: 0.67728418\n",
      "iteration: 102 loss: 2.75469923\n",
      "iteration: 103 loss: 3.32087827\n",
      "iteration: 104 loss: 0.42973188\n",
      "iteration: 105 loss: 0.75737381\n",
      "iteration: 106 loss: 0.38819745\n",
      "iteration: 107 loss: 2.23529959\n",
      "iteration: 108 loss: 2.00059485\n",
      "iteration: 109 loss: 1.30803585\n",
      "iteration: 110 loss: 3.53956771\n",
      "iteration: 111 loss: 2.61037612\n",
      "iteration: 112 loss: 3.42926645\n",
      "iteration: 113 loss: 3.16974211\n",
      "iteration: 114 loss: 2.27539396\n",
      "iteration: 115 loss: 3.56121254\n",
      "iteration: 116 loss: 0.88056350\n",
      "iteration: 117 loss: 0.45515409\n",
      "iteration: 118 loss: 0.79996693\n",
      "iteration: 119 loss: 2.18924952\n",
      "iteration: 120 loss: 0.68142474\n",
      "iteration: 121 loss: 0.70047331\n",
      "iteration: 122 loss: 0.39993560\n",
      "iteration: 123 loss: 1.96299088\n",
      "iteration: 124 loss: 2.37988281\n",
      "iteration: 125 loss: 2.40949249\n",
      "iteration: 126 loss: 3.62713337\n",
      "iteration: 127 loss: 1.68957293\n",
      "iteration: 128 loss: 2.02617550\n",
      "iteration: 129 loss: 1.16564345\n",
      "iteration: 130 loss: 2.03113079\n",
      "iteration: 131 loss: 2.44257212\n",
      "iteration: 132 loss: 3.82979012\n",
      "iteration: 133 loss: 3.03470373\n",
      "iteration: 134 loss: 2.64342666\n",
      "iteration: 135 loss: 1.21254063\n",
      "iteration: 136 loss: 1.01488626\n",
      "iteration: 137 loss: 2.04432559\n",
      "iteration: 138 loss: 0.63346517\n",
      "iteration: 139 loss: 0.88591313\n",
      "iteration: 140 loss: 3.29643345\n",
      "iteration: 141 loss: 0.60461861\n",
      "iteration: 142 loss: 2.47350645\n",
      "iteration: 143 loss: 2.37978482\n",
      "iteration: 144 loss: 2.72544837\n",
      "iteration: 145 loss: 2.45850611\n",
      "iteration: 146 loss: 2.43548489\n",
      "iteration: 147 loss: 1.02517891\n",
      "iteration: 148 loss: 2.38517928\n",
      "iteration: 149 loss: 2.93343616\n",
      "iteration: 150 loss: 1.18321598\n",
      "iteration: 151 loss: 1.55233419\n",
      "iteration: 152 loss: 2.68249917\n",
      "iteration: 153 loss: 4.12641954\n",
      "iteration: 154 loss: 1.01742280\n",
      "iteration: 155 loss: 2.71908092\n",
      "iteration: 156 loss: 0.60570669\n",
      "iteration: 157 loss: 0.67418313\n",
      "iteration: 158 loss: 0.83463222\n",
      "iteration: 159 loss: 0.50646073\n",
      "iteration: 160 loss: 1.10364187\n",
      "iteration: 161 loss: 1.90039837\n",
      "iteration: 162 loss: 0.53372639\n",
      "iteration: 163 loss: 3.00344563\n",
      "iteration: 164 loss: 2.90559101\n",
      "iteration: 165 loss: 3.32945013\n",
      "iteration: 166 loss: 2.10719180\n",
      "iteration: 167 loss: 1.65200865\n",
      "iteration: 168 loss: 0.66030860\n",
      "iteration: 169 loss: 1.76103497\n",
      "iteration: 170 loss: 2.78048825\n",
      "iteration: 171 loss: 2.07543898\n",
      "iteration: 172 loss: 0.65657055\n",
      "iteration: 173 loss: 1.06747222\n",
      "iteration: 174 loss: 3.08176517\n",
      "iteration: 175 loss: 3.45943737\n",
      "iteration: 176 loss: 2.69512105\n",
      "iteration: 177 loss: 1.04710531\n",
      "iteration: 178 loss: 2.37261605\n",
      "iteration: 179 loss: 0.75531864\n",
      "iteration: 180 loss: 2.96578193\n",
      "iteration: 181 loss: 1.10256004\n",
      "iteration: 182 loss: 1.06054163\n",
      "iteration: 183 loss: 2.61654973\n",
      "iteration: 184 loss: 2.06604671\n",
      "iteration: 185 loss: 2.81939220\n",
      "iteration: 186 loss: 2.93388176\n",
      "iteration: 187 loss: 0.65727454\n",
      "iteration: 188 loss: 3.32658935\n",
      "iteration: 189 loss: 2.49715161\n",
      "iteration: 190 loss: 3.63673568\n",
      "iteration: 191 loss: 2.97122741\n",
      "iteration: 192 loss: 1.05500472\n",
      "iteration: 193 loss: 2.17945123\n",
      "iteration: 194 loss: 1.33573318\n",
      "iteration: 195 loss: 2.83725286\n",
      "iteration: 196 loss: 2.38366771\n",
      "iteration: 197 loss: 2.75162935\n",
      "iteration: 198 loss: 0.78040552\n",
      "iteration: 199 loss: 1.30419827\n",
      "epoch:  14 mean loss training: 1.87920821\n",
      "epoch:  14 mean loss validation: 1.86695397\n",
      "iteration:   0 loss: 1.63853407\n",
      "iteration:   1 loss: 1.88872361\n",
      "iteration:   2 loss: 2.10800242\n",
      "iteration:   3 loss: 0.58986115\n",
      "iteration:   4 loss: 3.18765211\n",
      "iteration:   5 loss: 3.51892948\n",
      "iteration:   6 loss: 2.66672444\n",
      "iteration:   7 loss: 1.96792781\n",
      "iteration:   8 loss: 2.64826250\n",
      "iteration:   9 loss: 2.86049533\n",
      "iteration:  10 loss: 0.97554088\n",
      "iteration:  11 loss: 2.08027482\n",
      "iteration:  12 loss: 3.10200572\n",
      "iteration:  13 loss: 0.86275464\n",
      "iteration:  14 loss: 2.57727957\n",
      "iteration:  15 loss: 0.80435234\n",
      "iteration:  16 loss: 0.78909755\n",
      "iteration:  17 loss: 1.07561767\n",
      "iteration:  18 loss: 2.31727195\n",
      "iteration:  19 loss: 2.42566967\n",
      "iteration:  20 loss: 0.71945435\n",
      "iteration:  21 loss: 2.67818475\n",
      "iteration:  22 loss: 0.65971839\n",
      "iteration:  23 loss: 1.95905101\n",
      "iteration:  24 loss: 2.19638324\n",
      "iteration:  25 loss: 3.18152356\n",
      "iteration:  26 loss: 1.01857173\n",
      "iteration:  27 loss: 2.59273267\n",
      "iteration:  28 loss: 0.72414762\n",
      "iteration:  29 loss: 2.37307954\n",
      "iteration:  30 loss: 2.10826063\n",
      "iteration:  31 loss: 2.49191809\n",
      "iteration:  32 loss: 0.59234285\n",
      "iteration:  33 loss: 1.76759505\n",
      "iteration:  34 loss: 2.70220804\n",
      "iteration:  35 loss: 0.76695096\n",
      "iteration:  36 loss: 0.46794349\n",
      "iteration:  37 loss: 2.72992420\n",
      "iteration:  38 loss: 0.99553484\n",
      "iteration:  39 loss: 2.76619506\n",
      "iteration:  40 loss: 2.32808185\n",
      "iteration:  41 loss: 2.37931633\n",
      "iteration:  42 loss: 0.99524409\n",
      "iteration:  43 loss: 2.42272472\n",
      "iteration:  44 loss: 0.70623744\n",
      "iteration:  45 loss: 0.61710536\n",
      "iteration:  46 loss: 3.46909094\n",
      "iteration:  47 loss: 2.30815768\n",
      "iteration:  48 loss: 1.02421105\n",
      "iteration:  49 loss: 3.12437534\n",
      "iteration:  50 loss: 2.72516584\n",
      "iteration:  51 loss: 2.30444813\n",
      "iteration:  52 loss: 2.28176165\n",
      "iteration:  53 loss: 0.77451742\n",
      "iteration:  54 loss: 1.82248664\n",
      "iteration:  55 loss: 2.15762377\n",
      "iteration:  56 loss: 2.35925102\n",
      "iteration:  57 loss: 1.13058436\n",
      "iteration:  58 loss: 1.97980404\n",
      "iteration:  59 loss: 1.26775312\n",
      "iteration:  60 loss: 1.92727721\n",
      "iteration:  61 loss: 1.96415603\n",
      "iteration:  62 loss: 2.27604437\n",
      "iteration:  63 loss: 0.87817079\n",
      "iteration:  64 loss: 0.97001106\n",
      "iteration:  65 loss: 2.50262690\n",
      "iteration:  66 loss: 1.13532221\n",
      "iteration:  67 loss: 1.03709388\n",
      "iteration:  68 loss: 1.37649012\n",
      "iteration:  69 loss: 1.47592318\n",
      "iteration:  70 loss: 0.76485443\n",
      "iteration:  71 loss: 2.05518889\n",
      "iteration:  72 loss: 2.18543911\n",
      "iteration:  73 loss: 3.39746308\n",
      "iteration:  74 loss: 0.67107159\n",
      "iteration:  75 loss: 0.60022217\n",
      "iteration:  76 loss: 1.10284126\n",
      "iteration:  77 loss: 3.60863853\n",
      "iteration:  78 loss: 1.12216103\n",
      "iteration:  79 loss: 3.49977374\n",
      "iteration:  80 loss: 1.34400010\n",
      "iteration:  81 loss: 0.54811889\n",
      "iteration:  82 loss: 0.41370597\n",
      "iteration:  83 loss: 2.15428615\n",
      "iteration:  84 loss: 2.59344721\n",
      "iteration:  85 loss: 1.46452129\n",
      "iteration:  86 loss: 0.39745784\n",
      "iteration:  87 loss: 2.73437381\n",
      "iteration:  88 loss: 0.51076114\n",
      "iteration:  89 loss: 3.32201147\n",
      "iteration:  90 loss: 2.67557669\n",
      "iteration:  91 loss: 0.98258787\n",
      "iteration:  92 loss: 0.46649456\n",
      "iteration:  93 loss: 2.69286990\n",
      "iteration:  94 loss: 2.14359212\n",
      "iteration:  95 loss: 0.38353872\n",
      "iteration:  96 loss: 3.81877565\n",
      "iteration:  97 loss: 1.86126637\n",
      "iteration:  98 loss: 1.03739214\n",
      "iteration:  99 loss: 0.63375479\n",
      "iteration: 100 loss: 0.80457044\n",
      "iteration: 101 loss: 0.70531636\n",
      "iteration: 102 loss: 2.75608969\n",
      "iteration: 103 loss: 3.25113153\n",
      "iteration: 104 loss: 0.45630023\n",
      "iteration: 105 loss: 0.78482878\n",
      "iteration: 106 loss: 0.42076951\n",
      "iteration: 107 loss: 2.17961383\n",
      "iteration: 108 loss: 2.08471942\n",
      "iteration: 109 loss: 1.27761471\n",
      "iteration: 110 loss: 3.51091933\n",
      "iteration: 111 loss: 2.55693054\n",
      "iteration: 112 loss: 3.31740928\n",
      "iteration: 113 loss: 3.03962803\n",
      "iteration: 114 loss: 2.29322076\n",
      "iteration: 115 loss: 3.48837161\n",
      "iteration: 116 loss: 0.88496536\n",
      "iteration: 117 loss: 0.49337673\n",
      "iteration: 118 loss: 0.88894898\n",
      "iteration: 119 loss: 2.10651875\n",
      "iteration: 120 loss: 0.91252261\n",
      "iteration: 121 loss: 0.74285698\n",
      "iteration: 122 loss: 0.41318133\n",
      "iteration: 123 loss: 1.85213053\n",
      "iteration: 124 loss: 2.36924171\n",
      "iteration: 125 loss: 2.39682174\n",
      "iteration: 126 loss: 3.59313202\n",
      "iteration: 127 loss: 1.61679459\n",
      "iteration: 128 loss: 1.97257698\n",
      "iteration: 129 loss: 0.94540346\n",
      "iteration: 130 loss: 1.96133590\n",
      "iteration: 131 loss: 2.45436168\n",
      "iteration: 132 loss: 3.76599145\n",
      "iteration: 133 loss: 2.97657180\n",
      "iteration: 134 loss: 2.56815648\n",
      "iteration: 135 loss: 1.06538916\n",
      "iteration: 136 loss: 0.98159939\n",
      "iteration: 137 loss: 1.98946321\n",
      "iteration: 138 loss: 0.64843214\n",
      "iteration: 139 loss: 0.81436306\n",
      "iteration: 140 loss: 3.25077152\n",
      "iteration: 141 loss: 0.61567295\n",
      "iteration: 142 loss: 2.42304659\n",
      "iteration: 143 loss: 2.40855455\n",
      "iteration: 144 loss: 2.80485582\n",
      "iteration: 145 loss: 2.43492198\n",
      "iteration: 146 loss: 2.43140221\n",
      "iteration: 147 loss: 1.02222633\n",
      "iteration: 148 loss: 2.34840679\n",
      "iteration: 149 loss: 2.89022088\n",
      "iteration: 150 loss: 1.15142262\n",
      "iteration: 151 loss: 1.38756096\n",
      "iteration: 152 loss: 2.70366001\n",
      "iteration: 153 loss: 4.19616413\n",
      "iteration: 154 loss: 0.97008640\n",
      "iteration: 155 loss: 2.83631134\n",
      "iteration: 156 loss: 0.65564251\n",
      "iteration: 157 loss: 0.71409166\n",
      "iteration: 158 loss: 0.93055809\n",
      "iteration: 159 loss: 0.52620369\n",
      "iteration: 160 loss: 1.19451821\n",
      "iteration: 161 loss: 1.80811739\n",
      "iteration: 162 loss: 0.57985008\n",
      "iteration: 163 loss: 2.95837092\n",
      "iteration: 164 loss: 2.65845776\n",
      "iteration: 165 loss: 3.26589203\n",
      "iteration: 166 loss: 2.18622327\n",
      "iteration: 167 loss: 1.62757206\n",
      "iteration: 168 loss: 0.69637966\n",
      "iteration: 169 loss: 1.71363485\n",
      "iteration: 170 loss: 2.67519617\n",
      "iteration: 171 loss: 2.04842830\n",
      "iteration: 172 loss: 0.64865625\n",
      "iteration: 173 loss: 0.79566061\n",
      "iteration: 174 loss: 3.13814497\n",
      "iteration: 175 loss: 3.40167475\n",
      "iteration: 176 loss: 2.68453097\n",
      "iteration: 177 loss: 0.96177620\n",
      "iteration: 178 loss: 2.49856591\n",
      "iteration: 179 loss: 0.67166770\n",
      "iteration: 180 loss: 2.82776546\n",
      "iteration: 181 loss: 1.05951989\n",
      "iteration: 182 loss: 1.08363593\n",
      "iteration: 183 loss: 2.65405703\n",
      "iteration: 184 loss: 2.16451025\n",
      "iteration: 185 loss: 2.75223327\n",
      "iteration: 186 loss: 2.89648676\n",
      "iteration: 187 loss: 0.56789118\n",
      "iteration: 188 loss: 3.28108430\n",
      "iteration: 189 loss: 2.50347781\n",
      "iteration: 190 loss: 3.52403116\n",
      "iteration: 191 loss: 2.88252497\n",
      "iteration: 192 loss: 0.92176980\n",
      "iteration: 193 loss: 2.13304067\n",
      "iteration: 194 loss: 1.27777481\n",
      "iteration: 195 loss: 2.85548043\n",
      "iteration: 196 loss: 2.35807920\n",
      "iteration: 197 loss: 2.67805934\n",
      "iteration: 198 loss: 0.80927986\n",
      "iteration: 199 loss: 1.28908145\n",
      "epoch:  15 mean loss training: 1.86630189\n",
      "epoch:  15 mean loss validation: 1.85541153\n",
      "iteration:   0 loss: 1.52187610\n",
      "iteration:   1 loss: 1.86629307\n",
      "iteration:   2 loss: 2.09943843\n",
      "iteration:   3 loss: 0.53476274\n",
      "iteration:   4 loss: 3.15196872\n",
      "iteration:   5 loss: 3.43535471\n",
      "iteration:   6 loss: 2.63925147\n",
      "iteration:   7 loss: 1.95150268\n",
      "iteration:   8 loss: 2.59008622\n",
      "iteration:   9 loss: 2.76955271\n",
      "iteration:  10 loss: 0.98236209\n",
      "iteration:  11 loss: 2.08467484\n",
      "iteration:  12 loss: 3.07800937\n",
      "iteration:  13 loss: 0.88164371\n",
      "iteration:  14 loss: 2.60544753\n",
      "iteration:  15 loss: 0.88911921\n",
      "iteration:  16 loss: 0.79176044\n",
      "iteration:  17 loss: 1.22929847\n",
      "iteration:  18 loss: 2.33295655\n",
      "iteration:  19 loss: 2.36444664\n",
      "iteration:  20 loss: 0.74834239\n",
      "iteration:  21 loss: 2.30794597\n",
      "iteration:  22 loss: 0.71918732\n",
      "iteration:  23 loss: 1.84532917\n",
      "iteration:  24 loss: 2.01798725\n",
      "iteration:  25 loss: 3.16549754\n",
      "iteration:  26 loss: 1.04221570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  27 loss: 2.59969163\n",
      "iteration:  28 loss: 0.58830822\n",
      "iteration:  29 loss: 2.34177637\n",
      "iteration:  30 loss: 1.65240061\n",
      "iteration:  31 loss: 2.23143101\n",
      "iteration:  32 loss: 0.56764454\n",
      "iteration:  33 loss: 1.73048079\n",
      "iteration:  34 loss: 2.71399093\n",
      "iteration:  35 loss: 0.71395838\n",
      "iteration:  36 loss: 0.44834781\n",
      "iteration:  37 loss: 2.72670841\n",
      "iteration:  38 loss: 0.90807724\n",
      "iteration:  39 loss: 2.84453559\n",
      "iteration:  40 loss: 2.25837278\n",
      "iteration:  41 loss: 2.34721494\n",
      "iteration:  42 loss: 0.93410379\n",
      "iteration:  43 loss: 2.51869178\n",
      "iteration:  44 loss: 0.63345218\n",
      "iteration:  45 loss: 0.53117210\n",
      "iteration:  46 loss: 3.46034646\n",
      "iteration:  47 loss: 2.30899739\n",
      "iteration:  48 loss: 0.94692254\n",
      "iteration:  49 loss: 3.29568458\n",
      "iteration:  50 loss: 2.54805207\n",
      "iteration:  51 loss: 2.25091267\n",
      "iteration:  52 loss: 2.25160742\n",
      "iteration:  53 loss: 0.78384221\n",
      "iteration:  54 loss: 1.76443434\n",
      "iteration:  55 loss: 1.98635983\n",
      "iteration:  56 loss: 2.39236999\n",
      "iteration:  57 loss: 1.15072882\n",
      "iteration:  58 loss: 1.86142874\n",
      "iteration:  59 loss: 1.35553193\n",
      "iteration:  60 loss: 2.00664878\n",
      "iteration:  61 loss: 2.02056479\n",
      "iteration:  62 loss: 2.25892806\n",
      "iteration:  63 loss: 0.99389476\n",
      "iteration:  64 loss: 1.02221406\n",
      "iteration:  65 loss: 2.50721169\n",
      "iteration:  66 loss: 1.17445540\n",
      "iteration:  67 loss: 1.04885375\n",
      "iteration:  68 loss: 1.36657763\n",
      "iteration:  69 loss: 1.56165457\n",
      "iteration:  70 loss: 0.73462701\n",
      "iteration:  71 loss: 2.05356956\n",
      "iteration:  72 loss: 2.16853070\n",
      "iteration:  73 loss: 3.32560539\n",
      "iteration:  74 loss: 0.65043610\n",
      "iteration:  75 loss: 0.51391494\n",
      "iteration:  76 loss: 0.98431867\n",
      "iteration:  77 loss: 3.52164292\n",
      "iteration:  78 loss: 1.09491909\n",
      "iteration:  79 loss: 3.45386076\n",
      "iteration:  80 loss: 1.36065555\n",
      "iteration:  81 loss: 0.49892700\n",
      "iteration:  82 loss: 0.42255118\n",
      "iteration:  83 loss: 2.08379436\n",
      "iteration:  84 loss: 2.56696820\n",
      "iteration:  85 loss: 1.47686100\n",
      "iteration:  86 loss: 0.39681259\n",
      "iteration:  87 loss: 2.83409452\n",
      "iteration:  88 loss: 0.46286643\n",
      "iteration:  89 loss: 3.29468393\n",
      "iteration:  90 loss: 2.65095329\n",
      "iteration:  91 loss: 1.01331103\n",
      "iteration:  92 loss: 0.48384097\n",
      "iteration:  93 loss: 2.61651659\n",
      "iteration:  94 loss: 2.12239265\n",
      "iteration:  95 loss: 0.37676227\n",
      "iteration:  96 loss: 3.79005122\n",
      "iteration:  97 loss: 1.84263110\n",
      "iteration:  98 loss: 1.08933008\n",
      "iteration:  99 loss: 0.58259827\n",
      "iteration: 100 loss: 0.75198358\n",
      "iteration: 101 loss: 0.65471715\n",
      "iteration: 102 loss: 2.79992414\n",
      "iteration: 103 loss: 3.20004225\n",
      "iteration: 104 loss: 0.45339501\n",
      "iteration: 105 loss: 0.80357605\n",
      "iteration: 106 loss: 0.41709054\n",
      "iteration: 107 loss: 2.12374091\n",
      "iteration: 108 loss: 2.12947679\n",
      "iteration: 109 loss: 1.22360146\n",
      "iteration: 110 loss: 3.50885725\n",
      "iteration: 111 loss: 2.54150057\n",
      "iteration: 112 loss: 3.27809334\n",
      "iteration: 113 loss: 3.01540709\n",
      "iteration: 114 loss: 2.30861974\n",
      "iteration: 115 loss: 3.49967694\n",
      "iteration: 116 loss: 0.92672563\n",
      "iteration: 117 loss: 0.50418645\n",
      "iteration: 118 loss: 0.83611798\n",
      "iteration: 119 loss: 2.08969116\n",
      "iteration: 120 loss: 0.72384280\n",
      "iteration: 121 loss: 0.74726206\n",
      "iteration: 122 loss: 0.40809089\n",
      "iteration: 123 loss: 1.86422789\n",
      "iteration: 124 loss: 2.35885644\n",
      "iteration: 125 loss: 2.37767744\n",
      "iteration: 126 loss: 3.61321664\n",
      "iteration: 127 loss: 1.47456384\n",
      "iteration: 128 loss: 1.93756819\n",
      "iteration: 129 loss: 0.89270353\n",
      "iteration: 130 loss: 1.94112456\n",
      "iteration: 131 loss: 2.40368342\n",
      "iteration: 132 loss: 3.77293015\n",
      "iteration: 133 loss: 3.05024624\n",
      "iteration: 134 loss: 2.57132936\n",
      "iteration: 135 loss: 1.02969098\n",
      "iteration: 136 loss: 0.92136055\n",
      "iteration: 137 loss: 1.87810552\n",
      "iteration: 138 loss: 0.60708988\n",
      "iteration: 139 loss: 0.76040661\n",
      "iteration: 140 loss: 3.21419740\n",
      "iteration: 141 loss: 0.60730588\n",
      "iteration: 142 loss: 2.37013221\n",
      "iteration: 143 loss: 2.36431193\n",
      "iteration: 144 loss: 2.82963610\n",
      "iteration: 145 loss: 2.39009452\n",
      "iteration: 146 loss: 2.39661813\n",
      "iteration: 147 loss: 0.95613921\n",
      "iteration: 148 loss: 2.22923899\n",
      "iteration: 149 loss: 2.92700624\n",
      "iteration: 150 loss: 1.02228308\n",
      "iteration: 151 loss: 1.34769201\n",
      "iteration: 152 loss: 2.84650207\n",
      "iteration: 153 loss: 4.42769289\n",
      "iteration: 154 loss: 0.87939018\n",
      "iteration: 155 loss: 2.74853230\n",
      "iteration: 156 loss: 0.59055597\n",
      "iteration: 157 loss: 0.66520458\n",
      "iteration: 158 loss: 0.84882146\n",
      "iteration: 159 loss: 0.46933627\n",
      "iteration: 160 loss: 1.22255659\n",
      "iteration: 161 loss: 1.76616335\n",
      "iteration: 162 loss: 0.53972238\n",
      "iteration: 163 loss: 2.88424587\n",
      "iteration: 164 loss: 2.89534378\n",
      "iteration: 165 loss: 3.24783206\n",
      "iteration: 166 loss: 2.07977748\n",
      "iteration: 167 loss: 1.76562071\n",
      "iteration: 168 loss: 0.69214737\n",
      "iteration: 169 loss: 1.65208292\n",
      "iteration: 170 loss: 2.68250871\n",
      "iteration: 171 loss: 2.02961540\n",
      "iteration: 172 loss: 0.69346035\n",
      "iteration: 173 loss: 0.87004930\n",
      "iteration: 174 loss: 3.10609627\n",
      "iteration: 175 loss: 3.50840116\n",
      "iteration: 176 loss: 2.56236458\n",
      "iteration: 177 loss: 0.99616104\n",
      "iteration: 178 loss: 2.38990593\n",
      "iteration: 179 loss: 0.71123403\n",
      "iteration: 180 loss: 2.71985817\n",
      "iteration: 181 loss: 1.16383743\n",
      "iteration: 182 loss: 1.13019061\n",
      "iteration: 183 loss: 2.59226584\n",
      "iteration: 184 loss: 2.00517416\n",
      "iteration: 185 loss: 2.62654185\n",
      "iteration: 186 loss: 2.70378733\n",
      "iteration: 187 loss: 0.61056989\n",
      "iteration: 188 loss: 3.14933729\n",
      "iteration: 189 loss: 2.55603361\n",
      "iteration: 190 loss: 3.45269179\n",
      "iteration: 191 loss: 2.75095892\n",
      "iteration: 192 loss: 1.09405661\n",
      "iteration: 193 loss: 2.06048775\n",
      "iteration: 194 loss: 1.33877480\n",
      "iteration: 195 loss: 2.88626742\n",
      "iteration: 196 loss: 2.33288646\n",
      "iteration: 197 loss: 2.68572211\n",
      "iteration: 198 loss: 0.78364170\n",
      "iteration: 199 loss: 1.31972075\n",
      "epoch:  16 mean loss training: 1.84256053\n",
      "epoch:  16 mean loss validation: 1.84648061\n",
      "iteration:   0 loss: 1.51720917\n",
      "iteration:   1 loss: 1.81599700\n",
      "iteration:   2 loss: 2.03157258\n",
      "iteration:   3 loss: 0.53850383\n",
      "iteration:   4 loss: 3.14068556\n",
      "iteration:   5 loss: 3.34507966\n",
      "iteration:   6 loss: 2.58638120\n",
      "iteration:   7 loss: 1.89714277\n",
      "iteration:   8 loss: 2.58626008\n",
      "iteration:   9 loss: 2.91267872\n",
      "iteration:  10 loss: 0.96111220\n",
      "iteration:  11 loss: 2.02511740\n",
      "iteration:  12 loss: 2.98145151\n",
      "iteration:  13 loss: 0.88999599\n",
      "iteration:  14 loss: 2.59149361\n",
      "iteration:  15 loss: 0.82636905\n",
      "iteration:  16 loss: 0.80378914\n",
      "iteration:  17 loss: 1.05291498\n",
      "iteration:  18 loss: 2.24717283\n",
      "iteration:  19 loss: 2.33640718\n",
      "iteration:  20 loss: 0.73028141\n",
      "iteration:  21 loss: 2.73917818\n",
      "iteration:  22 loss: 0.67180258\n",
      "iteration:  23 loss: 1.72776675\n",
      "iteration:  24 loss: 1.99031210\n",
      "iteration:  25 loss: 3.09309196\n",
      "iteration:  26 loss: 1.04924762\n",
      "iteration:  27 loss: 2.53644586\n",
      "iteration:  28 loss: 0.62713110\n",
      "iteration:  29 loss: 2.31605649\n",
      "iteration:  30 loss: 1.87250793\n",
      "iteration:  31 loss: 2.21057105\n",
      "iteration:  32 loss: 0.58891761\n",
      "iteration:  33 loss: 1.68747854\n",
      "iteration:  34 loss: 2.69444489\n",
      "iteration:  35 loss: 0.74269623\n",
      "iteration:  36 loss: 0.40676594\n",
      "iteration:  37 loss: 2.68530893\n",
      "iteration:  38 loss: 0.87319255\n",
      "iteration:  39 loss: 2.89846635\n",
      "iteration:  40 loss: 2.27738833\n",
      "iteration:  41 loss: 2.27610922\n",
      "iteration:  42 loss: 0.99165088\n",
      "iteration:  43 loss: 2.40168095\n",
      "iteration:  44 loss: 0.70566106\n",
      "iteration:  45 loss: 0.52874166\n",
      "iteration:  46 loss: 3.38197613\n",
      "iteration:  47 loss: 2.33576703\n",
      "iteration:  48 loss: 0.98154157\n",
      "iteration:  49 loss: 3.05810785\n",
      "iteration:  50 loss: 2.68621659\n",
      "iteration:  51 loss: 2.19596195\n",
      "iteration:  52 loss: 2.24266553\n",
      "iteration:  53 loss: 0.75330049\n",
      "iteration:  54 loss: 1.73477817\n",
      "iteration:  55 loss: 2.04523754\n",
      "iteration:  56 loss: 2.33793330\n",
      "iteration:  57 loss: 1.12633312\n",
      "iteration:  58 loss: 1.83650386\n",
      "iteration:  59 loss: 1.36954939\n",
      "iteration:  60 loss: 1.98883152\n",
      "iteration:  61 loss: 2.00957036\n",
      "iteration:  62 loss: 2.20734382\n",
      "iteration:  63 loss: 0.89249557\n",
      "iteration:  64 loss: 0.97934550\n",
      "iteration:  65 loss: 2.47088051\n",
      "iteration:  66 loss: 1.13710022\n",
      "iteration:  67 loss: 0.93249243\n",
      "iteration:  68 loss: 1.40771627\n",
      "iteration:  69 loss: 1.33201146\n",
      "iteration:  70 loss: 0.76486039\n",
      "iteration:  71 loss: 1.87113130\n",
      "iteration:  72 loss: 2.12848830\n",
      "iteration:  73 loss: 3.20469069\n",
      "iteration:  74 loss: 0.67790419\n",
      "iteration:  75 loss: 0.53071374\n",
      "iteration:  76 loss: 0.97692150\n",
      "iteration:  77 loss: 3.38393641\n",
      "iteration:  78 loss: 1.08986712\n",
      "iteration:  79 loss: 3.34429932\n",
      "iteration:  80 loss: 1.36494803\n",
      "iteration:  81 loss: 0.45264384\n",
      "iteration:  82 loss: 0.44498411\n",
      "iteration:  83 loss: 1.98216677\n",
      "iteration:  84 loss: 2.65575099\n",
      "iteration:  85 loss: 1.50449812\n",
      "iteration:  86 loss: 0.40602937\n",
      "iteration:  87 loss: 2.73334885\n",
      "iteration:  88 loss: 0.45822558\n",
      "iteration:  89 loss: 3.30978584\n",
      "iteration:  90 loss: 2.77189755\n",
      "iteration:  91 loss: 1.04799676\n",
      "iteration:  92 loss: 0.43879139\n",
      "iteration:  93 loss: 2.58149433\n",
      "iteration:  94 loss: 2.16946340\n",
      "iteration:  95 loss: 0.35570613\n",
      "iteration:  96 loss: 3.91763735\n",
      "iteration:  97 loss: 1.84883857\n",
      "iteration:  98 loss: 1.03990376\n",
      "iteration:  99 loss: 0.62816060\n",
      "iteration: 100 loss: 0.68797934\n",
      "iteration: 101 loss: 0.70244974\n",
      "iteration: 102 loss: 2.72312617\n",
      "iteration: 103 loss: 3.20771670\n",
      "iteration: 104 loss: 0.39801306\n",
      "iteration: 105 loss: 0.74774230\n",
      "iteration: 106 loss: 0.38082755\n",
      "iteration: 107 loss: 2.17702293\n",
      "iteration: 108 loss: 2.07708836\n",
      "iteration: 109 loss: 1.21678746\n",
      "iteration: 110 loss: 3.53004289\n",
      "iteration: 111 loss: 2.62108588\n",
      "iteration: 112 loss: 3.44879770\n",
      "iteration: 113 loss: 3.13946056\n",
      "iteration: 114 loss: 2.31380177\n",
      "iteration: 115 loss: 3.54347777\n",
      "iteration: 116 loss: 0.88624531\n",
      "iteration: 117 loss: 0.51680821\n",
      "iteration: 118 loss: 0.77321929\n",
      "iteration: 119 loss: 2.11543202\n",
      "iteration: 120 loss: 0.65544754\n",
      "iteration: 121 loss: 0.72492558\n",
      "iteration: 122 loss: 0.40135765\n",
      "iteration: 123 loss: 1.81877410\n",
      "iteration: 124 loss: 2.37095356\n",
      "iteration: 125 loss: 2.42923737\n",
      "iteration: 126 loss: 3.62708831\n",
      "iteration: 127 loss: 1.55042076\n",
      "iteration: 128 loss: 1.94312978\n",
      "iteration: 129 loss: 1.03825760\n",
      "iteration: 130 loss: 2.05243111\n",
      "iteration: 131 loss: 2.35154700\n",
      "iteration: 132 loss: 3.70721698\n",
      "iteration: 133 loss: 3.00251389\n",
      "iteration: 134 loss: 2.60716581\n",
      "iteration: 135 loss: 1.25280964\n",
      "iteration: 136 loss: 0.92769122\n",
      "iteration: 137 loss: 1.96279681\n",
      "iteration: 138 loss: 0.62175506\n",
      "iteration: 139 loss: 0.88984299\n",
      "iteration: 140 loss: 3.17447257\n",
      "iteration: 141 loss: 0.57307094\n",
      "iteration: 142 loss: 2.30176044\n",
      "iteration: 143 loss: 2.35446692\n",
      "iteration: 144 loss: 2.75183511\n",
      "iteration: 145 loss: 2.47915769\n",
      "iteration: 146 loss: 2.45228457\n",
      "iteration: 147 loss: 1.06616127\n",
      "iteration: 148 loss: 2.23926854\n",
      "iteration: 149 loss: 2.85538507\n",
      "iteration: 150 loss: 1.07055080\n",
      "iteration: 151 loss: 1.38843453\n",
      "iteration: 152 loss: 2.82410622\n",
      "iteration: 153 loss: 4.12547684\n",
      "iteration: 154 loss: 1.03331864\n",
      "iteration: 155 loss: 2.74831796\n",
      "iteration: 156 loss: 0.66009593\n",
      "iteration: 157 loss: 0.72444326\n",
      "iteration: 158 loss: 0.87653816\n",
      "iteration: 159 loss: 0.50270629\n",
      "iteration: 160 loss: 1.24971509\n",
      "iteration: 161 loss: 1.74961376\n",
      "iteration: 162 loss: 0.55499822\n",
      "iteration: 163 loss: 2.83701944\n",
      "iteration: 164 loss: 2.78144383\n",
      "iteration: 165 loss: 3.30818009\n",
      "iteration: 166 loss: 2.10080528\n",
      "iteration: 167 loss: 1.43134439\n",
      "iteration: 168 loss: 0.68327963\n",
      "iteration: 169 loss: 1.56770337\n",
      "iteration: 170 loss: 2.64737034\n",
      "iteration: 171 loss: 1.91977489\n",
      "iteration: 172 loss: 0.65436703\n",
      "iteration: 173 loss: 0.70319641\n",
      "iteration: 174 loss: 3.22714019\n",
      "iteration: 175 loss: 3.39366102\n",
      "iteration: 176 loss: 2.65421271\n",
      "iteration: 177 loss: 1.02440572\n",
      "iteration: 178 loss: 2.42407012\n",
      "iteration: 179 loss: 0.69373113\n",
      "iteration: 180 loss: 2.83348751\n",
      "iteration: 181 loss: 1.08112621\n",
      "iteration: 182 loss: 1.08349764\n",
      "iteration: 183 loss: 2.55712891\n",
      "iteration: 184 loss: 2.01902390\n",
      "iteration: 185 loss: 2.66652346\n",
      "iteration: 186 loss: 2.73228121\n",
      "iteration: 187 loss: 0.61841565\n",
      "iteration: 188 loss: 3.19318891\n",
      "iteration: 189 loss: 2.51797462\n",
      "iteration: 190 loss: 3.53290987\n",
      "iteration: 191 loss: 2.70214033\n",
      "iteration: 192 loss: 1.04207015\n",
      "iteration: 193 loss: 2.06112909\n",
      "iteration: 194 loss: 1.31138361\n",
      "iteration: 195 loss: 2.69165254\n",
      "iteration: 196 loss: 2.41417074\n",
      "iteration: 197 loss: 2.60947394\n",
      "iteration: 198 loss: 0.84318125\n",
      "iteration: 199 loss: 1.36652148\n",
      "epoch:  17 mean loss training: 1.83383286\n",
      "epoch:  17 mean loss validation: 1.85154665\n",
      "iteration:   0 loss: 1.53232777\n",
      "iteration:   1 loss: 1.78830254\n",
      "iteration:   2 loss: 2.04150987\n",
      "iteration:   3 loss: 0.56034267\n",
      "iteration:   4 loss: 3.09504032\n",
      "iteration:   5 loss: 3.36847711\n",
      "iteration:   6 loss: 2.58381724\n",
      "iteration:   7 loss: 1.93244791\n",
      "iteration:   8 loss: 2.47142649\n",
      "iteration:   9 loss: 2.72605753\n",
      "iteration:  10 loss: 1.03932476\n",
      "iteration:  11 loss: 2.03693748\n",
      "iteration:  12 loss: 2.97732902\n",
      "iteration:  13 loss: 0.92568588\n",
      "iteration:  14 loss: 2.61555147\n",
      "iteration:  15 loss: 0.87473559\n",
      "iteration:  16 loss: 0.81530213\n",
      "iteration:  17 loss: 1.09794462\n",
      "iteration:  18 loss: 2.25056338\n",
      "iteration:  19 loss: 2.29165339\n",
      "iteration:  20 loss: 0.70029598\n",
      "iteration:  21 loss: 2.61199689\n",
      "iteration:  22 loss: 0.67742372\n",
      "iteration:  23 loss: 1.80455005\n",
      "iteration:  24 loss: 1.90694165\n",
      "iteration:  25 loss: 3.08996582\n",
      "iteration:  26 loss: 1.04358256\n",
      "iteration:  27 loss: 2.53782964\n",
      "iteration:  28 loss: 0.53601211\n",
      "iteration:  29 loss: 2.27445602\n",
      "iteration:  30 loss: 1.63147736\n",
      "iteration:  31 loss: 2.04055905\n",
      "iteration:  32 loss: 0.52757007\n",
      "iteration:  33 loss: 1.65307117\n",
      "iteration:  34 loss: 2.63358474\n",
      "iteration:  35 loss: 0.65215784\n",
      "iteration:  36 loss: 0.38823333\n",
      "iteration:  37 loss: 2.62462044\n",
      "iteration:  38 loss: 0.89121372\n",
      "iteration:  39 loss: 2.91931057\n",
      "iteration:  40 loss: 2.22754836\n",
      "iteration:  41 loss: 2.23535061\n",
      "iteration:  42 loss: 0.90847152\n",
      "iteration:  43 loss: 2.58614421\n",
      "iteration:  44 loss: 0.68482089\n",
      "iteration:  45 loss: 0.58408695\n",
      "iteration:  46 loss: 3.33175588\n",
      "iteration:  47 loss: 2.31280899\n",
      "iteration:  48 loss: 0.79110646\n",
      "iteration:  49 loss: 3.45020843\n",
      "iteration:  50 loss: 3.16135955\n",
      "iteration:  51 loss: 1.98895669\n",
      "iteration:  52 loss: 2.10185432\n",
      "iteration:  53 loss: 0.62696493\n",
      "iteration:  54 loss: 1.87955725\n",
      "iteration:  55 loss: 2.43733406\n",
      "iteration:  56 loss: 2.28867078\n",
      "iteration:  57 loss: 1.09092247\n",
      "iteration:  58 loss: 2.09963560\n",
      "iteration:  59 loss: 1.36285031\n",
      "iteration:  60 loss: 1.97341239\n",
      "iteration:  61 loss: 2.06856465\n",
      "iteration:  62 loss: 2.16024995\n",
      "iteration:  63 loss: 0.95096821\n",
      "iteration:  64 loss: 0.95523375\n",
      "iteration:  65 loss: 2.41591024\n",
      "iteration:  66 loss: 1.18816292\n",
      "iteration:  67 loss: 1.04003155\n",
      "iteration:  68 loss: 1.37058258\n",
      "iteration:  69 loss: 1.44650280\n",
      "iteration:  70 loss: 0.79730952\n",
      "iteration:  71 loss: 1.85235405\n",
      "iteration:  72 loss: 2.13427258\n",
      "iteration:  73 loss: 3.07378983\n",
      "iteration:  74 loss: 0.68081033\n",
      "iteration:  75 loss: 0.47819021\n",
      "iteration:  76 loss: 0.84257299\n",
      "iteration:  77 loss: 3.37855339\n",
      "iteration:  78 loss: 1.09381521\n",
      "iteration:  79 loss: 3.32499123\n",
      "iteration:  80 loss: 1.41005373\n",
      "iteration:  81 loss: 0.39974710\n",
      "iteration:  82 loss: 0.46654847\n",
      "iteration:  83 loss: 1.93509424\n",
      "iteration:  84 loss: 2.47796798\n",
      "iteration:  85 loss: 1.62450075\n",
      "iteration:  86 loss: 0.43257922\n",
      "iteration:  87 loss: 2.63319492\n",
      "iteration:  88 loss: 0.43919340\n",
      "iteration:  89 loss: 3.27638078\n",
      "iteration:  90 loss: 2.65091014\n",
      "iteration:  91 loss: 1.00832462\n",
      "iteration:  92 loss: 0.49857345\n",
      "iteration:  93 loss: 2.54167986\n",
      "iteration:  94 loss: 2.10379481\n",
      "iteration:  95 loss: 0.36736286\n",
      "iteration:  96 loss: 3.89302206\n",
      "iteration:  97 loss: 1.82820106\n",
      "iteration:  98 loss: 1.13092971\n",
      "iteration:  99 loss: 0.58010918\n",
      "iteration: 100 loss: 0.67487401\n",
      "iteration: 101 loss: 0.55085403\n",
      "iteration: 102 loss: 2.73456025\n",
      "iteration: 103 loss: 3.19008231\n",
      "iteration: 104 loss: 0.41285720\n",
      "iteration: 105 loss: 0.74804837\n",
      "iteration: 106 loss: 0.39017636\n",
      "iteration: 107 loss: 2.10733461\n",
      "iteration: 108 loss: 2.04258490\n",
      "iteration: 109 loss: 1.12361848\n",
      "iteration: 110 loss: 3.51914787\n",
      "iteration: 111 loss: 2.47184181\n",
      "iteration: 112 loss: 3.41395640\n",
      "iteration: 113 loss: 3.15253329\n",
      "iteration: 114 loss: 2.32507467\n",
      "iteration: 115 loss: 3.45536399\n",
      "iteration: 116 loss: 0.93137139\n",
      "iteration: 117 loss: 0.51431412\n",
      "iteration: 118 loss: 0.96683383\n",
      "iteration: 119 loss: 2.05052996\n",
      "iteration: 120 loss: 0.54373837\n",
      "iteration: 121 loss: 0.71855092\n",
      "iteration: 122 loss: 0.40527827\n",
      "iteration: 123 loss: 1.85707569\n",
      "iteration: 124 loss: 2.31352901\n",
      "iteration: 125 loss: 2.37728548\n",
      "iteration: 126 loss: 3.60738969\n",
      "iteration: 127 loss: 1.44670701\n",
      "iteration: 128 loss: 1.92989659\n",
      "iteration: 129 loss: 0.87878692\n",
      "iteration: 130 loss: 1.96434486\n",
      "iteration: 131 loss: 2.37320900\n",
      "iteration: 132 loss: 3.67129993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 133 loss: 2.98882699\n",
      "iteration: 134 loss: 2.40922713\n",
      "iteration: 135 loss: 1.02355695\n",
      "iteration: 136 loss: 0.93471295\n",
      "iteration: 137 loss: 1.89909959\n",
      "iteration: 138 loss: 0.59281689\n",
      "iteration: 139 loss: 0.71780068\n",
      "iteration: 140 loss: 3.17693853\n",
      "iteration: 141 loss: 0.56296116\n",
      "iteration: 142 loss: 2.32268739\n",
      "iteration: 143 loss: 2.36878943\n",
      "iteration: 144 loss: 2.83423543\n",
      "iteration: 145 loss: 2.38687181\n",
      "iteration: 146 loss: 2.39431691\n",
      "iteration: 147 loss: 1.04209483\n",
      "iteration: 148 loss: 2.27834535\n",
      "iteration: 149 loss: 2.84626150\n",
      "iteration: 150 loss: 1.00762093\n",
      "iteration: 151 loss: 1.41332483\n",
      "iteration: 152 loss: 2.73054576\n",
      "iteration: 153 loss: 4.28386402\n",
      "iteration: 154 loss: 0.95643300\n",
      "iteration: 155 loss: 2.77134848\n",
      "iteration: 156 loss: 0.60139281\n",
      "iteration: 157 loss: 0.71081424\n",
      "iteration: 158 loss: 0.82568729\n",
      "iteration: 159 loss: 0.48093915\n",
      "iteration: 160 loss: 0.88459069\n",
      "iteration: 161 loss: 1.71418321\n",
      "iteration: 162 loss: 0.59088051\n",
      "iteration: 163 loss: 2.79733944\n",
      "iteration: 164 loss: 2.79597282\n",
      "iteration: 165 loss: 3.21407485\n",
      "iteration: 166 loss: 2.02482629\n",
      "iteration: 167 loss: 1.59904635\n",
      "iteration: 168 loss: 0.72071230\n",
      "iteration: 169 loss: 1.51602733\n",
      "iteration: 170 loss: 2.70041394\n",
      "iteration: 171 loss: 1.94286275\n",
      "iteration: 172 loss: 0.67470729\n",
      "iteration: 173 loss: 0.86246341\n",
      "iteration: 174 loss: 3.20076394\n",
      "iteration: 175 loss: 3.43777061\n",
      "iteration: 176 loss: 2.53893447\n",
      "iteration: 177 loss: 1.00459826\n",
      "iteration: 178 loss: 2.46859217\n",
      "iteration: 179 loss: 0.64540797\n",
      "iteration: 180 loss: 2.73247600\n",
      "iteration: 181 loss: 1.08437765\n",
      "iteration: 182 loss: 1.06916606\n",
      "iteration: 183 loss: 2.55993152\n",
      "iteration: 184 loss: 2.00854850\n",
      "iteration: 185 loss: 2.59563422\n",
      "iteration: 186 loss: 2.68508554\n",
      "iteration: 187 loss: 0.57539982\n",
      "iteration: 188 loss: 3.22016811\n",
      "iteration: 189 loss: 2.47155213\n",
      "iteration: 190 loss: 3.43266559\n",
      "iteration: 191 loss: 2.70067739\n",
      "iteration: 192 loss: 1.01212537\n",
      "iteration: 193 loss: 2.03594327\n",
      "iteration: 194 loss: 1.30192423\n",
      "iteration: 195 loss: 2.92907000\n",
      "iteration: 196 loss: 2.40688872\n",
      "iteration: 197 loss: 2.71332502\n",
      "iteration: 198 loss: 0.79587036\n",
      "iteration: 199 loss: 1.25313663\n",
      "epoch:  18 mean loss training: 1.81793582\n",
      "epoch:  18 mean loss validation: 1.82720172\n",
      "iteration:   0 loss: 1.52755570\n",
      "iteration:   1 loss: 1.78707433\n",
      "iteration:   2 loss: 2.02308369\n",
      "iteration:   3 loss: 0.52023119\n",
      "iteration:   4 loss: 3.03697634\n",
      "iteration:   5 loss: 3.27750683\n",
      "iteration:   6 loss: 2.53123283\n",
      "iteration:   7 loss: 1.87703156\n",
      "iteration:   8 loss: 2.63726902\n",
      "iteration:   9 loss: 2.90939355\n",
      "iteration:  10 loss: 0.92139798\n",
      "iteration:  11 loss: 1.96575367\n",
      "iteration:  12 loss: 2.93175983\n",
      "iteration:  13 loss: 0.91595054\n",
      "iteration:  14 loss: 2.59105015\n",
      "iteration:  15 loss: 0.85533333\n",
      "iteration:  16 loss: 0.78078681\n",
      "iteration:  17 loss: 1.07108605\n",
      "iteration:  18 loss: 2.18755436\n",
      "iteration:  19 loss: 2.36212325\n",
      "iteration:  20 loss: 0.65023023\n",
      "iteration:  21 loss: 2.47967243\n",
      "iteration:  22 loss: 0.61944687\n",
      "iteration:  23 loss: 1.66343820\n",
      "iteration:  24 loss: 2.09138489\n",
      "iteration:  25 loss: 3.08669639\n",
      "iteration:  26 loss: 1.05412173\n",
      "iteration:  27 loss: 2.54028249\n",
      "iteration:  28 loss: 0.48708996\n",
      "iteration:  29 loss: 2.28442216\n",
      "iteration:  30 loss: 1.77013075\n",
      "iteration:  31 loss: 2.01186228\n",
      "iteration:  32 loss: 0.50060874\n",
      "iteration:  33 loss: 1.56649065\n",
      "iteration:  34 loss: 2.68597174\n",
      "iteration:  35 loss: 0.61239737\n",
      "iteration:  36 loss: 0.38653356\n",
      "iteration:  37 loss: 2.74653029\n",
      "iteration:  38 loss: 0.80163842\n",
      "iteration:  39 loss: 2.95289826\n",
      "iteration:  40 loss: 2.24511027\n",
      "iteration:  41 loss: 2.24524546\n",
      "iteration:  42 loss: 0.91252488\n",
      "iteration:  43 loss: 2.58077979\n",
      "iteration:  44 loss: 0.58691317\n",
      "iteration:  45 loss: 0.54408097\n",
      "iteration:  46 loss: 3.32011151\n",
      "iteration:  47 loss: 2.27681828\n",
      "iteration:  48 loss: 0.66224164\n",
      "iteration:  49 loss: 3.46528053\n",
      "iteration:  50 loss: 3.30856395\n",
      "iteration:  51 loss: 2.07435918\n",
      "iteration:  52 loss: 2.08262348\n",
      "iteration:  53 loss: 0.57596463\n",
      "iteration:  54 loss: 1.82365227\n",
      "iteration:  55 loss: 2.39108086\n",
      "iteration:  56 loss: 2.30791593\n",
      "iteration:  57 loss: 1.16894758\n",
      "iteration:  58 loss: 1.88663471\n",
      "iteration:  59 loss: 1.36406112\n",
      "iteration:  60 loss: 1.91919029\n",
      "iteration:  61 loss: 2.09398699\n",
      "iteration:  62 loss: 2.24308157\n",
      "iteration:  63 loss: 0.93396217\n",
      "iteration:  64 loss: 0.95316106\n",
      "iteration:  65 loss: 2.27564764\n",
      "iteration:  66 loss: 1.18316567\n",
      "iteration:  67 loss: 1.01790583\n",
      "iteration:  68 loss: 1.44404316\n",
      "iteration:  69 loss: 1.54442608\n",
      "iteration:  70 loss: 0.86089873\n",
      "iteration:  71 loss: 1.96843052\n",
      "iteration:  72 loss: 2.10970759\n",
      "iteration:  73 loss: 2.98692083\n",
      "iteration:  74 loss: 0.74573404\n",
      "iteration:  75 loss: 0.46627882\n",
      "iteration:  76 loss: 0.71801472\n",
      "iteration:  77 loss: 3.54066086\n",
      "iteration:  78 loss: 0.99169058\n",
      "iteration:  79 loss: 3.38344932\n",
      "iteration:  80 loss: 1.37037992\n",
      "iteration:  81 loss: 0.43309355\n",
      "iteration:  82 loss: 0.44167903\n",
      "iteration:  83 loss: 1.97750103\n",
      "iteration:  84 loss: 2.46896243\n",
      "iteration:  85 loss: 1.50553250\n",
      "iteration:  86 loss: 0.37563550\n",
      "iteration:  87 loss: 2.70989633\n",
      "iteration:  88 loss: 0.43106461\n",
      "iteration:  89 loss: 3.28232241\n",
      "iteration:  90 loss: 2.75647831\n",
      "iteration:  91 loss: 1.17704940\n",
      "iteration:  92 loss: 0.43670958\n",
      "iteration:  93 loss: 2.55848575\n",
      "iteration:  94 loss: 2.13910222\n",
      "iteration:  95 loss: 0.33245945\n",
      "iteration:  96 loss: 3.86092472\n",
      "iteration:  97 loss: 1.79694414\n",
      "iteration:  98 loss: 1.11935508\n",
      "iteration:  99 loss: 0.56277412\n",
      "iteration: 100 loss: 0.80188555\n",
      "iteration: 101 loss: 0.55228871\n",
      "iteration: 102 loss: 2.70595932\n",
      "iteration: 103 loss: 3.16385841\n",
      "iteration: 104 loss: 0.41321436\n",
      "iteration: 105 loss: 0.77349401\n",
      "iteration: 106 loss: 0.39709273\n",
      "iteration: 107 loss: 1.98340571\n",
      "iteration: 108 loss: 2.02978706\n",
      "iteration: 109 loss: 1.23628891\n",
      "iteration: 110 loss: 3.48092246\n",
      "iteration: 111 loss: 2.58925104\n",
      "iteration: 112 loss: 3.40708375\n",
      "iteration: 113 loss: 3.11468816\n",
      "iteration: 114 loss: 2.34249091\n",
      "iteration: 115 loss: 3.46480227\n",
      "iteration: 116 loss: 0.94825143\n",
      "iteration: 117 loss: 0.45870927\n",
      "iteration: 118 loss: 0.70797366\n",
      "iteration: 119 loss: 2.04090405\n",
      "iteration: 120 loss: 0.57453251\n",
      "iteration: 121 loss: 0.73065501\n",
      "iteration: 122 loss: 0.40814951\n",
      "iteration: 123 loss: 1.78935277\n",
      "iteration: 124 loss: 2.25803018\n",
      "iteration: 125 loss: 2.36957169\n",
      "iteration: 126 loss: 3.53664970\n",
      "iteration: 127 loss: 1.44998944\n",
      "iteration: 128 loss: 1.89719617\n",
      "iteration: 129 loss: 0.82374698\n",
      "iteration: 130 loss: 1.98071921\n",
      "iteration: 131 loss: 2.31930590\n",
      "iteration: 132 loss: 3.57043171\n",
      "iteration: 133 loss: 2.88136363\n",
      "iteration: 134 loss: 2.38762617\n",
      "iteration: 135 loss: 1.06971049\n",
      "iteration: 136 loss: 0.86098516\n",
      "iteration: 137 loss: 1.88785362\n",
      "iteration: 138 loss: 0.65571404\n",
      "iteration: 139 loss: 0.69469345\n",
      "iteration: 140 loss: 3.12364149\n",
      "iteration: 141 loss: 0.61332315\n",
      "iteration: 142 loss: 2.20943642\n",
      "iteration: 143 loss: 2.36132598\n",
      "iteration: 144 loss: 2.78881550\n",
      "iteration: 145 loss: 2.40214491\n",
      "iteration: 146 loss: 2.41337323\n",
      "iteration: 147 loss: 1.08774102\n",
      "iteration: 148 loss: 2.25265670\n",
      "iteration: 149 loss: 2.80675435\n",
      "iteration: 150 loss: 1.08954787\n",
      "iteration: 151 loss: 1.41582274\n",
      "iteration: 152 loss: 2.79963374\n",
      "iteration: 153 loss: 4.20610714\n",
      "iteration: 154 loss: 1.03005731\n",
      "iteration: 155 loss: 2.74634171\n",
      "iteration: 156 loss: 0.66257977\n",
      "iteration: 157 loss: 0.73045230\n",
      "iteration: 158 loss: 0.77765566\n",
      "iteration: 159 loss: 0.49631989\n",
      "iteration: 160 loss: 0.99541986\n",
      "iteration: 161 loss: 1.69525564\n",
      "iteration: 162 loss: 0.60639733\n",
      "iteration: 163 loss: 2.76948142\n",
      "iteration: 164 loss: 2.78632855\n",
      "iteration: 165 loss: 3.19953322\n",
      "iteration: 166 loss: 2.04465389\n",
      "iteration: 167 loss: 1.48845208\n",
      "iteration: 168 loss: 0.76157510\n",
      "iteration: 169 loss: 1.45526314\n",
      "iteration: 170 loss: 2.64221048\n",
      "iteration: 171 loss: 1.85042477\n",
      "iteration: 172 loss: 0.71346378\n",
      "iteration: 173 loss: 0.61866647\n",
      "iteration: 174 loss: 3.21614957\n",
      "iteration: 175 loss: 3.29579401\n",
      "iteration: 176 loss: 2.52197886\n",
      "iteration: 177 loss: 1.01449883\n",
      "iteration: 178 loss: 2.42874956\n",
      "iteration: 179 loss: 0.68286520\n",
      "iteration: 180 loss: 2.75414324\n",
      "iteration: 181 loss: 1.14241660\n",
      "iteration: 182 loss: 1.02851188\n",
      "iteration: 183 loss: 2.52666187\n",
      "iteration: 184 loss: 2.09411955\n",
      "iteration: 185 loss: 2.60088372\n",
      "iteration: 186 loss: 2.62096405\n",
      "iteration: 187 loss: 0.55873644\n",
      "iteration: 188 loss: 3.20432210\n",
      "iteration: 189 loss: 2.47520518\n",
      "iteration: 190 loss: 3.43555903\n",
      "iteration: 191 loss: 2.64158726\n",
      "iteration: 192 loss: 0.90588170\n",
      "iteration: 193 loss: 2.01911235\n",
      "iteration: 194 loss: 1.21317732\n",
      "iteration: 195 loss: 2.80821276\n",
      "iteration: 196 loss: 2.39877200\n",
      "iteration: 197 loss: 2.84567952\n",
      "iteration: 198 loss: 0.75525892\n",
      "iteration: 199 loss: 1.20654356\n",
      "epoch:  19 mean loss training: 1.80718434\n",
      "epoch:  19 mean loss validation: 1.81632638\n",
      "iteration:   0 loss: 1.42469978\n",
      "iteration:   1 loss: 1.76916349\n",
      "iteration:   2 loss: 1.99798882\n",
      "iteration:   3 loss: 0.51516467\n",
      "iteration:   4 loss: 3.02423000\n",
      "iteration:   5 loss: 3.27068210\n",
      "iteration:   6 loss: 2.51264453\n",
      "iteration:   7 loss: 1.90035045\n",
      "iteration:   8 loss: 2.43769097\n",
      "iteration:   9 loss: 2.72449470\n",
      "iteration:  10 loss: 1.00473344\n",
      "iteration:  11 loss: 1.93845463\n",
      "iteration:  12 loss: 2.98203778\n",
      "iteration:  13 loss: 0.95222282\n",
      "iteration:  14 loss: 2.59058666\n",
      "iteration:  15 loss: 0.82820261\n",
      "iteration:  16 loss: 0.84768891\n",
      "iteration:  17 loss: 0.98737812\n",
      "iteration:  18 loss: 2.25010109\n",
      "iteration:  19 loss: 2.22938633\n",
      "iteration:  20 loss: 0.73407394\n",
      "iteration:  21 loss: 2.64192748\n",
      "iteration:  22 loss: 0.63291800\n",
      "iteration:  23 loss: 1.65273356\n",
      "iteration:  24 loss: 2.06060243\n",
      "iteration:  25 loss: 3.06161976\n",
      "iteration:  26 loss: 1.07733560\n",
      "iteration:  27 loss: 2.60637927\n",
      "iteration:  28 loss: 0.46743467\n",
      "iteration:  29 loss: 2.26911664\n",
      "iteration:  30 loss: 1.61433673\n",
      "iteration:  31 loss: 1.92649806\n",
      "iteration:  32 loss: 0.52194703\n",
      "iteration:  33 loss: 1.58671856\n",
      "iteration:  34 loss: 2.73099828\n",
      "iteration:  35 loss: 0.66420269\n",
      "iteration:  36 loss: 0.45842740\n",
      "iteration:  37 loss: 2.77829480\n",
      "iteration:  38 loss: 0.80485088\n",
      "iteration:  39 loss: 2.84028912\n",
      "iteration:  40 loss: 2.22481823\n",
      "iteration:  41 loss: 2.18445230\n",
      "iteration:  42 loss: 0.78763056\n",
      "iteration:  43 loss: 2.47274637\n",
      "iteration:  44 loss: 0.62821859\n",
      "iteration:  45 loss: 0.63554716\n",
      "iteration:  46 loss: 3.25793004\n",
      "iteration:  47 loss: 2.27630901\n",
      "iteration:  48 loss: 0.63209546\n",
      "iteration:  49 loss: 3.36087871\n",
      "iteration:  50 loss: 3.33019733\n",
      "iteration:  51 loss: 1.98380709\n",
      "iteration:  52 loss: 2.06711769\n",
      "iteration:  53 loss: 0.60231549\n",
      "iteration:  54 loss: 1.72859585\n",
      "iteration:  55 loss: 2.54235339\n",
      "iteration:  56 loss: 2.28499269\n",
      "iteration:  57 loss: 1.49762154\n",
      "iteration:  58 loss: 2.15747690\n",
      "iteration:  59 loss: 1.34380698\n",
      "iteration:  60 loss: 1.97640407\n",
      "iteration:  61 loss: 2.03660774\n",
      "iteration:  62 loss: 2.21708274\n",
      "iteration:  63 loss: 0.78307045\n",
      "iteration:  64 loss: 1.01638138\n",
      "iteration:  65 loss: 2.32043576\n",
      "iteration:  66 loss: 1.23482573\n",
      "iteration:  67 loss: 1.03926265\n",
      "iteration:  68 loss: 1.61429894\n",
      "iteration:  69 loss: 1.55516112\n",
      "iteration:  70 loss: 1.01319075\n",
      "iteration:  71 loss: 1.50065124\n",
      "iteration:  72 loss: 2.05574679\n",
      "iteration:  73 loss: 2.89615059\n",
      "iteration:  74 loss: 0.72699928\n",
      "iteration:  75 loss: 0.61086518\n",
      "iteration:  76 loss: 0.88219392\n",
      "iteration:  77 loss: 3.54677367\n",
      "iteration:  78 loss: 0.94412452\n",
      "iteration:  79 loss: 3.31964016\n",
      "iteration:  80 loss: 1.38070941\n",
      "iteration:  81 loss: 0.38012198\n",
      "iteration:  82 loss: 0.46214521\n",
      "iteration:  83 loss: 2.00063658\n",
      "iteration:  84 loss: 2.63440752\n",
      "iteration:  85 loss: 1.47234106\n",
      "iteration:  86 loss: 0.35732296\n",
      "iteration:  87 loss: 2.83909345\n",
      "iteration:  88 loss: 0.41931906\n",
      "iteration:  89 loss: 3.27547693\n",
      "iteration:  90 loss: 2.60896158\n",
      "iteration:  91 loss: 1.19438958\n",
      "iteration:  92 loss: 0.37281919\n",
      "iteration:  93 loss: 2.48304558\n",
      "iteration:  94 loss: 2.13136816\n",
      "iteration:  95 loss: 0.32319427\n",
      "iteration:  96 loss: 3.89343095\n",
      "iteration:  97 loss: 1.84091914\n",
      "iteration:  98 loss: 0.96015310\n",
      "iteration:  99 loss: 0.51912814\n",
      "iteration: 100 loss: 0.59089917\n",
      "iteration: 101 loss: 0.60423034\n",
      "iteration: 102 loss: 2.79404116\n",
      "iteration: 103 loss: 3.17469883\n",
      "iteration: 104 loss: 0.35599998\n",
      "iteration: 105 loss: 0.67265683\n",
      "iteration: 106 loss: 0.34858942\n",
      "iteration: 107 loss: 2.15499520\n",
      "iteration: 108 loss: 1.86308944\n",
      "iteration: 109 loss: 1.09463060\n",
      "iteration: 110 loss: 3.53636122\n",
      "iteration: 111 loss: 2.64453459\n",
      "iteration: 112 loss: 3.50324368\n",
      "iteration: 113 loss: 3.05508018\n",
      "iteration: 114 loss: 2.22091460\n",
      "iteration: 115 loss: 3.57906461\n",
      "iteration: 116 loss: 0.89681011\n",
      "iteration: 117 loss: 0.40219954\n",
      "iteration: 118 loss: 0.93145663\n",
      "iteration: 119 loss: 2.02406716\n",
      "iteration: 120 loss: 0.55849493\n",
      "iteration: 121 loss: 0.67910445\n",
      "iteration: 122 loss: 0.41307285\n",
      "iteration: 123 loss: 1.96986938\n",
      "iteration: 124 loss: 2.21636415\n",
      "iteration: 125 loss: 2.33519626\n",
      "iteration: 126 loss: 3.63926578\n",
      "iteration: 127 loss: 1.44677949\n",
      "iteration: 128 loss: 1.88809586\n",
      "iteration: 129 loss: 0.89219576\n",
      "iteration: 130 loss: 2.05011535\n",
      "iteration: 131 loss: 2.24199605\n",
      "iteration: 132 loss: 3.57533097\n",
      "iteration: 133 loss: 2.96665144\n",
      "iteration: 134 loss: 2.38939595\n",
      "iteration: 135 loss: 1.22540176\n",
      "iteration: 136 loss: 0.89714891\n",
      "iteration: 137 loss: 1.84923744\n",
      "iteration: 138 loss: 0.67002213\n",
      "iteration: 139 loss: 0.84758705\n",
      "iteration: 140 loss: 3.02811766\n",
      "iteration: 141 loss: 0.66670418\n",
      "iteration: 142 loss: 2.24882507\n",
      "iteration: 143 loss: 2.30065846\n",
      "iteration: 144 loss: 2.77854133\n",
      "iteration: 145 loss: 2.35768437\n",
      "iteration: 146 loss: 2.39194012\n",
      "iteration: 147 loss: 1.09185433\n",
      "iteration: 148 loss: 2.18666601\n",
      "iteration: 149 loss: 2.74972796\n",
      "iteration: 150 loss: 1.07084608\n",
      "iteration: 151 loss: 1.52627993\n",
      "iteration: 152 loss: 2.84652901\n",
      "iteration: 153 loss: 4.31329584\n",
      "iteration: 154 loss: 0.97804528\n",
      "iteration: 155 loss: 2.67654467\n",
      "iteration: 156 loss: 0.60617763\n",
      "iteration: 157 loss: 0.67499030\n",
      "iteration: 158 loss: 0.96755046\n",
      "iteration: 159 loss: 0.48224828\n",
      "iteration: 160 loss: 1.13709927\n",
      "iteration: 161 loss: 1.65986335\n",
      "iteration: 162 loss: 0.52912772\n",
      "iteration: 163 loss: 2.68871045\n",
      "iteration: 164 loss: 2.97835064\n",
      "iteration: 165 loss: 3.22225904\n",
      "iteration: 166 loss: 1.94554234\n",
      "iteration: 167 loss: 1.52919400\n",
      "iteration: 168 loss: 0.73753679\n",
      "iteration: 169 loss: 1.38794243\n",
      "iteration: 170 loss: 2.59494901\n",
      "iteration: 171 loss: 1.82650268\n",
      "iteration: 172 loss: 0.67363441\n",
      "iteration: 173 loss: 0.54690057\n",
      "iteration: 174 loss: 3.30569530\n",
      "iteration: 175 loss: 3.23358870\n",
      "iteration: 176 loss: 2.57620382\n",
      "iteration: 177 loss: 0.96061009\n",
      "iteration: 178 loss: 2.42745852\n",
      "iteration: 179 loss: 0.67574185\n",
      "iteration: 180 loss: 2.86875582\n",
      "iteration: 181 loss: 1.00917304\n",
      "iteration: 182 loss: 1.04409134\n",
      "iteration: 183 loss: 2.39755034\n",
      "iteration: 184 loss: 2.09194207\n",
      "iteration: 185 loss: 2.80905819\n",
      "iteration: 186 loss: 2.63335657\n",
      "iteration: 187 loss: 0.50931305\n",
      "iteration: 188 loss: 3.23661137\n",
      "iteration: 189 loss: 2.53005528\n",
      "iteration: 190 loss: 3.45709825\n",
      "iteration: 191 loss: 2.51076603\n",
      "iteration: 192 loss: 0.89160156\n",
      "iteration: 193 loss: 1.97841501\n",
      "iteration: 194 loss: 1.22814941\n",
      "iteration: 195 loss: 2.89423943\n",
      "iteration: 196 loss: 2.45967293\n",
      "iteration: 197 loss: 2.35643053\n",
      "iteration: 198 loss: 0.78360534\n",
      "iteration: 199 loss: 1.25816977\n",
      "epoch:  20 mean loss training: 1.80441737\n",
      "epoch:  20 mean loss validation: 1.78406751\n",
      "iteration:   0 loss: 1.40902340\n",
      "iteration:   1 loss: 1.77895343\n",
      "iteration:   2 loss: 1.94494569\n",
      "iteration:   3 loss: 0.47293842\n",
      "iteration:   4 loss: 3.02455568\n",
      "iteration:   5 loss: 3.23376250\n",
      "iteration:   6 loss: 2.41012812\n",
      "iteration:   7 loss: 1.88065660\n",
      "iteration:   8 loss: 2.37387776\n",
      "iteration:   9 loss: 2.91462207\n",
      "iteration:  10 loss: 1.05774128\n",
      "iteration:  11 loss: 1.89752686\n",
      "iteration:  12 loss: 2.83741617\n",
      "iteration:  13 loss: 1.08105457\n",
      "iteration:  14 loss: 2.63818741\n",
      "iteration:  15 loss: 0.87061727\n",
      "iteration:  16 loss: 0.79065931\n",
      "iteration:  17 loss: 0.83025318\n",
      "iteration:  18 loss: 2.06793952\n",
      "iteration:  19 loss: 2.20013881\n",
      "iteration:  20 loss: 0.66779351\n",
      "iteration:  21 loss: 2.82391667\n",
      "iteration:  22 loss: 0.57649451\n",
      "iteration:  23 loss: 1.55694282\n",
      "iteration:  24 loss: 1.91923583\n",
      "iteration:  25 loss: 3.08831906\n",
      "iteration:  26 loss: 0.88246435\n",
      "iteration:  27 loss: 2.55293703\n",
      "iteration:  28 loss: 0.46471265\n",
      "iteration:  29 loss: 2.23690772\n",
      "iteration:  30 loss: 1.96396196\n",
      "iteration:  31 loss: 1.92377186\n",
      "iteration:  32 loss: 0.46436754\n",
      "iteration:  33 loss: 1.67339718\n",
      "iteration:  34 loss: 2.57180643\n",
      "iteration:  35 loss: 0.68142670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  36 loss: 0.39765206\n",
      "iteration:  37 loss: 2.51969147\n",
      "iteration:  38 loss: 0.74162173\n",
      "iteration:  39 loss: 2.97454953\n",
      "iteration:  40 loss: 2.13573098\n",
      "iteration:  41 loss: 2.06145525\n",
      "iteration:  42 loss: 0.76527649\n",
      "iteration:  43 loss: 2.61740088\n",
      "iteration:  44 loss: 0.58893508\n",
      "iteration:  45 loss: 0.91314322\n",
      "iteration:  46 loss: 3.30728006\n",
      "iteration:  47 loss: 2.21278572\n",
      "iteration:  48 loss: 0.58972263\n",
      "iteration:  49 loss: 3.17944145\n",
      "iteration:  50 loss: 3.19140601\n",
      "iteration:  51 loss: 2.07918262\n",
      "iteration:  52 loss: 2.04092145\n",
      "iteration:  53 loss: 0.59532994\n",
      "iteration:  54 loss: 1.63976359\n",
      "iteration:  55 loss: 2.21745610\n",
      "iteration:  56 loss: 2.37273312\n",
      "iteration:  57 loss: 1.08947647\n",
      "iteration:  58 loss: 1.82988560\n",
      "iteration:  59 loss: 1.49305725\n",
      "iteration:  60 loss: 1.87006891\n",
      "iteration:  61 loss: 2.05548787\n",
      "iteration:  62 loss: 2.23834634\n",
      "iteration:  63 loss: 0.89568853\n",
      "iteration:  64 loss: 0.93626690\n",
      "iteration:  65 loss: 2.31705022\n",
      "iteration:  66 loss: 1.24194777\n",
      "iteration:  67 loss: 0.95450830\n",
      "iteration:  68 loss: 1.47604132\n",
      "iteration:  69 loss: 1.47681737\n",
      "iteration:  70 loss: 0.91487932\n",
      "iteration:  71 loss: 1.79755163\n",
      "iteration:  72 loss: 2.03957486\n",
      "iteration:  73 loss: 3.06223655\n",
      "iteration:  74 loss: 0.64626855\n",
      "iteration:  75 loss: 0.50125641\n",
      "iteration:  76 loss: 0.80676979\n",
      "iteration:  77 loss: 3.47427297\n",
      "iteration:  78 loss: 0.87617034\n",
      "iteration:  79 loss: 3.29129577\n",
      "iteration:  80 loss: 1.44309688\n",
      "iteration:  81 loss: 0.40027201\n",
      "iteration:  82 loss: 0.50308388\n",
      "iteration:  83 loss: 1.97013652\n",
      "iteration:  84 loss: 2.51299405\n",
      "iteration:  85 loss: 1.51178360\n",
      "iteration:  86 loss: 0.39948884\n",
      "iteration:  87 loss: 2.65588164\n",
      "iteration:  88 loss: 0.42367029\n",
      "iteration:  89 loss: 3.20058203\n",
      "iteration:  90 loss: 3.18592167\n",
      "iteration:  91 loss: 1.11178398\n",
      "iteration:  92 loss: 0.40122071\n",
      "iteration:  93 loss: 2.49287915\n",
      "iteration:  94 loss: 2.03952360\n",
      "iteration:  95 loss: 0.36028802\n",
      "iteration:  96 loss: 3.86165547\n",
      "iteration:  97 loss: 1.90264976\n",
      "iteration:  98 loss: 1.15402114\n",
      "iteration:  99 loss: 0.54496515\n",
      "iteration: 100 loss: 0.70761710\n",
      "iteration: 101 loss: 0.59141982\n",
      "iteration: 102 loss: 2.71642852\n",
      "iteration: 103 loss: 3.04802561\n",
      "iteration: 104 loss: 0.40464169\n",
      "iteration: 105 loss: 0.87847120\n",
      "iteration: 106 loss: 0.39382818\n",
      "iteration: 107 loss: 2.15796161\n",
      "iteration: 108 loss: 2.42981577\n",
      "iteration: 109 loss: 1.40008330\n",
      "iteration: 110 loss: 3.55236936\n",
      "iteration: 111 loss: 2.76455998\n",
      "iteration: 112 loss: 3.36887789\n",
      "iteration: 113 loss: 3.05723977\n",
      "iteration: 114 loss: 2.13477683\n",
      "iteration: 115 loss: 3.61358571\n",
      "iteration: 116 loss: 1.03184199\n",
      "iteration: 117 loss: 0.52213490\n",
      "iteration: 118 loss: 0.42744356\n",
      "iteration: 119 loss: 2.10593772\n",
      "iteration: 120 loss: 0.39530733\n",
      "iteration: 121 loss: 0.81541926\n",
      "iteration: 122 loss: 0.41037375\n",
      "iteration: 123 loss: 1.87577033\n",
      "iteration: 124 loss: 2.22457528\n",
      "iteration: 125 loss: 2.32486010\n",
      "iteration: 126 loss: 3.51825571\n",
      "iteration: 127 loss: 1.39575398\n",
      "iteration: 128 loss: 1.82635105\n",
      "iteration: 129 loss: 0.71631974\n",
      "iteration: 130 loss: 1.91149843\n",
      "iteration: 131 loss: 2.22701406\n",
      "iteration: 132 loss: 3.45058775\n",
      "iteration: 133 loss: 2.78295827\n",
      "iteration: 134 loss: 2.20602393\n",
      "iteration: 135 loss: 1.12176335\n",
      "iteration: 136 loss: 0.82772338\n",
      "iteration: 137 loss: 1.81323659\n",
      "iteration: 138 loss: 0.74256235\n",
      "iteration: 139 loss: 0.70336515\n",
      "iteration: 140 loss: 2.95795298\n",
      "iteration: 141 loss: 0.68999726\n",
      "iteration: 142 loss: 2.09209561\n",
      "iteration: 143 loss: 2.27919054\n",
      "iteration: 144 loss: 2.79643011\n",
      "iteration: 145 loss: 2.40628529\n",
      "iteration: 146 loss: 2.39932179\n",
      "iteration: 147 loss: 1.10631979\n",
      "iteration: 148 loss: 2.15828204\n",
      "iteration: 149 loss: 2.68206048\n",
      "iteration: 150 loss: 1.09998941\n",
      "iteration: 151 loss: 1.64337230\n",
      "iteration: 152 loss: 2.86026478\n",
      "iteration: 153 loss: 4.16875553\n",
      "iteration: 154 loss: 1.01963627\n",
      "iteration: 155 loss: 2.65075302\n",
      "iteration: 156 loss: 0.65876257\n",
      "iteration: 157 loss: 0.75787878\n",
      "iteration: 158 loss: 0.89289308\n",
      "iteration: 159 loss: 0.49384668\n",
      "iteration: 160 loss: 0.72090638\n",
      "iteration: 161 loss: 1.56619656\n",
      "iteration: 162 loss: 0.56838095\n",
      "iteration: 163 loss: 2.53803134\n",
      "iteration: 164 loss: 3.10099888\n",
      "iteration: 165 loss: 3.17528343\n",
      "iteration: 166 loss: 1.75093186\n",
      "iteration: 167 loss: 1.59246385\n",
      "iteration: 168 loss: 0.80440128\n",
      "iteration: 169 loss: 1.29748416\n",
      "iteration: 170 loss: 2.74690604\n",
      "iteration: 171 loss: 1.90159678\n",
      "iteration: 172 loss: 0.70115840\n",
      "iteration: 173 loss: 0.53343230\n",
      "iteration: 174 loss: 3.45879436\n",
      "iteration: 175 loss: 2.71380448\n",
      "iteration: 176 loss: 2.27367878\n",
      "iteration: 177 loss: 0.92147815\n",
      "iteration: 178 loss: 2.59981346\n",
      "iteration: 179 loss: 0.43521035\n",
      "iteration: 180 loss: 2.76435733\n",
      "iteration: 181 loss: 1.07423627\n",
      "iteration: 182 loss: 0.85085136\n",
      "iteration: 183 loss: 2.13268375\n",
      "iteration: 184 loss: 1.94437861\n",
      "iteration: 185 loss: 2.42150998\n",
      "iteration: 186 loss: 2.45991063\n",
      "iteration: 187 loss: 0.53825039\n",
      "iteration: 188 loss: 3.14623380\n",
      "iteration: 189 loss: 2.81229258\n",
      "iteration: 190 loss: 3.33122253\n",
      "iteration: 191 loss: 2.30414391\n",
      "iteration: 192 loss: 1.03219485\n",
      "iteration: 193 loss: 1.89541316\n",
      "iteration: 194 loss: 1.06363344\n",
      "iteration: 195 loss: 2.93942332\n",
      "iteration: 196 loss: 2.58619237\n",
      "iteration: 197 loss: 2.17828679\n",
      "iteration: 198 loss: 0.85805577\n",
      "iteration: 199 loss: 1.22485805\n",
      "epoch:  21 mean loss training: 1.77817857\n",
      "epoch:  21 mean loss validation: 1.74987602\n",
      "iteration:   0 loss: 1.37836134\n",
      "iteration:   1 loss: 1.70801699\n",
      "iteration:   2 loss: 1.92002058\n",
      "iteration:   3 loss: 0.54939109\n",
      "iteration:   4 loss: 2.73370528\n",
      "iteration:   5 loss: 2.95054388\n",
      "iteration:   6 loss: 2.24167204\n",
      "iteration:   7 loss: 1.84395635\n",
      "iteration:   8 loss: 2.21738935\n",
      "iteration:   9 loss: 3.12674546\n",
      "iteration:  10 loss: 1.14232051\n",
      "iteration:  11 loss: 1.81490266\n",
      "iteration:  12 loss: 2.76318407\n",
      "iteration:  13 loss: 1.33658278\n",
      "iteration:  14 loss: 2.47358012\n",
      "iteration:  15 loss: 0.98555452\n",
      "iteration:  16 loss: 0.95925444\n",
      "iteration:  17 loss: 0.89152163\n",
      "iteration:  18 loss: 2.10865140\n",
      "iteration:  19 loss: 2.33420014\n",
      "iteration:  20 loss: 0.75401509\n",
      "iteration:  21 loss: 2.65449071\n",
      "iteration:  22 loss: 0.63921767\n",
      "iteration:  23 loss: 1.59694779\n",
      "iteration:  24 loss: 1.99403870\n",
      "iteration:  25 loss: 3.14570475\n",
      "iteration:  26 loss: 0.93786198\n",
      "iteration:  27 loss: 2.55021763\n",
      "iteration:  28 loss: 0.47420862\n",
      "iteration:  29 loss: 2.20858121\n",
      "iteration:  30 loss: 1.85553670\n",
      "iteration:  31 loss: 1.91320705\n",
      "iteration:  32 loss: 0.49375567\n",
      "iteration:  33 loss: 1.50792909\n",
      "iteration:  34 loss: 2.60508013\n",
      "iteration:  35 loss: 0.63474774\n",
      "iteration:  36 loss: 0.34329605\n",
      "iteration:  37 loss: 2.50874519\n",
      "iteration:  38 loss: 0.65718567\n",
      "iteration:  39 loss: 2.90103030\n",
      "iteration:  40 loss: 2.11502051\n",
      "iteration:  41 loss: 1.96613574\n",
      "iteration:  42 loss: 0.53344268\n",
      "iteration:  43 loss: 2.22647166\n",
      "iteration:  44 loss: 0.66351098\n",
      "iteration:  45 loss: 0.82398313\n",
      "iteration:  46 loss: 3.16901040\n",
      "iteration:  47 loss: 2.20765781\n",
      "iteration:  48 loss: 0.61904258\n",
      "iteration:  49 loss: 3.13938546\n",
      "iteration:  50 loss: 3.23967242\n",
      "iteration:  51 loss: 1.99859571\n",
      "iteration:  52 loss: 2.00978851\n",
      "iteration:  53 loss: 0.66554302\n",
      "iteration:  54 loss: 1.65426123\n",
      "iteration:  55 loss: 2.32035851\n",
      "iteration:  56 loss: 2.27882957\n",
      "iteration:  57 loss: 1.11848497\n",
      "iteration:  58 loss: 2.38004494\n",
      "iteration:  59 loss: 1.44887412\n",
      "iteration:  60 loss: 1.73496878\n",
      "iteration:  61 loss: 2.02643180\n",
      "iteration:  62 loss: 2.20138597\n",
      "iteration:  63 loss: 0.86654413\n",
      "iteration:  64 loss: 0.98088425\n",
      "iteration:  65 loss: 2.16511846\n",
      "iteration:  66 loss: 1.35060823\n",
      "iteration:  67 loss: 1.10260820\n",
      "iteration:  68 loss: 1.46820116\n",
      "iteration:  69 loss: 1.54012501\n",
      "iteration:  70 loss: 0.97765499\n",
      "iteration:  71 loss: 1.37665915\n",
      "iteration:  72 loss: 2.02110481\n",
      "iteration:  73 loss: 3.07417846\n",
      "iteration:  74 loss: 0.63736618\n",
      "iteration:  75 loss: 0.60274345\n",
      "iteration:  76 loss: 0.98154879\n",
      "iteration:  77 loss: 3.47826385\n",
      "iteration:  78 loss: 0.99783063\n",
      "iteration:  79 loss: 3.18889189\n",
      "iteration:  80 loss: 1.41137040\n",
      "iteration:  81 loss: 0.38071170\n",
      "iteration:  82 loss: 0.58031863\n",
      "iteration:  83 loss: 1.94067931\n",
      "iteration:  84 loss: 2.47593713\n",
      "iteration:  85 loss: 1.42043960\n",
      "iteration:  86 loss: 0.40920764\n",
      "iteration:  87 loss: 2.78963113\n",
      "iteration:  88 loss: 0.43280745\n",
      "iteration:  89 loss: 3.18499970\n",
      "iteration:  90 loss: 2.64949846\n",
      "iteration:  91 loss: 0.81468475\n",
      "iteration:  92 loss: 0.43669200\n",
      "iteration:  93 loss: 2.51390767\n",
      "iteration:  94 loss: 2.13579583\n",
      "iteration:  95 loss: 0.35237613\n",
      "iteration:  96 loss: 3.86184478\n",
      "iteration:  97 loss: 1.79135656\n",
      "iteration:  98 loss: 0.81737334\n",
      "iteration:  99 loss: 0.65633976\n",
      "iteration: 100 loss: 0.49819279\n",
      "iteration: 101 loss: 0.51273632\n",
      "iteration: 102 loss: 2.61344981\n",
      "iteration: 103 loss: 3.00902200\n",
      "iteration: 104 loss: 0.44601396\n",
      "iteration: 105 loss: 0.84100831\n",
      "iteration: 106 loss: 0.43304950\n",
      "iteration: 107 loss: 2.03901935\n",
      "iteration: 108 loss: 1.80380619\n",
      "iteration: 109 loss: 1.13211429\n",
      "iteration: 110 loss: 3.56803322\n",
      "iteration: 111 loss: 2.53114152\n",
      "iteration: 112 loss: 3.33604789\n",
      "iteration: 113 loss: 3.07459426\n",
      "iteration: 114 loss: 2.14993358\n",
      "iteration: 115 loss: 3.50467515\n",
      "iteration: 116 loss: 1.04621530\n",
      "iteration: 117 loss: 0.38963896\n",
      "iteration: 118 loss: 0.58517474\n",
      "iteration: 119 loss: 1.97774875\n",
      "iteration: 120 loss: 1.11353433\n",
      "iteration: 121 loss: 0.77337945\n",
      "iteration: 122 loss: 0.47001851\n",
      "iteration: 123 loss: 1.92874789\n",
      "iteration: 124 loss: 2.24975061\n",
      "iteration: 125 loss: 2.37270904\n",
      "iteration: 126 loss: 3.55911636\n",
      "iteration: 127 loss: 1.37833166\n",
      "iteration: 128 loss: 1.87059772\n",
      "iteration: 129 loss: 1.06741691\n",
      "iteration: 130 loss: 1.89196301\n",
      "iteration: 131 loss: 2.23928046\n",
      "iteration: 132 loss: 3.36428881\n",
      "iteration: 133 loss: 2.84395576\n",
      "iteration: 134 loss: 2.11614776\n",
      "iteration: 135 loss: 1.12181711\n",
      "iteration: 136 loss: 1.02365410\n",
      "iteration: 137 loss: 1.62698698\n",
      "iteration: 138 loss: 0.88727659\n",
      "iteration: 139 loss: 0.95108205\n",
      "iteration: 140 loss: 2.86432266\n",
      "iteration: 141 loss: 0.85371709\n",
      "iteration: 142 loss: 2.38201237\n",
      "iteration: 143 loss: 2.20047808\n",
      "iteration: 144 loss: 2.85445309\n",
      "iteration: 145 loss: 2.35612011\n",
      "iteration: 146 loss: 2.36498332\n",
      "iteration: 147 loss: 1.08823907\n",
      "iteration: 148 loss: 2.06996894\n",
      "iteration: 149 loss: 2.70544600\n",
      "iteration: 150 loss: 1.27101493\n",
      "iteration: 151 loss: 1.64588261\n",
      "iteration: 152 loss: 3.04442668\n",
      "iteration: 153 loss: 3.93900633\n",
      "iteration: 154 loss: 1.12541258\n",
      "iteration: 155 loss: 2.73684478\n",
      "iteration: 156 loss: 0.62153119\n",
      "iteration: 157 loss: 0.75815541\n",
      "iteration: 158 loss: 1.14218915\n",
      "iteration: 159 loss: 0.47735539\n",
      "iteration: 160 loss: 0.71148920\n",
      "iteration: 161 loss: 1.69968784\n",
      "iteration: 162 loss: 0.53341633\n",
      "iteration: 163 loss: 2.73500323\n",
      "iteration: 164 loss: 3.14804387\n",
      "iteration: 165 loss: 3.19034672\n",
      "iteration: 166 loss: 1.71662593\n",
      "iteration: 167 loss: 2.03678727\n",
      "iteration: 168 loss: 0.86538875\n",
      "iteration: 169 loss: 1.50440025\n",
      "iteration: 170 loss: 2.89572310\n",
      "iteration: 171 loss: 1.88404262\n",
      "iteration: 172 loss: 0.75472981\n",
      "iteration: 173 loss: 0.75121826\n",
      "iteration: 174 loss: 3.21475172\n",
      "iteration: 175 loss: 3.37507415\n",
      "iteration: 176 loss: 2.26185703\n",
      "iteration: 177 loss: 0.99340415\n",
      "iteration: 178 loss: 2.30709362\n",
      "iteration: 179 loss: 0.59501034\n",
      "iteration: 180 loss: 2.60447097\n",
      "iteration: 181 loss: 1.09335196\n",
      "iteration: 182 loss: 0.77345663\n",
      "iteration: 183 loss: 2.33948207\n",
      "iteration: 184 loss: 1.91174960\n",
      "iteration: 185 loss: 2.37991309\n",
      "iteration: 186 loss: 2.45423579\n",
      "iteration: 187 loss: 0.63592649\n",
      "iteration: 188 loss: 3.18828297\n",
      "iteration: 189 loss: 2.83381581\n",
      "iteration: 190 loss: 3.30001998\n",
      "iteration: 191 loss: 2.24853849\n",
      "iteration: 192 loss: 1.04196084\n",
      "iteration: 193 loss: 1.84419131\n",
      "iteration: 194 loss: 1.06536925\n",
      "iteration: 195 loss: 2.89695215\n",
      "iteration: 196 loss: 2.73129559\n",
      "iteration: 197 loss: 2.13811564\n",
      "iteration: 198 loss: 0.98253465\n",
      "iteration: 199 loss: 1.21758258\n",
      "epoch:  22 mean loss training: 1.77980566\n",
      "epoch:  22 mean loss validation: 1.74050534\n",
      "iteration:   0 loss: 1.57142031\n",
      "iteration:   1 loss: 1.70531631\n",
      "iteration:   2 loss: 1.84570920\n",
      "iteration:   3 loss: 0.61982131\n",
      "iteration:   4 loss: 3.22274232\n",
      "iteration:   5 loss: 2.94023657\n",
      "iteration:   6 loss: 2.25026894\n",
      "iteration:   7 loss: 1.74777555\n",
      "iteration:   8 loss: 2.63100958\n",
      "iteration:   9 loss: 3.08023763\n",
      "iteration:  10 loss: 1.21282244\n",
      "iteration:  11 loss: 1.74455953\n",
      "iteration:  12 loss: 2.61904621\n",
      "iteration:  13 loss: 1.09224510\n",
      "iteration:  14 loss: 2.56729031\n",
      "iteration:  15 loss: 0.88471234\n",
      "iteration:  16 loss: 1.07016683\n",
      "iteration:  17 loss: 0.95786977\n",
      "iteration:  18 loss: 1.86753857\n",
      "iteration:  19 loss: 2.36616254\n",
      "iteration:  20 loss: 0.75239396\n",
      "iteration:  21 loss: 2.74820995\n",
      "iteration:  22 loss: 0.71819812\n",
      "iteration:  23 loss: 1.41549361\n",
      "iteration:  24 loss: 1.66281271\n",
      "iteration:  25 loss: 2.93030310\n",
      "iteration:  26 loss: 1.28484833\n",
      "iteration:  27 loss: 2.55609298\n",
      "iteration:  28 loss: 0.48987654\n",
      "iteration:  29 loss: 2.16306019\n",
      "iteration:  30 loss: 1.74523664\n",
      "iteration:  31 loss: 2.14080977\n",
      "iteration:  32 loss: 0.59906107\n",
      "iteration:  33 loss: 1.40532708\n",
      "iteration:  34 loss: 2.39926505\n",
      "iteration:  35 loss: 0.74145180\n",
      "iteration:  36 loss: 0.38598096\n",
      "iteration:  37 loss: 2.32995296\n",
      "iteration:  38 loss: 0.87101752\n",
      "iteration:  39 loss: 3.06731653\n",
      "iteration:  40 loss: 2.14152813\n",
      "iteration:  41 loss: 2.19428372\n",
      "iteration:  42 loss: 0.86247706\n",
      "iteration:  43 loss: 2.12506628\n",
      "iteration:  44 loss: 0.72734791\n",
      "iteration:  45 loss: 0.70147914\n",
      "iteration:  46 loss: 3.38008142\n",
      "iteration:  47 loss: 2.01772237\n",
      "iteration:  48 loss: 0.59234971\n",
      "iteration:  49 loss: 2.82936025\n",
      "iteration:  50 loss: 2.88020134\n",
      "iteration:  51 loss: 2.26805949\n",
      "iteration:  52 loss: 1.85541534\n",
      "iteration:  53 loss: 0.64827776\n",
      "iteration:  54 loss: 1.61531568\n",
      "iteration:  55 loss: 2.12578559\n",
      "iteration:  56 loss: 1.93053412\n",
      "iteration:  57 loss: 1.10873735\n",
      "iteration:  58 loss: 2.01522827\n",
      "iteration:  59 loss: 1.44264698\n",
      "iteration:  60 loss: 1.99074972\n",
      "iteration:  61 loss: 1.94087529\n",
      "iteration:  62 loss: 2.31297374\n",
      "iteration:  63 loss: 1.07924569\n",
      "iteration:  64 loss: 1.07117939\n",
      "iteration:  65 loss: 1.97820914\n",
      "iteration:  66 loss: 1.30911660\n",
      "iteration:  67 loss: 1.22077668\n",
      "iteration:  68 loss: 1.65871096\n",
      "iteration:  69 loss: 2.18780541\n",
      "iteration:  70 loss: 1.19811690\n",
      "iteration:  71 loss: 1.10116768\n",
      "iteration:  72 loss: 1.99243522\n",
      "iteration:  73 loss: 2.80607367\n",
      "iteration:  74 loss: 0.54141712\n",
      "iteration:  75 loss: 0.52534181\n",
      "iteration:  76 loss: 0.72150171\n",
      "iteration:  77 loss: 3.23546314\n",
      "iteration:  78 loss: 0.95605499\n",
      "iteration:  79 loss: 2.97118545\n",
      "iteration:  80 loss: 1.48866105\n",
      "iteration:  81 loss: 0.38842201\n",
      "iteration:  82 loss: 0.62620676\n",
      "iteration:  83 loss: 1.97839093\n",
      "iteration:  84 loss: 2.31626225\n",
      "iteration:  85 loss: 1.06043839\n",
      "iteration:  86 loss: 0.45446458\n",
      "iteration:  87 loss: 2.81820488\n",
      "iteration:  88 loss: 0.50223094\n",
      "iteration:  89 loss: 3.01577139\n",
      "iteration:  90 loss: 2.84951329\n",
      "iteration:  91 loss: 0.98110616\n",
      "iteration:  92 loss: 0.43850628\n",
      "iteration:  93 loss: 2.42051339\n",
      "iteration:  94 loss: 2.00516200\n",
      "iteration:  95 loss: 0.40147343\n",
      "iteration:  96 loss: 3.66446185\n",
      "iteration:  97 loss: 1.99276614\n",
      "iteration:  98 loss: 0.40831533\n",
      "iteration:  99 loss: 0.45195135\n",
      "iteration: 100 loss: 1.20870590\n",
      "iteration: 101 loss: 0.79814017\n",
      "iteration: 102 loss: 2.38325524\n",
      "iteration: 103 loss: 2.87166333\n",
      "iteration: 104 loss: 0.49749553\n",
      "iteration: 105 loss: 0.77558523\n",
      "iteration: 106 loss: 0.49895465\n",
      "iteration: 107 loss: 2.19483423\n",
      "iteration: 108 loss: 2.01436877\n",
      "iteration: 109 loss: 1.47970116\n",
      "iteration: 110 loss: 3.42598939\n",
      "iteration: 111 loss: 2.39409900\n",
      "iteration: 112 loss: 3.30538464\n",
      "iteration: 113 loss: 2.39566994\n",
      "iteration: 114 loss: 2.11847281\n",
      "iteration: 115 loss: 3.35805035\n",
      "iteration: 116 loss: 1.07531321\n",
      "iteration: 117 loss: 0.60817683\n",
      "iteration: 118 loss: 0.65391093\n",
      "iteration: 119 loss: 1.93270576\n",
      "iteration: 120 loss: 0.50624692\n",
      "iteration: 121 loss: 1.04486191\n",
      "iteration: 122 loss: 0.52516490\n",
      "iteration: 123 loss: 1.78033698\n",
      "iteration: 124 loss: 2.26476026\n",
      "iteration: 125 loss: 2.39508414\n",
      "iteration: 126 loss: 3.50619149\n",
      "iteration: 127 loss: 1.40003002\n",
      "iteration: 128 loss: 1.89052546\n",
      "iteration: 129 loss: 0.84467578\n",
      "iteration: 130 loss: 1.93199861\n",
      "iteration: 131 loss: 2.19548059\n",
      "iteration: 132 loss: 3.27405715\n",
      "iteration: 133 loss: 2.65017343\n",
      "iteration: 134 loss: 2.05481911\n",
      "iteration: 135 loss: 1.45666444\n",
      "iteration: 136 loss: 0.90669101\n",
      "iteration: 137 loss: 1.82350183\n",
      "iteration: 138 loss: 0.89120960\n",
      "iteration: 139 loss: 0.82546896\n",
      "iteration: 140 loss: 2.77546501\n",
      "iteration: 141 loss: 0.80460817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 142 loss: 2.05628824\n",
      "iteration: 143 loss: 2.18869758\n",
      "iteration: 144 loss: 2.70524573\n",
      "iteration: 145 loss: 2.35939860\n",
      "iteration: 146 loss: 2.30360198\n",
      "iteration: 147 loss: 1.08503711\n",
      "iteration: 148 loss: 2.03784370\n",
      "iteration: 149 loss: 2.35528731\n",
      "iteration: 150 loss: 1.16717827\n",
      "iteration: 151 loss: 1.72526765\n",
      "iteration: 152 loss: 2.79089355\n",
      "iteration: 153 loss: 3.87524772\n",
      "iteration: 154 loss: 1.34057879\n",
      "iteration: 155 loss: 2.99043703\n",
      "iteration: 156 loss: 0.66043043\n",
      "iteration: 157 loss: 0.84178257\n",
      "iteration: 158 loss: 0.96519119\n",
      "iteration: 159 loss: 0.50847846\n",
      "iteration: 160 loss: 0.82281023\n",
      "iteration: 161 loss: 1.48268926\n",
      "iteration: 162 loss: 0.55736226\n",
      "iteration: 163 loss: 2.74576783\n",
      "iteration: 164 loss: 3.54594517\n",
      "iteration: 165 loss: 3.09990525\n",
      "iteration: 166 loss: 1.82163298\n",
      "iteration: 167 loss: 1.52504277\n",
      "iteration: 168 loss: 0.95081323\n",
      "iteration: 169 loss: 1.33734405\n",
      "iteration: 170 loss: 2.62832093\n",
      "iteration: 171 loss: 1.66744506\n",
      "iteration: 172 loss: 0.76673090\n",
      "iteration: 173 loss: 0.60917783\n",
      "iteration: 174 loss: 3.24694657\n",
      "iteration: 175 loss: 3.38961720\n",
      "iteration: 176 loss: 2.33253288\n",
      "iteration: 177 loss: 0.97294742\n",
      "iteration: 178 loss: 2.34918904\n",
      "iteration: 179 loss: 0.60027438\n",
      "iteration: 180 loss: 2.63758135\n",
      "iteration: 181 loss: 1.05811989\n",
      "iteration: 182 loss: 0.66905606\n",
      "iteration: 183 loss: 2.29549479\n",
      "iteration: 184 loss: 1.84472001\n",
      "iteration: 185 loss: 2.38947606\n",
      "iteration: 186 loss: 2.51950216\n",
      "iteration: 187 loss: 0.65844923\n",
      "iteration: 188 loss: 3.27814174\n",
      "iteration: 189 loss: 2.53384590\n",
      "iteration: 190 loss: 3.19346333\n",
      "iteration: 191 loss: 2.22531915\n",
      "iteration: 192 loss: 1.18566346\n",
      "iteration: 193 loss: 1.80026150\n",
      "iteration: 194 loss: 1.16387010\n",
      "iteration: 195 loss: 3.00191689\n",
      "iteration: 196 loss: 2.71167588\n",
      "iteration: 197 loss: 2.05555844\n",
      "iteration: 198 loss: 0.96724391\n",
      "iteration: 199 loss: 1.37137771\n",
      "epoch:  23 mean loss training: 1.76317704\n",
      "epoch:  23 mean loss validation: 1.75220311\n",
      "iteration:   0 loss: 1.75879848\n",
      "iteration:   1 loss: 1.85755396\n",
      "iteration:   2 loss: 1.82751870\n",
      "iteration:   3 loss: 0.59644830\n",
      "iteration:   4 loss: 3.02355957\n",
      "iteration:   5 loss: 3.11280560\n",
      "iteration:   6 loss: 2.22574472\n",
      "iteration:   7 loss: 1.71881521\n",
      "iteration:   8 loss: 2.60559487\n",
      "iteration:   9 loss: 3.30192161\n",
      "iteration:  10 loss: 1.24950528\n",
      "iteration:  11 loss: 1.59971464\n",
      "iteration:  12 loss: 2.57950473\n",
      "iteration:  13 loss: 1.20352757\n",
      "iteration:  14 loss: 2.49768925\n",
      "iteration:  15 loss: 0.87489408\n",
      "iteration:  16 loss: 1.07996440\n",
      "iteration:  17 loss: 1.21107590\n",
      "iteration:  18 loss: 1.95852280\n",
      "iteration:  19 loss: 2.04014015\n",
      "iteration:  20 loss: 0.67303085\n",
      "iteration:  21 loss: 2.26406670\n",
      "iteration:  22 loss: 0.60003918\n",
      "iteration:  23 loss: 1.32617903\n",
      "iteration:  24 loss: 2.05402017\n",
      "iteration:  25 loss: 2.92879844\n",
      "iteration:  26 loss: 1.24974990\n",
      "iteration:  27 loss: 2.52035546\n",
      "iteration:  28 loss: 0.58620358\n",
      "iteration:  29 loss: 2.14621568\n",
      "iteration:  30 loss: 1.24424124\n",
      "iteration:  31 loss: 1.91342938\n",
      "iteration:  32 loss: 0.55276030\n",
      "iteration:  33 loss: 1.23001337\n",
      "iteration:  34 loss: 2.23536229\n",
      "iteration:  35 loss: 0.80023396\n",
      "iteration:  36 loss: 0.36259654\n",
      "iteration:  37 loss: 2.59170175\n",
      "iteration:  38 loss: 0.97639900\n",
      "iteration:  39 loss: 2.77498674\n",
      "iteration:  40 loss: 2.20037365\n",
      "iteration:  41 loss: 2.37236404\n",
      "iteration:  42 loss: 0.95801204\n",
      "iteration:  43 loss: 1.83160627\n",
      "iteration:  44 loss: 0.67484665\n",
      "iteration:  45 loss: 0.52212274\n",
      "iteration:  46 loss: 3.30978942\n",
      "iteration:  47 loss: 2.14554095\n",
      "iteration:  48 loss: 0.75143898\n",
      "iteration:  49 loss: 2.95286846\n",
      "iteration:  50 loss: 2.72583389\n",
      "iteration:  51 loss: 1.90130901\n",
      "iteration:  52 loss: 2.18178439\n",
      "iteration:  53 loss: 0.65695375\n",
      "iteration:  54 loss: 1.48228824\n",
      "iteration:  55 loss: 2.32707310\n",
      "iteration:  56 loss: 2.11537385\n",
      "iteration:  57 loss: 1.78295362\n",
      "iteration:  58 loss: 2.06238341\n",
      "iteration:  59 loss: 1.42163849\n",
      "iteration:  60 loss: 1.72247684\n",
      "iteration:  61 loss: 1.86923659\n",
      "iteration:  62 loss: 2.14269161\n",
      "iteration:  63 loss: 0.79090530\n",
      "iteration:  64 loss: 1.03482175\n",
      "iteration:  65 loss: 2.18586946\n",
      "iteration:  66 loss: 1.32965124\n",
      "iteration:  67 loss: 1.03276849\n",
      "iteration:  68 loss: 1.49393189\n",
      "iteration:  69 loss: 1.64982343\n",
      "iteration:  70 loss: 1.08057177\n",
      "iteration:  71 loss: 1.21066713\n",
      "iteration:  72 loss: 1.97828567\n",
      "iteration:  73 loss: 2.91709423\n",
      "iteration:  74 loss: 0.82582593\n",
      "iteration:  75 loss: 0.51826626\n",
      "iteration:  76 loss: 0.78824091\n",
      "iteration:  77 loss: 3.19350696\n",
      "iteration:  78 loss: 1.06813467\n",
      "iteration:  79 loss: 2.96250772\n",
      "iteration:  80 loss: 1.56816864\n",
      "iteration:  81 loss: 0.39947635\n",
      "iteration:  82 loss: 0.68907738\n",
      "iteration:  83 loss: 1.90716064\n",
      "iteration:  84 loss: 2.49497747\n",
      "iteration:  85 loss: 0.52271110\n",
      "iteration:  86 loss: 0.49924621\n",
      "iteration:  87 loss: 2.84790540\n",
      "iteration:  88 loss: 0.48660195\n",
      "iteration:  89 loss: 2.92130852\n",
      "iteration:  90 loss: 2.47218943\n",
      "iteration:  91 loss: 0.51599818\n",
      "iteration:  92 loss: 0.48849508\n",
      "iteration:  93 loss: 2.48037100\n",
      "iteration:  94 loss: 1.91294253\n",
      "iteration:  95 loss: 0.42711216\n",
      "iteration:  96 loss: 3.67986560\n",
      "iteration:  97 loss: 1.68404114\n",
      "iteration:  98 loss: 0.63026285\n",
      "iteration:  99 loss: 0.49845347\n",
      "iteration: 100 loss: 2.12253952\n",
      "iteration: 101 loss: 0.99033493\n",
      "iteration: 102 loss: 2.48346353\n",
      "iteration: 103 loss: 2.86336136\n",
      "iteration: 104 loss: 0.53328449\n",
      "iteration: 105 loss: 0.85871482\n",
      "iteration: 106 loss: 0.48986867\n",
      "iteration: 107 loss: 1.88716745\n",
      "iteration: 108 loss: 1.40971923\n",
      "iteration: 109 loss: 1.05856049\n",
      "iteration: 110 loss: 3.36148858\n",
      "iteration: 111 loss: 2.32382584\n",
      "iteration: 112 loss: 3.30863428\n",
      "iteration: 113 loss: 2.83885717\n",
      "iteration: 114 loss: 2.11260629\n",
      "iteration: 115 loss: 3.33715630\n",
      "iteration: 116 loss: 1.17282808\n",
      "iteration: 117 loss: 0.46803507\n",
      "iteration: 118 loss: 0.62654632\n",
      "iteration: 119 loss: 1.78432417\n",
      "iteration: 120 loss: 0.53285372\n",
      "iteration: 121 loss: 0.89753628\n",
      "iteration: 122 loss: 0.52868098\n",
      "iteration: 123 loss: 1.68534899\n",
      "iteration: 124 loss: 2.27148557\n",
      "iteration: 125 loss: 2.38126469\n",
      "iteration: 126 loss: 3.47277856\n",
      "iteration: 127 loss: 1.27812076\n",
      "iteration: 128 loss: 1.84287655\n",
      "iteration: 129 loss: 0.61416155\n",
      "iteration: 130 loss: 1.97674739\n",
      "iteration: 131 loss: 2.17560673\n",
      "iteration: 132 loss: 3.17665410\n",
      "iteration: 133 loss: 2.59120941\n",
      "iteration: 134 loss: 1.94805670\n",
      "iteration: 135 loss: 1.35607004\n",
      "iteration: 136 loss: 0.87667012\n",
      "iteration: 137 loss: 1.73221946\n",
      "iteration: 138 loss: 0.91564584\n",
      "iteration: 139 loss: 0.80315459\n",
      "iteration: 140 loss: 2.71150422\n",
      "iteration: 141 loss: 0.82657444\n",
      "iteration: 142 loss: 1.92221785\n",
      "iteration: 143 loss: 2.17308354\n",
      "iteration: 144 loss: 2.64136171\n",
      "iteration: 145 loss: 2.37349105\n",
      "iteration: 146 loss: 2.25525928\n",
      "iteration: 147 loss: 1.05453730\n",
      "iteration: 148 loss: 2.07901669\n",
      "iteration: 149 loss: 2.04168320\n",
      "iteration: 150 loss: 1.11607528\n",
      "iteration: 151 loss: 2.82280517\n",
      "iteration: 152 loss: 2.82156491\n",
      "iteration: 153 loss: 3.83716869\n",
      "iteration: 154 loss: 1.23086548\n",
      "iteration: 155 loss: 2.82943344\n",
      "iteration: 156 loss: 0.64158040\n",
      "iteration: 157 loss: 0.79381251\n",
      "iteration: 158 loss: 0.88285047\n",
      "iteration: 159 loss: 0.53954327\n",
      "iteration: 160 loss: 0.78614992\n",
      "iteration: 161 loss: 1.42879033\n",
      "iteration: 162 loss: 0.50552619\n",
      "iteration: 163 loss: 2.53364348\n",
      "iteration: 164 loss: 3.56133723\n",
      "iteration: 165 loss: 2.97314620\n",
      "iteration: 166 loss: 1.70089507\n",
      "iteration: 167 loss: 1.35535586\n",
      "iteration: 168 loss: 0.94296438\n",
      "iteration: 169 loss: 1.25477588\n",
      "iteration: 170 loss: 2.75210428\n",
      "iteration: 171 loss: 1.44571376\n",
      "iteration: 172 loss: 0.70070422\n",
      "iteration: 173 loss: 0.48135659\n",
      "iteration: 174 loss: 3.36647344\n",
      "iteration: 175 loss: 2.81580472\n",
      "iteration: 176 loss: 2.29756188\n",
      "iteration: 177 loss: 1.02495110\n",
      "iteration: 178 loss: 2.50476837\n",
      "iteration: 179 loss: 0.68077701\n",
      "iteration: 180 loss: 2.91190100\n",
      "iteration: 181 loss: 1.03808904\n",
      "iteration: 182 loss: 0.65359336\n",
      "iteration: 183 loss: 2.20796108\n",
      "iteration: 184 loss: 1.82425308\n",
      "iteration: 185 loss: 2.33892989\n",
      "iteration: 186 loss: 2.51778436\n",
      "iteration: 187 loss: 0.58704036\n",
      "iteration: 188 loss: 3.31822658\n",
      "iteration: 189 loss: 2.43061876\n",
      "iteration: 190 loss: 3.17182636\n",
      "iteration: 191 loss: 2.10517716\n",
      "iteration: 192 loss: 1.40705252\n",
      "iteration: 193 loss: 1.77184713\n",
      "iteration: 194 loss: 1.14380968\n",
      "iteration: 195 loss: 3.47574615\n",
      "iteration: 196 loss: 2.79317451\n",
      "iteration: 197 loss: 1.89655066\n",
      "iteration: 198 loss: 0.98244482\n",
      "iteration: 199 loss: 1.44570374\n",
      "epoch:  24 mean loss training: 1.74234653\n",
      "epoch:  24 mean loss validation: 1.74693191\n",
      "iteration:   0 loss: 1.94216692\n",
      "iteration:   1 loss: 1.88660276\n",
      "iteration:   2 loss: 1.78846252\n",
      "iteration:   3 loss: 0.52934784\n",
      "iteration:   4 loss: 2.72479916\n",
      "iteration:   5 loss: 3.18268466\n",
      "iteration:   6 loss: 2.05664754\n",
      "iteration:   7 loss: 1.70700049\n",
      "iteration:   8 loss: 2.93491626\n",
      "iteration:   9 loss: 3.32385135\n",
      "iteration:  10 loss: 1.20906425\n",
      "iteration:  11 loss: 1.55095315\n",
      "iteration:  12 loss: 2.67134953\n",
      "iteration:  13 loss: 1.37172389\n",
      "iteration:  14 loss: 2.60788965\n",
      "iteration:  15 loss: 0.78876644\n",
      "iteration:  16 loss: 0.98869318\n",
      "iteration:  17 loss: 1.11593962\n",
      "iteration:  18 loss: 1.89978981\n",
      "iteration:  19 loss: 2.01967978\n",
      "iteration:  20 loss: 0.57502037\n",
      "iteration:  21 loss: 1.87617278\n",
      "iteration:  22 loss: 0.48174381\n",
      "iteration:  23 loss: 1.30539608\n",
      "iteration:  24 loss: 2.11684966\n",
      "iteration:  25 loss: 2.96702433\n",
      "iteration:  26 loss: 1.14819896\n",
      "iteration:  27 loss: 2.53605151\n",
      "iteration:  28 loss: 0.50549024\n",
      "iteration:  29 loss: 2.10747313\n",
      "iteration:  30 loss: 1.40816998\n",
      "iteration:  31 loss: 1.84642780\n",
      "iteration:  32 loss: 0.52723908\n",
      "iteration:  33 loss: 1.09053075\n",
      "iteration:  34 loss: 2.50591207\n",
      "iteration:  35 loss: 0.84660560\n",
      "iteration:  36 loss: 0.37796068\n",
      "iteration:  37 loss: 2.47188163\n",
      "iteration:  38 loss: 0.87898654\n",
      "iteration:  39 loss: 2.99082541\n",
      "iteration:  40 loss: 2.09504771\n",
      "iteration:  41 loss: 2.28413200\n",
      "iteration:  42 loss: 0.90485471\n",
      "iteration:  43 loss: 2.60040689\n",
      "iteration:  44 loss: 0.75140560\n",
      "iteration:  45 loss: 0.55428594\n",
      "iteration:  46 loss: 3.13131189\n",
      "iteration:  47 loss: 2.11021852\n",
      "iteration:  48 loss: 0.78942442\n",
      "iteration:  49 loss: 2.40419316\n",
      "iteration:  50 loss: 2.49810958\n",
      "iteration:  51 loss: 1.28627837\n",
      "iteration:  52 loss: 2.30555248\n",
      "iteration:  53 loss: 0.77560771\n",
      "iteration:  54 loss: 1.50717926\n",
      "iteration:  55 loss: 2.12846565\n",
      "iteration:  56 loss: 1.91797817\n",
      "iteration:  57 loss: 1.76544547\n",
      "iteration:  58 loss: 1.92244637\n",
      "iteration:  59 loss: 1.64129090\n",
      "iteration:  60 loss: 1.67214608\n",
      "iteration:  61 loss: 1.71560442\n",
      "iteration:  62 loss: 2.60613251\n",
      "iteration:  63 loss: 0.98375040\n",
      "iteration:  64 loss: 1.15143681\n",
      "iteration:  65 loss: 2.23597932\n",
      "iteration:  66 loss: 1.53329635\n",
      "iteration:  67 loss: 1.23819733\n",
      "iteration:  68 loss: 1.60446000\n",
      "iteration:  69 loss: 1.84989464\n",
      "iteration:  70 loss: 1.03735161\n",
      "iteration:  71 loss: 1.03584802\n",
      "iteration:  72 loss: 1.93800724\n",
      "iteration:  73 loss: 2.93823719\n",
      "iteration:  74 loss: 1.11472249\n",
      "iteration:  75 loss: 0.80339825\n",
      "iteration:  76 loss: 0.89404923\n",
      "iteration:  77 loss: 3.21730018\n",
      "iteration:  78 loss: 1.24390638\n",
      "iteration:  79 loss: 3.30037808\n",
      "iteration:  80 loss: 1.76350629\n",
      "iteration:  81 loss: 0.33908620\n",
      "iteration:  82 loss: 0.51364988\n",
      "iteration:  83 loss: 2.13267851\n",
      "iteration:  84 loss: 2.74930406\n",
      "iteration:  85 loss: 0.45283583\n",
      "iteration:  86 loss: 0.34415877\n",
      "iteration:  87 loss: 2.77010012\n",
      "iteration:  88 loss: 0.63139558\n",
      "iteration:  89 loss: 2.98776174\n",
      "iteration:  90 loss: 1.99646902\n",
      "iteration:  91 loss: 0.54580247\n",
      "iteration:  92 loss: 0.39240700\n",
      "iteration:  93 loss: 2.81298280\n",
      "iteration:  94 loss: 2.27699733\n",
      "iteration:  95 loss: 0.28534403\n",
      "iteration:  96 loss: 3.78425360\n",
      "iteration:  97 loss: 1.73208392\n",
      "iteration:  98 loss: 0.43372375\n",
      "iteration:  99 loss: 0.61885136\n",
      "iteration: 100 loss: 0.59506959\n",
      "iteration: 101 loss: 0.59565103\n",
      "iteration: 102 loss: 2.60451412\n",
      "iteration: 103 loss: 2.92924476\n",
      "iteration: 104 loss: 0.37340820\n",
      "iteration: 105 loss: 0.65180135\n",
      "iteration: 106 loss: 0.36914277\n",
      "iteration: 107 loss: 2.02188611\n",
      "iteration: 108 loss: 1.75039661\n",
      "iteration: 109 loss: 0.89594811\n",
      "iteration: 110 loss: 3.48563433\n",
      "iteration: 111 loss: 2.49471617\n",
      "iteration: 112 loss: 3.47120309\n",
      "iteration: 113 loss: 3.08403563\n",
      "iteration: 114 loss: 2.25633192\n",
      "iteration: 115 loss: 3.58025980\n",
      "iteration: 116 loss: 1.01729238\n",
      "iteration: 117 loss: 0.34837931\n",
      "iteration: 118 loss: 0.49275935\n",
      "iteration: 119 loss: 1.81806517\n",
      "iteration: 120 loss: 0.57206082\n",
      "iteration: 121 loss: 0.68451220\n",
      "iteration: 122 loss: 0.43875876\n",
      "iteration: 123 loss: 2.00279832\n",
      "iteration: 124 loss: 2.06237411\n",
      "iteration: 125 loss: 2.23469257\n",
      "iteration: 126 loss: 3.50847244\n",
      "iteration: 127 loss: 1.23671091\n",
      "iteration: 128 loss: 1.81768525\n",
      "iteration: 129 loss: 0.73849636\n",
      "iteration: 130 loss: 2.12989330\n",
      "iteration: 131 loss: 2.01714468\n",
      "iteration: 132 loss: 3.07212210\n",
      "iteration: 133 loss: 2.54348183\n",
      "iteration: 134 loss: 2.06380844\n",
      "iteration: 135 loss: 1.42060173\n",
      "iteration: 136 loss: 0.88677353\n",
      "iteration: 137 loss: 1.71631908\n",
      "iteration: 138 loss: 0.91823167\n",
      "iteration: 139 loss: 0.80360764\n",
      "iteration: 140 loss: 2.57895565\n",
      "iteration: 141 loss: 0.89826041\n",
      "iteration: 142 loss: 1.94947541\n",
      "iteration: 143 loss: 1.99716866\n",
      "iteration: 144 loss: 2.80769658\n",
      "iteration: 145 loss: 2.20577741\n",
      "iteration: 146 loss: 2.16334558\n",
      "iteration: 147 loss: 1.18073547\n",
      "iteration: 148 loss: 1.93292439\n",
      "iteration: 149 loss: 2.21329188\n",
      "iteration: 150 loss: 1.38120902\n",
      "iteration: 151 loss: 1.33171070\n",
      "iteration: 152 loss: 2.99658966\n",
      "iteration: 153 loss: 3.73094773\n",
      "iteration: 154 loss: 1.31831503\n",
      "iteration: 155 loss: 2.80874062\n",
      "iteration: 156 loss: 0.70537555\n",
      "iteration: 157 loss: 0.80387068\n",
      "iteration: 158 loss: 1.03900683\n",
      "iteration: 159 loss: 0.53047013\n",
      "iteration: 160 loss: 0.76467884\n",
      "iteration: 161 loss: 1.29191089\n",
      "iteration: 162 loss: 0.54888612\n",
      "iteration: 163 loss: 2.43482375\n",
      "iteration: 164 loss: 3.35084748\n",
      "iteration: 165 loss: 2.91031122\n",
      "iteration: 166 loss: 1.63938618\n",
      "iteration: 167 loss: 1.40967131\n",
      "iteration: 168 loss: 1.03693616\n",
      "iteration: 169 loss: 1.18641889\n",
      "iteration: 170 loss: 2.55637836\n",
      "iteration: 171 loss: 1.51856744\n",
      "iteration: 172 loss: 0.83021808\n",
      "iteration: 173 loss: 0.50329065\n",
      "iteration: 174 loss: 3.23944139\n",
      "iteration: 175 loss: 2.93142319\n",
      "iteration: 176 loss: 2.22443795\n",
      "iteration: 177 loss: 1.02720511\n",
      "iteration: 178 loss: 2.16707468\n",
      "iteration: 179 loss: 0.71104276\n",
      "iteration: 180 loss: 2.73887992\n",
      "iteration: 181 loss: 1.07654417\n",
      "iteration: 182 loss: 0.86140609\n",
      "iteration: 183 loss: 2.15391827\n",
      "iteration: 184 loss: 1.70298219\n",
      "iteration: 185 loss: 2.24068761\n",
      "iteration: 186 loss: 2.33726168\n",
      "iteration: 187 loss: 0.55436832\n",
      "iteration: 188 loss: 3.08911729\n",
      "iteration: 189 loss: 2.75594711\n",
      "iteration: 190 loss: 3.10806894\n",
      "iteration: 191 loss: 1.89587653\n",
      "iteration: 192 loss: 1.08334613\n",
      "iteration: 193 loss: 1.56484771\n",
      "iteration: 194 loss: 1.18316138\n",
      "iteration: 195 loss: 2.85028863\n",
      "iteration: 196 loss: 3.06946969\n",
      "iteration: 197 loss: 1.84848547\n",
      "iteration: 198 loss: 1.34087634\n",
      "iteration: 199 loss: 1.75013733\n",
      "epoch:  25 mean loss training: 1.72841096\n",
      "epoch:  25 mean loss validation: 1.77372253\n",
      "iteration:   0 loss: 1.85439265\n",
      "iteration:   1 loss: 1.62501907\n",
      "iteration:   2 loss: 1.89051533\n",
      "iteration:   3 loss: 0.56443429\n",
      "iteration:   4 loss: 2.73219371\n",
      "iteration:   5 loss: 3.54564452\n",
      "iteration:   6 loss: 2.23796725\n",
      "iteration:   7 loss: 1.73310423\n",
      "iteration:   8 loss: 2.24987674\n",
      "iteration:   9 loss: 3.37367153\n",
      "iteration:  10 loss: 1.51083040\n",
      "iteration:  11 loss: 1.68832612\n",
      "iteration:  12 loss: 2.61431074\n",
      "iteration:  13 loss: 1.74326146\n",
      "iteration:  14 loss: 2.41244483\n",
      "iteration:  15 loss: 1.00706720\n",
      "iteration:  16 loss: 1.52136993\n",
      "iteration:  17 loss: 1.43967295\n",
      "iteration:  18 loss: 1.70568621\n",
      "iteration:  19 loss: 2.49936724\n",
      "iteration:  20 loss: 0.64526492\n",
      "iteration:  21 loss: 2.37392402\n",
      "iteration:  22 loss: 0.57442373\n",
      "iteration:  23 loss: 1.36046076\n",
      "iteration:  24 loss: 1.46417141\n",
      "iteration:  25 loss: 2.93259120\n",
      "iteration:  26 loss: 1.12368596\n",
      "iteration:  27 loss: 2.61780286\n",
      "iteration:  28 loss: 0.53945327\n",
      "iteration:  29 loss: 2.14316106\n",
      "iteration:  30 loss: 1.45587730\n",
      "iteration:  31 loss: 1.96911561\n",
      "iteration:  32 loss: 0.54692739\n",
      "iteration:  33 loss: 1.17936516\n",
      "iteration:  34 loss: 2.58728504\n",
      "iteration:  35 loss: 0.76159340\n",
      "iteration:  36 loss: 0.39253452\n",
      "iteration:  37 loss: 2.56754851\n",
      "iteration:  38 loss: 0.60666335\n",
      "iteration:  39 loss: 2.92430282\n",
      "iteration:  40 loss: 1.89953411\n",
      "iteration:  41 loss: 2.02694273\n",
      "iteration:  42 loss: 0.86563206\n",
      "iteration:  43 loss: 2.97114038\n",
      "iteration:  44 loss: 0.76733667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  45 loss: 0.89029264\n",
      "iteration:  46 loss: 3.07632804\n",
      "iteration:  47 loss: 1.84766901\n",
      "iteration:  48 loss: 0.67215353\n",
      "iteration:  49 loss: 2.64934111\n",
      "iteration:  50 loss: 3.23553348\n",
      "iteration:  51 loss: 1.91532516\n",
      "iteration:  52 loss: 1.78229964\n",
      "iteration:  53 loss: 0.65840322\n",
      "iteration:  54 loss: 1.37491548\n",
      "iteration:  55 loss: 2.82913542\n",
      "iteration:  56 loss: 1.55833089\n",
      "iteration:  57 loss: 1.44858623\n",
      "iteration:  58 loss: 1.80811536\n",
      "iteration:  59 loss: 1.48923254\n",
      "iteration:  60 loss: 1.75746489\n",
      "iteration:  61 loss: 1.64767456\n",
      "iteration:  62 loss: 2.22618842\n",
      "iteration:  63 loss: 0.75210720\n",
      "iteration:  64 loss: 1.01780355\n",
      "iteration:  65 loss: 1.90500057\n",
      "iteration:  66 loss: 0.99565548\n",
      "iteration:  67 loss: 0.98863608\n",
      "iteration:  68 loss: 1.17945278\n",
      "iteration:  69 loss: 1.73424864\n",
      "iteration:  70 loss: 0.85531330\n",
      "iteration:  71 loss: 0.84982026\n",
      "iteration:  72 loss: 1.71490836\n",
      "iteration:  73 loss: 2.83762908\n",
      "iteration:  74 loss: 0.58756995\n",
      "iteration:  75 loss: 0.73197758\n",
      "iteration:  76 loss: 0.85806948\n",
      "iteration:  77 loss: 3.08078861\n",
      "iteration:  78 loss: 1.19877255\n",
      "iteration:  79 loss: 1.72583425\n",
      "iteration:  80 loss: 1.64983416\n",
      "iteration:  81 loss: 0.32266331\n",
      "iteration:  82 loss: 0.66313148\n",
      "iteration:  83 loss: 1.77726233\n",
      "iteration:  84 loss: 2.53523183\n",
      "iteration:  85 loss: 0.49257356\n",
      "iteration:  86 loss: 0.48780152\n",
      "iteration:  87 loss: 2.94147491\n",
      "iteration:  88 loss: 0.52842712\n",
      "iteration:  89 loss: 2.73457122\n",
      "iteration:  90 loss: 2.82054234\n",
      "iteration:  91 loss: 0.58386272\n",
      "iteration:  92 loss: 0.55412066\n",
      "iteration:  93 loss: 2.43283892\n",
      "iteration:  94 loss: 1.79615414\n",
      "iteration:  95 loss: 0.43444586\n",
      "iteration:  96 loss: 3.55568838\n",
      "iteration:  97 loss: 1.74245131\n",
      "iteration:  98 loss: 0.55938423\n",
      "iteration:  99 loss: 0.48086885\n",
      "iteration: 100 loss: 1.66546893\n",
      "iteration: 101 loss: 0.97987878\n",
      "iteration: 102 loss: 2.37499881\n",
      "iteration: 103 loss: 2.65248132\n",
      "iteration: 104 loss: 0.45921943\n",
      "iteration: 105 loss: 0.76499820\n",
      "iteration: 106 loss: 0.43411210\n",
      "iteration: 107 loss: 2.08855891\n",
      "iteration: 108 loss: 1.86571133\n",
      "iteration: 109 loss: 1.39035535\n",
      "iteration: 110 loss: 3.24359608\n",
      "iteration: 111 loss: 2.82786775\n",
      "iteration: 112 loss: 3.39224148\n",
      "iteration: 113 loss: 2.09450293\n",
      "iteration: 114 loss: 1.90284395\n",
      "iteration: 115 loss: 3.31903648\n",
      "iteration: 116 loss: 1.28215671\n",
      "iteration: 117 loss: 0.76898652\n",
      "iteration: 118 loss: 0.56317109\n",
      "iteration: 119 loss: 1.34665942\n",
      "iteration: 120 loss: 0.44744971\n",
      "iteration: 121 loss: 1.16735399\n",
      "iteration: 122 loss: 0.78108394\n",
      "iteration: 123 loss: 2.37356353\n",
      "iteration: 124 loss: 2.12667131\n",
      "iteration: 125 loss: 2.10755181\n",
      "iteration: 126 loss: 3.42068720\n",
      "iteration: 127 loss: 1.07744730\n",
      "iteration: 128 loss: 1.84632540\n",
      "iteration: 129 loss: 0.68665534\n",
      "iteration: 130 loss: 2.07879519\n",
      "iteration: 131 loss: 2.11191010\n",
      "iteration: 132 loss: 2.91963673\n",
      "iteration: 133 loss: 2.41956830\n",
      "iteration: 134 loss: 1.70844913\n",
      "iteration: 135 loss: 0.70696688\n",
      "iteration: 136 loss: 1.00413203\n",
      "iteration: 137 loss: 1.28522539\n",
      "iteration: 138 loss: 0.99123472\n",
      "iteration: 139 loss: 0.79914665\n",
      "iteration: 140 loss: 2.60081387\n",
      "iteration: 141 loss: 0.92141509\n",
      "iteration: 142 loss: 1.92285621\n",
      "iteration: 143 loss: 1.86587095\n",
      "iteration: 144 loss: 3.24858642\n",
      "iteration: 145 loss: 2.03400207\n",
      "iteration: 146 loss: 2.01008439\n",
      "iteration: 147 loss: 0.76648796\n",
      "iteration: 148 loss: 1.66435575\n",
      "iteration: 149 loss: 2.19268036\n",
      "iteration: 150 loss: 1.16280174\n",
      "iteration: 151 loss: 1.48318422\n",
      "iteration: 152 loss: 3.20619607\n",
      "iteration: 153 loss: 3.79825449\n",
      "iteration: 154 loss: 1.06713235\n",
      "iteration: 155 loss: 2.87795234\n",
      "iteration: 156 loss: 0.57385951\n",
      "iteration: 157 loss: 0.69224238\n",
      "iteration: 158 loss: 0.66571009\n",
      "iteration: 159 loss: 0.40878367\n",
      "iteration: 160 loss: 0.64169419\n",
      "iteration: 161 loss: 1.56356966\n",
      "iteration: 162 loss: 0.46757722\n",
      "iteration: 163 loss: 2.53872561\n",
      "iteration: 164 loss: 2.77725887\n",
      "iteration: 165 loss: 2.80644226\n",
      "iteration: 166 loss: 1.20663917\n",
      "iteration: 167 loss: 1.69051790\n",
      "iteration: 168 loss: 0.80236441\n",
      "iteration: 169 loss: 1.44082463\n",
      "iteration: 170 loss: 3.29522872\n",
      "iteration: 171 loss: 1.61907935\n",
      "iteration: 172 loss: 0.72216213\n",
      "iteration: 173 loss: 0.83462840\n",
      "iteration: 174 loss: 3.20817041\n",
      "iteration: 175 loss: 2.84314585\n",
      "iteration: 176 loss: 1.82078135\n",
      "iteration: 177 loss: 0.92477560\n",
      "iteration: 178 loss: 1.82156992\n",
      "iteration: 179 loss: 0.61127454\n",
      "iteration: 180 loss: 2.61530924\n",
      "iteration: 181 loss: 1.38023019\n",
      "iteration: 182 loss: 0.96860999\n",
      "iteration: 183 loss: 2.11585236\n",
      "iteration: 184 loss: 0.79795349\n",
      "iteration: 185 loss: 1.65466058\n",
      "iteration: 186 loss: 1.83589935\n",
      "iteration: 187 loss: 0.85275382\n",
      "iteration: 188 loss: 2.96965075\n",
      "iteration: 189 loss: 2.30534506\n",
      "iteration: 190 loss: 2.79398727\n",
      "iteration: 191 loss: 1.49245191\n",
      "iteration: 192 loss: 1.60371017\n",
      "iteration: 193 loss: 1.47493458\n",
      "iteration: 194 loss: 1.20529091\n",
      "iteration: 195 loss: 2.83095169\n",
      "iteration: 196 loss: 2.95792389\n",
      "iteration: 197 loss: 1.40065563\n",
      "iteration: 198 loss: 1.21118557\n",
      "iteration: 199 loss: 2.02346063\n",
      "epoch:  26 mean loss training: 1.68493509\n",
      "epoch:  26 mean loss validation: 1.73769832\n",
      "iteration:   0 loss: 2.31502819\n",
      "iteration:   1 loss: 1.43869793\n",
      "iteration:   2 loss: 1.65064418\n",
      "iteration:   3 loss: 0.68651855\n",
      "iteration:   4 loss: 3.43598557\n",
      "iteration:   5 loss: 3.55788016\n",
      "iteration:   6 loss: 1.97605979\n",
      "iteration:   7 loss: 1.82605314\n",
      "iteration:   8 loss: 2.05462265\n",
      "iteration:   9 loss: 3.49520159\n",
      "iteration:  10 loss: 1.65011001\n",
      "iteration:  11 loss: 1.53321064\n",
      "iteration:  12 loss: 2.50821781\n",
      "iteration:  13 loss: 1.51997006\n",
      "iteration:  14 loss: 2.71175361\n",
      "iteration:  15 loss: 0.85124147\n",
      "iteration:  16 loss: 1.33878529\n",
      "iteration:  17 loss: 1.15676141\n",
      "iteration:  18 loss: 1.76473916\n",
      "iteration:  19 loss: 1.94172883\n",
      "iteration:  20 loss: 0.87577224\n",
      "iteration:  21 loss: 2.36389971\n",
      "iteration:  22 loss: 0.77575243\n",
      "iteration:  23 loss: 1.49609077\n",
      "iteration:  24 loss: 1.37925315\n",
      "iteration:  25 loss: 2.90619349\n",
      "iteration:  26 loss: 1.33345318\n",
      "iteration:  27 loss: 2.93178129\n",
      "iteration:  28 loss: 0.39214101\n",
      "iteration:  29 loss: 1.98004866\n",
      "iteration:  30 loss: 1.25236917\n",
      "iteration:  31 loss: 1.87665260\n",
      "iteration:  32 loss: 0.49228331\n",
      "iteration:  33 loss: 0.99703538\n",
      "iteration:  34 loss: 2.18705130\n",
      "iteration:  35 loss: 0.89207572\n",
      "iteration:  36 loss: 0.30377465\n",
      "iteration:  37 loss: 1.35936522\n",
      "iteration:  38 loss: 0.66696787\n",
      "iteration:  39 loss: 3.82606244\n",
      "iteration:  40 loss: 1.97055829\n",
      "iteration:  41 loss: 1.99624896\n",
      "iteration:  42 loss: 0.70263553\n",
      "iteration:  43 loss: 1.81198728\n",
      "iteration:  44 loss: 0.92408299\n",
      "iteration:  45 loss: 0.46931714\n",
      "iteration:  46 loss: 2.59767795\n",
      "iteration:  47 loss: 1.92397523\n",
      "iteration:  48 loss: 1.06407177\n",
      "iteration:  49 loss: 1.54596901\n",
      "iteration:  50 loss: 2.49165726\n",
      "iteration:  51 loss: 0.66302437\n",
      "iteration:  52 loss: 1.71367383\n",
      "iteration:  53 loss: 0.71363282\n",
      "iteration:  54 loss: 1.25154614\n",
      "iteration:  55 loss: 1.75747001\n",
      "iteration:  56 loss: 1.48122489\n",
      "iteration:  57 loss: 1.47979987\n",
      "iteration:  58 loss: 2.36695433\n",
      "iteration:  59 loss: 1.80738926\n",
      "iteration:  60 loss: 0.95032746\n",
      "iteration:  61 loss: 0.70870399\n",
      "iteration:  62 loss: 2.22607923\n",
      "iteration:  63 loss: 1.41100311\n",
      "iteration:  64 loss: 1.11813307\n",
      "iteration:  65 loss: 2.04999638\n",
      "iteration:  66 loss: 1.41851342\n",
      "iteration:  67 loss: 1.46024692\n",
      "iteration:  68 loss: 1.40835679\n",
      "iteration:  69 loss: 2.35537219\n",
      "iteration:  70 loss: 1.45096660\n",
      "iteration:  71 loss: 1.15244782\n",
      "iteration:  72 loss: 1.73745298\n",
      "iteration:  73 loss: 3.37798476\n",
      "iteration:  74 loss: 1.16439021\n",
      "iteration:  75 loss: 0.51972359\n",
      "iteration:  76 loss: 0.91108137\n",
      "iteration:  77 loss: 2.42912340\n",
      "iteration:  78 loss: 1.47075868\n",
      "iteration:  79 loss: 0.68915606\n",
      "iteration:  80 loss: 1.65987921\n",
      "iteration:  81 loss: 0.38272575\n",
      "iteration:  82 loss: 0.94845301\n",
      "iteration:  83 loss: 1.87454784\n",
      "iteration:  84 loss: 3.19978023\n",
      "iteration:  85 loss: 0.46318641\n",
      "iteration:  86 loss: 0.54307896\n",
      "iteration:  87 loss: 2.98502254\n",
      "iteration:  88 loss: 0.51269931\n",
      "iteration:  89 loss: 2.79960060\n",
      "iteration:  90 loss: 2.82919717\n",
      "iteration:  91 loss: 0.57385361\n",
      "iteration:  92 loss: 0.62728935\n",
      "iteration:  93 loss: 2.45873761\n",
      "iteration:  94 loss: 1.85193419\n",
      "iteration:  95 loss: 0.75629342\n",
      "iteration:  96 loss: 4.19363928\n",
      "iteration:  97 loss: 1.62884903\n",
      "iteration:  98 loss: 0.59591621\n",
      "iteration:  99 loss: 0.45154822\n",
      "iteration: 100 loss: 1.98980105\n",
      "iteration: 101 loss: 0.41920394\n",
      "iteration: 102 loss: 0.88794166\n",
      "iteration: 103 loss: 2.36964011\n",
      "iteration: 104 loss: 0.44262815\n",
      "iteration: 105 loss: 0.85990155\n",
      "iteration: 106 loss: 0.37812054\n",
      "iteration: 107 loss: 2.06254983\n",
      "iteration: 108 loss: 2.30729365\n",
      "iteration: 109 loss: 1.31976938\n",
      "iteration: 110 loss: 3.33736706\n",
      "iteration: 111 loss: 2.97105241\n",
      "iteration: 112 loss: 2.95347095\n",
      "iteration: 113 loss: 2.76709199\n",
      "iteration: 114 loss: 2.18075109\n",
      "iteration: 115 loss: 3.49998307\n",
      "iteration: 116 loss: 1.17567992\n",
      "iteration: 117 loss: 0.50793380\n",
      "iteration: 118 loss: 0.53717870\n",
      "iteration: 119 loss: 1.05342710\n",
      "iteration: 120 loss: 0.40420505\n",
      "iteration: 121 loss: 1.35077059\n",
      "iteration: 122 loss: 0.85458612\n",
      "iteration: 123 loss: 1.47321868\n",
      "iteration: 124 loss: 2.48552942\n",
      "iteration: 125 loss: 2.54691434\n",
      "iteration: 126 loss: 3.53490853\n",
      "iteration: 127 loss: 1.16562819\n",
      "iteration: 128 loss: 2.06646824\n",
      "iteration: 129 loss: 0.77444774\n",
      "iteration: 130 loss: 1.91547322\n",
      "iteration: 131 loss: 2.26343298\n",
      "iteration: 132 loss: 3.15708637\n",
      "iteration: 133 loss: 1.87211144\n",
      "iteration: 134 loss: 2.08842731\n",
      "iteration: 135 loss: 0.93306738\n",
      "iteration: 136 loss: 1.07496440\n",
      "iteration: 137 loss: 1.60699713\n",
      "iteration: 138 loss: 1.18129337\n",
      "iteration: 139 loss: 0.71480310\n",
      "iteration: 140 loss: 2.88451624\n",
      "iteration: 141 loss: 0.75054747\n",
      "iteration: 142 loss: 2.17061853\n",
      "iteration: 143 loss: 2.19521594\n",
      "iteration: 144 loss: 3.35686207\n",
      "iteration: 145 loss: 2.48323131\n",
      "iteration: 146 loss: 2.32523632\n",
      "iteration: 147 loss: 1.06359470\n",
      "iteration: 148 loss: 1.57313025\n",
      "iteration: 149 loss: 2.07212329\n",
      "iteration: 150 loss: 1.29906964\n",
      "iteration: 151 loss: 1.87613595\n",
      "iteration: 152 loss: 3.01514840\n",
      "iteration: 153 loss: 3.52797651\n",
      "iteration: 154 loss: 1.26733375\n",
      "iteration: 155 loss: 2.93393493\n",
      "iteration: 156 loss: 0.42898279\n",
      "iteration: 157 loss: 0.93512058\n",
      "iteration: 158 loss: 0.96544033\n",
      "iteration: 159 loss: 0.41354924\n",
      "iteration: 160 loss: 0.61513382\n",
      "iteration: 161 loss: 1.74170041\n",
      "iteration: 162 loss: 0.48472071\n",
      "iteration: 163 loss: 3.21543741\n",
      "iteration: 164 loss: 3.26768565\n",
      "iteration: 165 loss: 2.88572955\n",
      "iteration: 166 loss: 2.00520110\n",
      "iteration: 167 loss: 0.37587190\n",
      "iteration: 168 loss: 0.82696283\n",
      "iteration: 169 loss: 1.56994414\n",
      "iteration: 170 loss: 2.58761501\n",
      "iteration: 171 loss: 1.41661537\n",
      "iteration: 172 loss: 0.74859774\n",
      "iteration: 173 loss: 0.33893004\n",
      "iteration: 174 loss: 3.47910547\n",
      "iteration: 175 loss: 2.69992208\n",
      "iteration: 176 loss: 2.45230746\n",
      "iteration: 177 loss: 0.94274324\n",
      "iteration: 178 loss: 0.85404104\n",
      "iteration: 179 loss: 0.41928247\n",
      "iteration: 180 loss: 2.31397009\n",
      "iteration: 181 loss: 1.06225920\n",
      "iteration: 182 loss: 0.50937504\n",
      "iteration: 183 loss: 2.28178620\n",
      "iteration: 184 loss: 0.99168861\n",
      "iteration: 185 loss: 2.05208302\n",
      "iteration: 186 loss: 2.02402472\n",
      "iteration: 187 loss: 0.69065356\n",
      "iteration: 188 loss: 3.24242735\n",
      "iteration: 189 loss: 2.36785364\n",
      "iteration: 190 loss: 3.09427762\n",
      "iteration: 191 loss: 1.58728600\n",
      "iteration: 192 loss: 1.07127762\n",
      "iteration: 193 loss: 1.62382960\n",
      "iteration: 194 loss: 0.98335332\n",
      "iteration: 195 loss: 3.13166666\n",
      "iteration: 196 loss: 2.88367295\n",
      "iteration: 197 loss: 1.61983228\n",
      "iteration: 198 loss: 0.97115320\n",
      "iteration: 199 loss: 1.45537901\n",
      "epoch:  27 mean loss training: 1.67844248\n",
      "epoch:  27 mean loss validation: 1.67746150\n",
      "iteration:   0 loss: 2.06069827\n",
      "iteration:   1 loss: 1.24090362\n",
      "iteration:   2 loss: 1.60678422\n",
      "iteration:   3 loss: 0.60255617\n",
      "iteration:   4 loss: 2.84219337\n",
      "iteration:   5 loss: 3.02260447\n",
      "iteration:   6 loss: 1.87926483\n",
      "iteration:   7 loss: 1.56193542\n",
      "iteration:   8 loss: 2.69992542\n",
      "iteration:   9 loss: 3.43183184\n",
      "iteration:  10 loss: 1.20304334\n",
      "iteration:  11 loss: 1.32264400\n",
      "iteration:  12 loss: 2.23835731\n",
      "iteration:  13 loss: 1.40208483\n",
      "iteration:  14 loss: 2.55784941\n",
      "iteration:  15 loss: 0.73849630\n",
      "iteration:  16 loss: 0.99218202\n",
      "iteration:  17 loss: 0.94940037\n",
      "iteration:  18 loss: 1.56968737\n",
      "iteration:  19 loss: 1.69827735\n",
      "iteration:  20 loss: 0.73142946\n",
      "iteration:  21 loss: 2.44940329\n",
      "iteration:  22 loss: 0.58498073\n",
      "iteration:  23 loss: 1.45466888\n",
      "iteration:  24 loss: 0.97580332\n",
      "iteration:  25 loss: 2.83343315\n",
      "iteration:  26 loss: 1.34951186\n",
      "iteration:  27 loss: 2.91003180\n",
      "iteration:  28 loss: 0.38282093\n",
      "iteration:  29 loss: 1.74107337\n",
      "iteration:  30 loss: 1.40764582\n",
      "iteration:  31 loss: 1.82185411\n",
      "iteration:  32 loss: 0.48578313\n",
      "iteration:  33 loss: 1.02972567\n",
      "iteration:  34 loss: 2.82697678\n",
      "iteration:  35 loss: 0.96460968\n",
      "iteration:  36 loss: 0.29851669\n",
      "iteration:  37 loss: 1.29810095\n",
      "iteration:  38 loss: 0.57903618\n",
      "iteration:  39 loss: 4.15677595\n",
      "iteration:  40 loss: 1.92019892\n",
      "iteration:  41 loss: 1.60478735\n",
      "iteration:  42 loss: 0.68513966\n",
      "iteration:  43 loss: 2.87926531\n",
      "iteration:  44 loss: 0.91069508\n",
      "iteration:  45 loss: 0.42078865\n",
      "iteration:  46 loss: 1.69058502\n",
      "iteration:  47 loss: 1.89163446\n",
      "iteration:  48 loss: 1.03657734\n",
      "iteration:  49 loss: 1.07930243\n",
      "iteration:  50 loss: 1.79655945\n",
      "iteration:  51 loss: 0.64507532\n",
      "iteration:  52 loss: 1.52907956\n",
      "iteration:  53 loss: 0.78353441\n",
      "iteration:  54 loss: 1.16451633\n",
      "iteration:  55 loss: 2.15746331\n",
      "iteration:  56 loss: 1.35818589\n",
      "iteration:  57 loss: 1.94900620\n",
      "iteration:  58 loss: 3.18751836\n",
      "iteration:  59 loss: 2.00024080\n",
      "iteration:  60 loss: 0.61866874\n",
      "iteration:  61 loss: 0.59907591\n",
      "iteration:  62 loss: 2.22342968\n",
      "iteration:  63 loss: 1.37601578\n",
      "iteration:  64 loss: 0.90690929\n",
      "iteration:  65 loss: 2.13255763\n",
      "iteration:  66 loss: 1.65094912\n",
      "iteration:  67 loss: 1.07505822\n",
      "iteration:  68 loss: 1.22474635\n",
      "iteration:  69 loss: 1.68684161\n",
      "iteration:  70 loss: 0.53044933\n",
      "iteration:  71 loss: 1.03897071\n",
      "iteration:  72 loss: 1.70080113\n",
      "iteration:  73 loss: 3.25253320\n",
      "iteration:  74 loss: 0.54484469\n",
      "iteration:  75 loss: 0.40108109\n",
      "iteration:  76 loss: 0.53361690\n",
      "iteration:  77 loss: 2.44339490\n",
      "iteration:  78 loss: 1.28030431\n",
      "iteration:  79 loss: 1.34727514\n",
      "iteration:  80 loss: 1.54995179\n",
      "iteration:  81 loss: 0.29277244\n",
      "iteration:  82 loss: 0.49894166\n",
      "iteration:  83 loss: 1.62871408\n",
      "iteration:  84 loss: 2.86335301\n",
      "iteration:  85 loss: 0.37098420\n",
      "iteration:  86 loss: 0.29607800\n",
      "iteration:  87 loss: 2.95946741\n",
      "iteration:  88 loss: 0.37847054\n",
      "iteration:  89 loss: 2.81860662\n",
      "iteration:  90 loss: 2.96000433\n",
      "iteration:  91 loss: 0.69124514\n",
      "iteration:  92 loss: 0.58993763\n",
      "iteration:  93 loss: 2.64388895\n",
      "iteration:  94 loss: 2.00453091\n",
      "iteration:  95 loss: 0.36956829\n",
      "iteration:  96 loss: 4.00447226\n",
      "iteration:  97 loss: 1.38498068\n",
      "iteration:  98 loss: 0.58783519\n",
      "iteration:  99 loss: 0.33520573\n",
      "iteration: 100 loss: 1.81926572\n",
      "iteration: 101 loss: 0.54889858\n",
      "iteration: 102 loss: 1.42357457\n",
      "iteration: 103 loss: 2.72900558\n",
      "iteration: 104 loss: 0.34388199\n",
      "iteration: 105 loss: 0.78486001\n",
      "iteration: 106 loss: 0.34370327\n",
      "iteration: 107 loss: 1.85603416\n",
      "iteration: 108 loss: 1.61314130\n",
      "iteration: 109 loss: 1.16718245\n",
      "iteration: 110 loss: 3.19681048\n",
      "iteration: 111 loss: 2.89434719\n",
      "iteration: 112 loss: 2.93164897\n",
      "iteration: 113 loss: 1.52476442\n",
      "iteration: 114 loss: 2.36578941\n",
      "iteration: 115 loss: 3.12249684\n",
      "iteration: 116 loss: 1.78502071\n",
      "iteration: 117 loss: 0.61301917\n",
      "iteration: 118 loss: 0.76156557\n",
      "iteration: 119 loss: 1.15361273\n",
      "iteration: 120 loss: 0.43646428\n",
      "iteration: 121 loss: 1.13026416\n",
      "iteration: 122 loss: 1.18842208\n",
      "iteration: 123 loss: 2.35558152\n",
      "iteration: 124 loss: 2.25122619\n",
      "iteration: 125 loss: 2.59956312\n",
      "iteration: 126 loss: 3.29503345\n",
      "iteration: 127 loss: 1.14815700\n",
      "iteration: 128 loss: 1.94900513\n",
      "iteration: 129 loss: 0.59382689\n",
      "iteration: 130 loss: 2.31035638\n",
      "iteration: 131 loss: 2.03967595\n",
      "iteration: 132 loss: 2.67018151\n",
      "iteration: 133 loss: 1.75558245\n",
      "iteration: 134 loss: 1.16775191\n",
      "iteration: 135 loss: 0.92004722\n",
      "iteration: 136 loss: 1.00897133\n",
      "iteration: 137 loss: 1.48398149\n",
      "iteration: 138 loss: 0.69670463\n",
      "iteration: 139 loss: 0.98168868\n",
      "iteration: 140 loss: 2.43300772\n",
      "iteration: 141 loss: 0.93049479\n",
      "iteration: 142 loss: 0.81171346\n",
      "iteration: 143 loss: 1.76281643\n",
      "iteration: 144 loss: 2.99415278\n",
      "iteration: 145 loss: 2.09065866\n",
      "iteration: 146 loss: 0.61160338\n",
      "iteration: 147 loss: 0.68813872\n",
      "iteration: 148 loss: 0.80534595\n",
      "iteration: 149 loss: 1.83840096\n",
      "iteration: 150 loss: 1.35654020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 151 loss: 2.18058085\n",
      "iteration: 152 loss: 3.57131910\n",
      "iteration: 153 loss: 3.25223827\n",
      "iteration: 154 loss: 1.57895768\n",
      "iteration: 155 loss: 3.04657555\n",
      "iteration: 156 loss: 0.66955817\n",
      "iteration: 157 loss: 1.22493505\n",
      "iteration: 158 loss: 1.14820611\n",
      "iteration: 159 loss: 0.45882505\n",
      "iteration: 160 loss: 0.51565242\n",
      "iteration: 161 loss: 1.33411670\n",
      "iteration: 162 loss: 0.52141374\n",
      "iteration: 163 loss: 3.05578709\n",
      "iteration: 164 loss: 3.08499289\n",
      "iteration: 165 loss: 2.27795649\n",
      "iteration: 166 loss: 0.54471880\n",
      "iteration: 167 loss: 1.60231054\n",
      "iteration: 168 loss: 1.15044749\n",
      "iteration: 169 loss: 1.01032710\n",
      "iteration: 170 loss: 3.33049846\n",
      "iteration: 171 loss: 1.27938008\n",
      "iteration: 172 loss: 0.93035454\n",
      "iteration: 173 loss: 0.83232981\n",
      "iteration: 174 loss: 3.32095361\n",
      "iteration: 175 loss: 2.56970382\n",
      "iteration: 176 loss: 2.54639459\n",
      "iteration: 177 loss: 0.92421907\n",
      "iteration: 178 loss: 0.69595265\n",
      "iteration: 179 loss: 0.52759552\n",
      "iteration: 180 loss: 2.10711074\n",
      "iteration: 181 loss: 0.96008587\n",
      "iteration: 182 loss: 0.47421321\n",
      "iteration: 183 loss: 2.06764674\n",
      "iteration: 184 loss: 1.28902173\n",
      "iteration: 185 loss: 2.02560782\n",
      "iteration: 186 loss: 2.18919826\n",
      "iteration: 187 loss: 0.45247602\n",
      "iteration: 188 loss: 3.70607066\n",
      "iteration: 189 loss: 2.98530245\n",
      "iteration: 190 loss: 3.53718257\n",
      "iteration: 191 loss: 1.29375350\n",
      "iteration: 192 loss: 1.19086897\n",
      "iteration: 193 loss: 1.48232794\n",
      "iteration: 194 loss: 0.82785904\n",
      "iteration: 195 loss: 3.29165554\n",
      "iteration: 196 loss: 2.95666552\n",
      "iteration: 197 loss: 1.34613252\n",
      "iteration: 198 loss: 1.13375807\n",
      "iteration: 199 loss: 1.17990541\n",
      "epoch:  28 mean loss training: 1.59678340\n",
      "epoch:  28 mean loss validation: 1.57537460\n",
      "iteration:   0 loss: 2.67947245\n",
      "iteration:   1 loss: 2.41289854\n",
      "iteration:   2 loss: 1.53260767\n",
      "iteration:   3 loss: 0.65435630\n",
      "iteration:   4 loss: 3.62661457\n",
      "iteration:   5 loss: 3.03642631\n",
      "iteration:   6 loss: 2.07258368\n",
      "iteration:   7 loss: 1.51460826\n",
      "iteration:   8 loss: 2.80069399\n",
      "iteration:   9 loss: 3.49159479\n",
      "iteration:  10 loss: 0.99600244\n",
      "iteration:  11 loss: 1.02516961\n",
      "iteration:  12 loss: 2.17114973\n",
      "iteration:  13 loss: 1.17840183\n",
      "iteration:  14 loss: 1.17069685\n",
      "iteration:  15 loss: 0.79141152\n",
      "iteration:  16 loss: 1.09784579\n",
      "iteration:  17 loss: 0.94576520\n",
      "iteration:  18 loss: 1.65313685\n",
      "iteration:  19 loss: 0.85087764\n",
      "iteration:  20 loss: 0.75041747\n",
      "iteration:  21 loss: 2.17984653\n",
      "iteration:  22 loss: 0.54841322\n",
      "iteration:  23 loss: 1.23862684\n",
      "iteration:  24 loss: 0.91707242\n",
      "iteration:  25 loss: 2.82961321\n",
      "iteration:  26 loss: 1.49710286\n",
      "iteration:  27 loss: 2.91102242\n",
      "iteration:  28 loss: 0.37995097\n",
      "iteration:  29 loss: 1.84502745\n",
      "iteration:  30 loss: 1.42814457\n",
      "iteration:  31 loss: 1.45027447\n",
      "iteration:  32 loss: 0.46495607\n",
      "iteration:  33 loss: 0.87398052\n",
      "iteration:  34 loss: 2.24327946\n",
      "iteration:  35 loss: 0.97060984\n",
      "iteration:  36 loss: 0.31001127\n",
      "iteration:  37 loss: 0.74906337\n",
      "iteration:  38 loss: 1.45719719\n",
      "iteration:  39 loss: 3.40115333\n",
      "iteration:  40 loss: 2.33139348\n",
      "iteration:  41 loss: 1.63809335\n",
      "iteration:  42 loss: 1.13953745\n",
      "iteration:  43 loss: 1.88156700\n",
      "iteration:  44 loss: 0.86641127\n",
      "iteration:  45 loss: 0.78897196\n",
      "iteration:  46 loss: 1.69254696\n",
      "iteration:  47 loss: 2.03505158\n",
      "iteration:  48 loss: 1.40802145\n",
      "iteration:  49 loss: 1.14657629\n",
      "iteration:  50 loss: 1.51612175\n",
      "iteration:  51 loss: 0.74383450\n",
      "iteration:  52 loss: 1.55239701\n",
      "iteration:  53 loss: 0.59680885\n",
      "iteration:  54 loss: 0.98981607\n",
      "iteration:  55 loss: 3.19523835\n",
      "iteration:  56 loss: 0.96381241\n",
      "iteration:  57 loss: 1.40443909\n",
      "iteration:  58 loss: 3.84889340\n",
      "iteration:  59 loss: 1.51591921\n",
      "iteration:  60 loss: 0.60192513\n",
      "iteration:  61 loss: 0.63708836\n",
      "iteration:  62 loss: 0.82924682\n",
      "iteration:  63 loss: 0.88095552\n",
      "iteration:  64 loss: 0.56949538\n",
      "iteration:  65 loss: 2.81455350\n",
      "iteration:  66 loss: 0.78703594\n",
      "iteration:  67 loss: 0.41114566\n",
      "iteration:  68 loss: 0.33354023\n",
      "iteration:  69 loss: 0.64155006\n",
      "iteration:  70 loss: 0.33161277\n",
      "iteration:  71 loss: 0.63285524\n",
      "iteration:  72 loss: 1.41804361\n",
      "iteration:  73 loss: 2.24900126\n",
      "iteration:  74 loss: 0.80164582\n",
      "iteration:  75 loss: 0.40427315\n",
      "iteration:  76 loss: 0.63225579\n",
      "iteration:  77 loss: 1.17241371\n",
      "iteration:  78 loss: 1.59507322\n",
      "iteration:  79 loss: 0.75587338\n",
      "iteration:  80 loss: 1.85881388\n",
      "iteration:  81 loss: 0.29297173\n",
      "iteration:  82 loss: 0.68137312\n",
      "iteration:  83 loss: 1.76517308\n",
      "iteration:  84 loss: 3.04896951\n",
      "iteration:  85 loss: 0.36908162\n",
      "iteration:  86 loss: 0.62674296\n",
      "iteration:  87 loss: 3.02778506\n",
      "iteration:  88 loss: 0.28538817\n",
      "iteration:  89 loss: 2.66366863\n",
      "iteration:  90 loss: 2.84422517\n",
      "iteration:  91 loss: 0.56187242\n",
      "iteration:  92 loss: 0.64203513\n",
      "iteration:  93 loss: 2.01833415\n",
      "iteration:  94 loss: 2.03499103\n",
      "iteration:  95 loss: 0.56682938\n",
      "iteration:  96 loss: 4.15437937\n",
      "iteration:  97 loss: 1.48067951\n",
      "iteration:  98 loss: 0.53829968\n",
      "iteration:  99 loss: 0.43539262\n",
      "iteration: 100 loss: 1.16950309\n",
      "iteration: 101 loss: 0.48685929\n",
      "iteration: 102 loss: 0.88493389\n",
      "iteration: 103 loss: 2.08541131\n",
      "iteration: 104 loss: 0.41007555\n",
      "iteration: 105 loss: 0.81450969\n",
      "iteration: 106 loss: 0.41883901\n",
      "iteration: 107 loss: 1.76288784\n",
      "iteration: 108 loss: 1.55706525\n",
      "iteration: 109 loss: 0.87714809\n",
      "iteration: 110 loss: 3.21871662\n",
      "iteration: 111 loss: 2.98002315\n",
      "iteration: 112 loss: 3.39711213\n",
      "iteration: 113 loss: 2.82567310\n",
      "iteration: 114 loss: 2.19310474\n",
      "iteration: 115 loss: 3.96850157\n",
      "iteration: 116 loss: 1.22785437\n",
      "iteration: 117 loss: 0.39226726\n",
      "iteration: 118 loss: 1.00754619\n",
      "iteration: 119 loss: 0.94917995\n",
      "iteration: 120 loss: 0.52346939\n",
      "iteration: 121 loss: 0.80032051\n",
      "iteration: 122 loss: 0.53885669\n",
      "iteration: 123 loss: 2.36939740\n",
      "iteration: 124 loss: 2.42296243\n",
      "iteration: 125 loss: 2.54481840\n",
      "iteration: 126 loss: 3.38738465\n",
      "iteration: 127 loss: 1.36233902\n",
      "iteration: 128 loss: 2.02480102\n",
      "iteration: 129 loss: 0.94812924\n",
      "iteration: 130 loss: 1.85483634\n",
      "iteration: 131 loss: 2.16120100\n",
      "iteration: 132 loss: 2.86719012\n",
      "iteration: 133 loss: 1.46557319\n",
      "iteration: 134 loss: 1.11116397\n",
      "iteration: 135 loss: 1.24900961\n",
      "iteration: 136 loss: 1.10880685\n",
      "iteration: 137 loss: 2.51487255\n",
      "iteration: 138 loss: 0.91533470\n",
      "iteration: 139 loss: 0.94449282\n",
      "iteration: 140 loss: 2.74742723\n",
      "iteration: 141 loss: 0.83054614\n",
      "iteration: 142 loss: 0.98171425\n",
      "iteration: 143 loss: 1.52472162\n",
      "iteration: 144 loss: 3.44468665\n",
      "iteration: 145 loss: 2.67677450\n",
      "iteration: 146 loss: 0.74271548\n",
      "iteration: 147 loss: 0.94001150\n",
      "iteration: 148 loss: 1.64343858\n",
      "iteration: 149 loss: 1.21993554\n",
      "iteration: 150 loss: 1.25037646\n",
      "iteration: 151 loss: 1.70085454\n",
      "iteration: 152 loss: 2.75583315\n",
      "iteration: 153 loss: 3.22032261\n",
      "iteration: 154 loss: 1.31116343\n",
      "iteration: 155 loss: 2.78792953\n",
      "iteration: 156 loss: 0.46735144\n",
      "iteration: 157 loss: 0.81054384\n",
      "iteration: 158 loss: 0.88024443\n",
      "iteration: 159 loss: 0.42216584\n",
      "iteration: 160 loss: 0.58277518\n",
      "iteration: 161 loss: 2.00715661\n",
      "iteration: 162 loss: 0.50973231\n",
      "iteration: 163 loss: 4.17312479\n",
      "iteration: 164 loss: 3.40744567\n",
      "iteration: 165 loss: 1.85438216\n",
      "iteration: 166 loss: 0.77873552\n",
      "iteration: 167 loss: 0.31786966\n",
      "iteration: 168 loss: 1.13130462\n",
      "iteration: 169 loss: 1.02607691\n",
      "iteration: 170 loss: 2.71242881\n",
      "iteration: 171 loss: 0.91760772\n",
      "iteration: 172 loss: 0.94315338\n",
      "iteration: 173 loss: 0.49309170\n",
      "iteration: 174 loss: 3.43603230\n",
      "iteration: 175 loss: 2.10515642\n",
      "iteration: 176 loss: 1.81195593\n",
      "iteration: 177 loss: 0.97544116\n",
      "iteration: 178 loss: 0.75651771\n",
      "iteration: 179 loss: 0.42891568\n",
      "iteration: 180 loss: 1.51995814\n",
      "iteration: 181 loss: 0.97608322\n",
      "iteration: 182 loss: 0.51432770\n",
      "iteration: 183 loss: 2.48379135\n",
      "iteration: 184 loss: 1.12186038\n",
      "iteration: 185 loss: 1.92716265\n",
      "iteration: 186 loss: 2.17034197\n",
      "iteration: 187 loss: 0.84722531\n",
      "iteration: 188 loss: 3.65847468\n",
      "iteration: 189 loss: 2.10547924\n",
      "iteration: 190 loss: 3.17193103\n",
      "iteration: 191 loss: 1.49125814\n",
      "iteration: 192 loss: 0.99966151\n",
      "iteration: 193 loss: 1.31241119\n",
      "iteration: 194 loss: 0.94358820\n",
      "iteration: 195 loss: 3.08966279\n",
      "iteration: 196 loss: 3.43176651\n",
      "iteration: 197 loss: 0.80694306\n",
      "iteration: 198 loss: 1.12188435\n",
      "iteration: 199 loss: 1.24162006\n",
      "epoch:  29 mean loss training: 1.53602672\n",
      "epoch:  29 mean loss validation: 1.60812831\n",
      "iteration:   0 loss: 2.48298693\n",
      "iteration:   1 loss: 1.93954945\n",
      "iteration:   2 loss: 1.64010859\n",
      "iteration:   3 loss: 0.78420818\n",
      "iteration:   4 loss: 3.53463125\n",
      "iteration:   5 loss: 3.01700687\n",
      "iteration:   6 loss: 2.13427496\n",
      "iteration:   7 loss: 1.63381469\n",
      "iteration:   8 loss: 2.80484128\n",
      "iteration:   9 loss: 3.44670415\n",
      "iteration:  10 loss: 1.01946533\n",
      "iteration:  11 loss: 1.05584061\n",
      "iteration:  12 loss: 1.97823858\n",
      "iteration:  13 loss: 1.50095558\n",
      "iteration:  14 loss: 1.25328612\n",
      "iteration:  15 loss: 0.79983974\n",
      "iteration:  16 loss: 1.15575445\n",
      "iteration:  17 loss: 0.99695152\n",
      "iteration:  18 loss: 1.69429028\n",
      "iteration:  19 loss: 0.81560296\n",
      "iteration:  20 loss: 0.97836566\n",
      "iteration:  21 loss: 2.59310150\n",
      "iteration:  22 loss: 0.53009075\n",
      "iteration:  23 loss: 1.10159874\n",
      "iteration:  24 loss: 0.91093278\n",
      "iteration:  25 loss: 2.80885696\n",
      "iteration:  26 loss: 1.51716959\n",
      "iteration:  27 loss: 2.73237872\n",
      "iteration:  28 loss: 0.39029348\n",
      "iteration:  29 loss: 1.45278239\n",
      "iteration:  30 loss: 1.49158537\n",
      "iteration:  31 loss: 1.48001611\n",
      "iteration:  32 loss: 0.48771966\n",
      "iteration:  33 loss: 0.68879312\n",
      "iteration:  34 loss: 2.38508606\n",
      "iteration:  35 loss: 1.12239921\n",
      "iteration:  36 loss: 0.27032328\n",
      "iteration:  37 loss: 0.49673942\n",
      "iteration:  38 loss: 1.22373772\n",
      "iteration:  39 loss: 3.41324472\n",
      "iteration:  40 loss: 2.11079669\n",
      "iteration:  41 loss: 1.32224095\n",
      "iteration:  42 loss: 1.18287742\n",
      "iteration:  43 loss: 2.97394371\n",
      "iteration:  44 loss: 0.84662139\n",
      "iteration:  45 loss: 0.76395935\n",
      "iteration:  46 loss: 1.30970812\n",
      "iteration:  47 loss: 1.95597363\n",
      "iteration:  48 loss: 1.25334001\n",
      "iteration:  49 loss: 1.21461761\n",
      "iteration:  50 loss: 2.05537915\n",
      "iteration:  51 loss: 0.63642073\n",
      "iteration:  52 loss: 1.51799786\n",
      "iteration:  53 loss: 1.03243351\n",
      "iteration:  54 loss: 1.00861526\n",
      "iteration:  55 loss: 2.09830070\n",
      "iteration:  56 loss: 1.34906864\n",
      "iteration:  57 loss: 1.29576159\n",
      "iteration:  58 loss: 3.92657971\n",
      "iteration:  59 loss: 2.84333038\n",
      "iteration:  60 loss: 0.73132491\n",
      "iteration:  61 loss: 0.87464529\n",
      "iteration:  62 loss: 1.00707829\n",
      "iteration:  63 loss: 1.45896685\n",
      "iteration:  64 loss: 0.53680933\n",
      "iteration:  65 loss: 1.10286486\n",
      "iteration:  66 loss: 0.77474660\n",
      "iteration:  67 loss: 0.39222863\n",
      "iteration:  68 loss: 0.30354726\n",
      "iteration:  69 loss: 1.08384085\n",
      "iteration:  70 loss: 1.05502260\n",
      "iteration:  71 loss: 0.70969802\n",
      "iteration:  72 loss: 1.34651291\n",
      "iteration:  73 loss: 2.31559706\n",
      "iteration:  74 loss: 0.94664121\n",
      "iteration:  75 loss: 0.63471866\n",
      "iteration:  76 loss: 0.64245337\n",
      "iteration:  77 loss: 1.07290661\n",
      "iteration:  78 loss: 1.48892307\n",
      "iteration:  79 loss: 0.75493848\n",
      "iteration:  80 loss: 1.64859319\n",
      "iteration:  81 loss: 0.33044705\n",
      "iteration:  82 loss: 0.86879879\n",
      "iteration:  83 loss: 1.65682650\n",
      "iteration:  84 loss: 2.94412684\n",
      "iteration:  85 loss: 0.38071123\n",
      "iteration:  86 loss: 0.58690667\n",
      "iteration:  87 loss: 3.07165742\n",
      "iteration:  88 loss: 0.28589398\n",
      "iteration:  89 loss: 2.55533814\n",
      "iteration:  90 loss: 2.77637649\n",
      "iteration:  91 loss: 0.62299782\n",
      "iteration:  92 loss: 0.74813890\n",
      "iteration:  93 loss: 1.80142951\n",
      "iteration:  94 loss: 1.73983026\n",
      "iteration:  95 loss: 0.53678161\n",
      "iteration:  96 loss: 4.09695721\n",
      "iteration:  97 loss: 1.21301341\n",
      "iteration:  98 loss: 0.48409858\n",
      "iteration:  99 loss: 0.32092065\n",
      "iteration: 100 loss: 1.13292325\n",
      "iteration: 101 loss: 0.42622572\n",
      "iteration: 102 loss: 0.87606961\n",
      "iteration: 103 loss: 2.73087454\n",
      "iteration: 104 loss: 0.38300875\n",
      "iteration: 105 loss: 0.78202927\n",
      "iteration: 106 loss: 0.34202507\n",
      "iteration: 107 loss: 1.60351348\n",
      "iteration: 108 loss: 1.74116755\n",
      "iteration: 109 loss: 1.25732386\n",
      "iteration: 110 loss: 3.16495085\n",
      "iteration: 111 loss: 2.99728894\n",
      "iteration: 112 loss: 3.85698700\n",
      "iteration: 113 loss: 1.91345656\n",
      "iteration: 114 loss: 2.30785632\n",
      "iteration: 115 loss: 4.02136135\n",
      "iteration: 116 loss: 1.84969175\n",
      "iteration: 117 loss: 0.52484745\n",
      "iteration: 118 loss: 0.53724724\n",
      "iteration: 119 loss: 0.97594374\n",
      "iteration: 120 loss: 0.38679612\n",
      "iteration: 121 loss: 0.80071938\n",
      "iteration: 122 loss: 0.46222726\n",
      "iteration: 123 loss: 1.99641979\n",
      "iteration: 124 loss: 2.26636481\n",
      "iteration: 125 loss: 3.23150992\n",
      "iteration: 126 loss: 3.20540118\n",
      "iteration: 127 loss: 1.11927497\n",
      "iteration: 128 loss: 2.15710855\n",
      "iteration: 129 loss: 0.69403052\n",
      "iteration: 130 loss: 2.36350632\n",
      "iteration: 131 loss: 2.24645615\n",
      "iteration: 132 loss: 2.42943358\n",
      "iteration: 133 loss: 1.51702857\n",
      "iteration: 134 loss: 1.19162345\n",
      "iteration: 135 loss: 0.89572477\n",
      "iteration: 136 loss: 1.00185239\n",
      "iteration: 137 loss: 1.35749555\n",
      "iteration: 138 loss: 0.83393389\n",
      "iteration: 139 loss: 0.97174913\n",
      "iteration: 140 loss: 2.47731590\n",
      "iteration: 141 loss: 1.12363863\n",
      "iteration: 142 loss: 1.07902908\n",
      "iteration: 143 loss: 1.58871245\n",
      "iteration: 144 loss: 3.05605221\n",
      "iteration: 145 loss: 2.25811601\n",
      "iteration: 146 loss: 0.82459700\n",
      "iteration: 147 loss: 0.65881056\n",
      "iteration: 148 loss: 0.97505438\n",
      "iteration: 149 loss: 1.79348969\n",
      "iteration: 150 loss: 1.42910492\n",
      "iteration: 151 loss: 2.30401349\n",
      "iteration: 152 loss: 2.94168258\n",
      "iteration: 153 loss: 3.02355552\n",
      "iteration: 154 loss: 1.24867606\n",
      "iteration: 155 loss: 2.87773275\n",
      "iteration: 156 loss: 0.57462585\n",
      "iteration: 157 loss: 1.06766355\n",
      "iteration: 158 loss: 1.05285382\n",
      "iteration: 159 loss: 0.51361126\n",
      "iteration: 160 loss: 0.55819398\n",
      "iteration: 161 loss: 1.36063802\n",
      "iteration: 162 loss: 0.58839178\n",
      "iteration: 163 loss: 3.25653434\n",
      "iteration: 164 loss: 2.84037519\n",
      "iteration: 165 loss: 1.72253799\n",
      "iteration: 166 loss: 0.67197913\n",
      "iteration: 167 loss: 1.41365075\n",
      "iteration: 168 loss: 0.89082426\n",
      "iteration: 169 loss: 2.27205062\n",
      "iteration: 170 loss: 3.09157467\n",
      "iteration: 171 loss: 1.65046394\n",
      "iteration: 172 loss: 0.59157568\n",
      "iteration: 173 loss: 0.64079458\n",
      "iteration: 174 loss: 3.01864433\n",
      "iteration: 175 loss: 2.76385760\n",
      "iteration: 176 loss: 2.27242637\n",
      "iteration: 177 loss: 0.91771978\n",
      "iteration: 178 loss: 0.58569247\n",
      "iteration: 179 loss: 0.48730633\n",
      "iteration: 180 loss: 2.01840830\n",
      "iteration: 181 loss: 1.02109778\n",
      "iteration: 182 loss: 0.34507921\n",
      "iteration: 183 loss: 2.48363519\n",
      "iteration: 184 loss: 1.14805865\n",
      "iteration: 185 loss: 2.28699708\n",
      "iteration: 186 loss: 2.82821226\n",
      "iteration: 187 loss: 0.48523355\n",
      "iteration: 188 loss: 3.46469927\n",
      "iteration: 189 loss: 2.17334676\n",
      "iteration: 190 loss: 2.88683367\n",
      "iteration: 191 loss: 1.59511244\n",
      "iteration: 192 loss: 0.98383236\n",
      "iteration: 193 loss: 1.40173471\n",
      "iteration: 194 loss: 0.92125154\n",
      "iteration: 195 loss: 3.28820467\n",
      "iteration: 196 loss: 2.98296428\n",
      "iteration: 197 loss: 1.49238455\n",
      "iteration: 198 loss: 0.89821190\n",
      "iteration: 199 loss: 1.18954468\n",
      "epoch:  30 mean loss training: 1.54638362\n",
      "epoch:  30 mean loss validation: 1.56616688\n",
      "iteration:   0 loss: 2.44624233\n",
      "iteration:   1 loss: 1.39694655\n",
      "iteration:   2 loss: 1.57756352\n",
      "iteration:   3 loss: 0.73044217\n",
      "iteration:   4 loss: 2.56241870\n",
      "iteration:   5 loss: 3.52905846\n",
      "iteration:   6 loss: 2.39446020\n",
      "iteration:   7 loss: 1.70525086\n",
      "iteration:   8 loss: 2.35361958\n",
      "iteration:   9 loss: 3.61932874\n",
      "iteration:  10 loss: 1.41708636\n",
      "iteration:  11 loss: 1.18277419\n",
      "iteration:  12 loss: 2.05341721\n",
      "iteration:  13 loss: 1.62008452\n",
      "iteration:  14 loss: 1.94914269\n",
      "iteration:  15 loss: 0.94179887\n",
      "iteration:  16 loss: 1.43471193\n",
      "iteration:  17 loss: 1.27313662\n",
      "iteration:  18 loss: 1.69622886\n",
      "iteration:  19 loss: 1.64308524\n",
      "iteration:  20 loss: 0.75275767\n",
      "iteration:  21 loss: 2.37810040\n",
      "iteration:  22 loss: 0.42934719\n",
      "iteration:  23 loss: 1.34745836\n",
      "iteration:  24 loss: 1.04514527\n",
      "iteration:  25 loss: 3.11694956\n",
      "iteration:  26 loss: 1.32959998\n",
      "iteration:  27 loss: 3.02853870\n",
      "iteration:  28 loss: 0.34202841\n",
      "iteration:  29 loss: 1.59156668\n",
      "iteration:  30 loss: 1.40026307\n",
      "iteration:  31 loss: 0.54557478\n",
      "iteration:  32 loss: 0.39490479\n",
      "iteration:  33 loss: 0.78486556\n",
      "iteration:  34 loss: 2.14923859\n",
      "iteration:  35 loss: 1.14801264\n",
      "iteration:  36 loss: 0.31451160\n",
      "iteration:  37 loss: 0.79564768\n",
      "iteration:  38 loss: 1.19702494\n",
      "iteration:  39 loss: 3.49224520\n",
      "iteration:  40 loss: 1.98364055\n",
      "iteration:  41 loss: 0.94293511\n",
      "iteration:  42 loss: 1.05770850\n",
      "iteration:  43 loss: 2.89378548\n",
      "iteration:  44 loss: 0.65423024\n",
      "iteration:  45 loss: 0.85162276\n",
      "iteration:  46 loss: 1.58655906\n",
      "iteration:  47 loss: 1.84511721\n",
      "iteration:  48 loss: 0.89830190\n",
      "iteration:  49 loss: 1.07792521\n",
      "iteration:  50 loss: 1.74420869\n",
      "iteration:  51 loss: 0.50617892\n",
      "iteration:  52 loss: 1.58042526\n",
      "iteration:  53 loss: 0.44243887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  54 loss: 0.90897411\n",
      "iteration:  55 loss: 3.24737239\n",
      "iteration:  56 loss: 0.90620232\n",
      "iteration:  57 loss: 1.29594707\n",
      "iteration:  58 loss: 3.85400271\n",
      "iteration:  59 loss: 1.57969153\n",
      "iteration:  60 loss: 0.57304031\n",
      "iteration:  61 loss: 0.71587336\n",
      "iteration:  62 loss: 0.96914190\n",
      "iteration:  63 loss: 1.00898075\n",
      "iteration:  64 loss: 0.54186547\n",
      "iteration:  65 loss: 2.07586360\n",
      "iteration:  66 loss: 0.93618739\n",
      "iteration:  67 loss: 0.60476953\n",
      "iteration:  68 loss: 0.43276507\n",
      "iteration:  69 loss: 0.97195154\n",
      "iteration:  70 loss: 0.48834696\n",
      "iteration:  71 loss: 0.72916943\n",
      "iteration:  72 loss: 1.32414186\n",
      "iteration:  73 loss: 2.83516860\n",
      "iteration:  74 loss: 1.08137953\n",
      "iteration:  75 loss: 0.72753060\n",
      "iteration:  76 loss: 0.70580930\n",
      "iteration:  77 loss: 0.95518285\n",
      "iteration:  78 loss: 1.62169826\n",
      "iteration:  79 loss: 0.77623832\n",
      "iteration:  80 loss: 2.01739669\n",
      "iteration:  81 loss: 0.37738043\n",
      "iteration:  82 loss: 1.15612590\n",
      "iteration:  83 loss: 1.12230468\n",
      "iteration:  84 loss: 3.06574082\n",
      "iteration:  85 loss: 0.39118937\n",
      "iteration:  86 loss: 0.83927262\n",
      "iteration:  87 loss: 3.20558381\n",
      "iteration:  88 loss: 0.39992300\n",
      "iteration:  89 loss: 3.10742521\n",
      "iteration:  90 loss: 2.61263752\n",
      "iteration:  91 loss: 0.65346628\n",
      "iteration:  92 loss: 0.89570934\n",
      "iteration:  93 loss: 2.41293645\n",
      "iteration:  94 loss: 1.00993121\n",
      "iteration:  95 loss: 0.85682368\n",
      "iteration:  96 loss: 4.15249825\n",
      "iteration:  97 loss: 1.00141096\n",
      "iteration:  98 loss: 0.65954435\n",
      "iteration:  99 loss: 0.54758245\n",
      "iteration: 100 loss: 1.26895952\n",
      "iteration: 101 loss: 0.46768355\n",
      "iteration: 102 loss: 0.92075390\n",
      "iteration: 103 loss: 2.75228238\n",
      "iteration: 104 loss: 0.62187016\n",
      "iteration: 105 loss: 0.80388165\n",
      "iteration: 106 loss: 0.44394696\n",
      "iteration: 107 loss: 1.41045487\n",
      "iteration: 108 loss: 1.66870642\n",
      "iteration: 109 loss: 1.12138188\n",
      "iteration: 110 loss: 3.07451487\n",
      "iteration: 111 loss: 2.99801397\n",
      "iteration: 112 loss: 3.15739965\n",
      "iteration: 113 loss: 1.79233348\n",
      "iteration: 114 loss: 2.49612522\n",
      "iteration: 115 loss: 3.72538781\n",
      "iteration: 116 loss: 1.87517643\n",
      "iteration: 117 loss: 0.32968879\n",
      "iteration: 118 loss: 0.72109872\n",
      "iteration: 119 loss: 1.04415226\n",
      "iteration: 120 loss: 0.40305546\n",
      "iteration: 121 loss: 0.67534399\n",
      "iteration: 122 loss: 0.52082568\n",
      "iteration: 123 loss: 2.53559089\n",
      "iteration: 124 loss: 1.77562428\n",
      "iteration: 125 loss: 2.42600870\n",
      "iteration: 126 loss: 2.76782870\n",
      "iteration: 127 loss: 1.10034347\n",
      "iteration: 128 loss: 2.05404925\n",
      "iteration: 129 loss: 0.49678174\n",
      "iteration: 130 loss: 2.61259031\n",
      "iteration: 131 loss: 1.90986431\n",
      "iteration: 132 loss: 1.81614208\n",
      "iteration: 133 loss: 0.83321279\n",
      "iteration: 134 loss: 0.76196766\n",
      "iteration: 135 loss: 1.32028341\n",
      "iteration: 136 loss: 1.22220385\n",
      "iteration: 137 loss: 1.59149551\n",
      "iteration: 138 loss: 1.14841306\n",
      "iteration: 139 loss: 1.06770372\n",
      "iteration: 140 loss: 1.77939141\n",
      "iteration: 141 loss: 1.50221872\n",
      "iteration: 142 loss: 1.05329955\n",
      "iteration: 143 loss: 0.95485979\n",
      "iteration: 144 loss: 3.43368244\n",
      "iteration: 145 loss: 2.78865147\n",
      "iteration: 146 loss: 1.09880614\n",
      "iteration: 147 loss: 1.04862285\n",
      "iteration: 148 loss: 1.06488788\n",
      "iteration: 149 loss: 1.60637403\n",
      "iteration: 150 loss: 1.97263229\n",
      "iteration: 151 loss: 1.61212587\n",
      "iteration: 152 loss: 3.66506839\n",
      "iteration: 153 loss: 3.32260633\n",
      "iteration: 154 loss: 1.73977494\n",
      "iteration: 155 loss: 2.92951441\n",
      "iteration: 156 loss: 0.49374679\n",
      "iteration: 157 loss: 0.97341251\n",
      "iteration: 158 loss: 0.93324769\n",
      "iteration: 159 loss: 0.49317673\n",
      "iteration: 160 loss: 0.58201134\n",
      "iteration: 161 loss: 1.55028367\n",
      "iteration: 162 loss: 0.49252918\n",
      "iteration: 163 loss: 3.24483275\n",
      "iteration: 164 loss: 3.43613434\n",
      "iteration: 165 loss: 2.21026206\n",
      "iteration: 166 loss: 0.76183170\n",
      "iteration: 167 loss: 0.34123132\n",
      "iteration: 168 loss: 0.76649940\n",
      "iteration: 169 loss: 1.17266738\n",
      "iteration: 170 loss: 3.08651876\n",
      "iteration: 171 loss: 1.05171764\n",
      "iteration: 172 loss: 0.69068909\n",
      "iteration: 173 loss: 0.46288279\n",
      "iteration: 174 loss: 3.49394822\n",
      "iteration: 175 loss: 2.49702382\n",
      "iteration: 176 loss: 2.39610147\n",
      "iteration: 177 loss: 1.01767826\n",
      "iteration: 178 loss: 1.27377832\n",
      "iteration: 179 loss: 0.40583605\n",
      "iteration: 180 loss: 0.63283622\n",
      "iteration: 181 loss: 0.93899304\n",
      "iteration: 182 loss: 0.39466029\n",
      "iteration: 183 loss: 2.56131268\n",
      "iteration: 184 loss: 0.97167766\n",
      "iteration: 185 loss: 1.74151039\n",
      "iteration: 186 loss: 1.87051964\n",
      "iteration: 187 loss: 0.94340885\n",
      "iteration: 188 loss: 3.79411411\n",
      "iteration: 189 loss: 2.10398936\n",
      "iteration: 190 loss: 2.60995650\n",
      "iteration: 191 loss: 1.29737937\n",
      "iteration: 192 loss: 1.05198395\n",
      "iteration: 193 loss: 1.39608049\n",
      "iteration: 194 loss: 0.93484432\n",
      "iteration: 195 loss: 3.13320661\n",
      "iteration: 196 loss: 3.21147084\n",
      "iteration: 197 loss: 0.79513335\n",
      "iteration: 198 loss: 0.92903578\n",
      "iteration: 199 loss: 1.59843528\n",
      "epoch:  31 mean loss training: 1.52786052\n",
      "epoch:  31 mean loss validation: 1.58608973\n",
      "iteration:   0 loss: 3.13168836\n",
      "iteration:   1 loss: 2.30457354\n",
      "iteration:   2 loss: 1.56167328\n",
      "iteration:   3 loss: 0.89893705\n",
      "iteration:   4 loss: 3.07243180\n",
      "iteration:   5 loss: 2.81738663\n",
      "iteration:   6 loss: 2.20095968\n",
      "iteration:   7 loss: 1.50049055\n",
      "iteration:   8 loss: 2.87236404\n",
      "iteration:   9 loss: 3.55224848\n",
      "iteration:  10 loss: 1.41763186\n",
      "iteration:  11 loss: 0.68229192\n",
      "iteration:  12 loss: 1.78307247\n",
      "iteration:  13 loss: 1.57077646\n",
      "iteration:  14 loss: 1.19602358\n",
      "iteration:  15 loss: 0.93766111\n",
      "iteration:  16 loss: 1.62546265\n",
      "iteration:  17 loss: 1.19520664\n",
      "iteration:  18 loss: 1.91124928\n",
      "iteration:  19 loss: 0.75918728\n",
      "iteration:  20 loss: 1.14622843\n",
      "iteration:  21 loss: 2.22360778\n",
      "iteration:  22 loss: 0.38902521\n",
      "iteration:  23 loss: 1.13476801\n",
      "iteration:  24 loss: 0.35906309\n",
      "iteration:  25 loss: 2.46369696\n",
      "iteration:  26 loss: 1.45308590\n",
      "iteration:  27 loss: 2.48124313\n",
      "iteration:  28 loss: 0.38247994\n",
      "iteration:  29 loss: 1.32843494\n",
      "iteration:  30 loss: 1.35353112\n",
      "iteration:  31 loss: 0.47270781\n",
      "iteration:  32 loss: 0.37487787\n",
      "iteration:  33 loss: 0.81145436\n",
      "iteration:  34 loss: 2.12403774\n",
      "iteration:  35 loss: 1.03086388\n",
      "iteration:  36 loss: 0.23976816\n",
      "iteration:  37 loss: 0.39726236\n",
      "iteration:  38 loss: 1.14876115\n",
      "iteration:  39 loss: 3.57368994\n",
      "iteration:  40 loss: 2.57993722\n",
      "iteration:  41 loss: 0.86519527\n",
      "iteration:  42 loss: 0.99308205\n",
      "iteration:  43 loss: 3.04438376\n",
      "iteration:  44 loss: 0.49245262\n",
      "iteration:  45 loss: 0.84726101\n",
      "iteration:  46 loss: 1.55904746\n",
      "iteration:  47 loss: 1.97788370\n",
      "iteration:  48 loss: 0.85082203\n",
      "iteration:  49 loss: 1.32346666\n",
      "iteration:  50 loss: 1.60906839\n",
      "iteration:  51 loss: 0.50708729\n",
      "iteration:  52 loss: 1.58952880\n",
      "iteration:  53 loss: 0.55703843\n",
      "iteration:  54 loss: 0.95146507\n",
      "iteration:  55 loss: 2.96530914\n",
      "iteration:  56 loss: 0.95850641\n",
      "iteration:  57 loss: 1.47682047\n",
      "iteration:  58 loss: 3.28773570\n",
      "iteration:  59 loss: 1.52892125\n",
      "iteration:  60 loss: 0.59575164\n",
      "iteration:  61 loss: 0.77792329\n",
      "iteration:  62 loss: 0.90607309\n",
      "iteration:  63 loss: 1.02463841\n",
      "iteration:  64 loss: 0.55227160\n",
      "iteration:  65 loss: 1.98443043\n",
      "iteration:  66 loss: 0.90080750\n",
      "iteration:  67 loss: 0.57845736\n",
      "iteration:  68 loss: 0.41493285\n",
      "iteration:  69 loss: 0.94228911\n",
      "iteration:  70 loss: 0.44858399\n",
      "iteration:  71 loss: 0.66810542\n",
      "iteration:  72 loss: 1.34800911\n",
      "iteration:  73 loss: 2.72510123\n",
      "iteration:  74 loss: 1.01247895\n",
      "iteration:  75 loss: 0.55461729\n",
      "iteration:  76 loss: 0.69944477\n",
      "iteration:  77 loss: 1.05101609\n",
      "iteration:  78 loss: 1.59601903\n",
      "iteration:  79 loss: 0.71044344\n",
      "iteration:  80 loss: 2.02253866\n",
      "iteration:  81 loss: 0.32024071\n",
      "iteration:  82 loss: 0.93014449\n",
      "iteration:  83 loss: 1.18591630\n",
      "iteration:  84 loss: 3.08610487\n",
      "iteration:  85 loss: 0.33975899\n",
      "iteration:  86 loss: 0.70082557\n",
      "iteration:  87 loss: 3.09315252\n",
      "iteration:  88 loss: 0.34917611\n",
      "iteration:  89 loss: 2.83486605\n",
      "iteration:  90 loss: 2.44497705\n",
      "iteration:  91 loss: 0.58432937\n",
      "iteration:  92 loss: 0.89755553\n",
      "iteration:  93 loss: 1.99804473\n",
      "iteration:  94 loss: 1.07070076\n",
      "iteration:  95 loss: 0.89586204\n",
      "iteration:  96 loss: 4.39630032\n",
      "iteration:  97 loss: 0.99110198\n",
      "iteration:  98 loss: 0.59176183\n",
      "iteration:  99 loss: 0.46775481\n",
      "iteration: 100 loss: 2.23153830\n",
      "iteration: 101 loss: 0.59250313\n",
      "iteration: 102 loss: 0.99104035\n",
      "iteration: 103 loss: 2.82112408\n",
      "iteration: 104 loss: 0.44821370\n",
      "iteration: 105 loss: 0.81091958\n",
      "iteration: 106 loss: 0.38630688\n",
      "iteration: 107 loss: 1.23347926\n",
      "iteration: 108 loss: 1.44214547\n",
      "iteration: 109 loss: 0.83648849\n",
      "iteration: 110 loss: 3.01284528\n",
      "iteration: 111 loss: 3.00034237\n",
      "iteration: 112 loss: 3.15264606\n",
      "iteration: 113 loss: 1.95046127\n",
      "iteration: 114 loss: 2.47712183\n",
      "iteration: 115 loss: 3.91368389\n",
      "iteration: 116 loss: 1.32436669\n",
      "iteration: 117 loss: 0.24190407\n",
      "iteration: 118 loss: 0.57653534\n",
      "iteration: 119 loss: 0.91905266\n",
      "iteration: 120 loss: 0.32816902\n",
      "iteration: 121 loss: 0.66100812\n",
      "iteration: 122 loss: 0.37370408\n",
      "iteration: 123 loss: 2.38228726\n",
      "iteration: 124 loss: 2.02274799\n",
      "iteration: 125 loss: 2.91929579\n",
      "iteration: 126 loss: 3.17797518\n",
      "iteration: 127 loss: 1.21180499\n",
      "iteration: 128 loss: 2.23928809\n",
      "iteration: 129 loss: 0.46355712\n",
      "iteration: 130 loss: 2.54974437\n",
      "iteration: 131 loss: 2.21478987\n",
      "iteration: 132 loss: 2.54533458\n",
      "iteration: 133 loss: 0.67442393\n",
      "iteration: 134 loss: 0.83152682\n",
      "iteration: 135 loss: 1.23419678\n",
      "iteration: 136 loss: 1.00551915\n",
      "iteration: 137 loss: 1.55038130\n",
      "iteration: 138 loss: 0.86346591\n",
      "iteration: 139 loss: 0.92291242\n",
      "iteration: 140 loss: 2.38105512\n",
      "iteration: 141 loss: 0.90353698\n",
      "iteration: 142 loss: 0.99074012\n",
      "iteration: 143 loss: 0.78190029\n",
      "iteration: 144 loss: 3.44451451\n",
      "iteration: 145 loss: 2.93511748\n",
      "iteration: 146 loss: 1.24705434\n",
      "iteration: 147 loss: 0.91093093\n",
      "iteration: 148 loss: 0.86808199\n",
      "iteration: 149 loss: 1.73819327\n",
      "iteration: 150 loss: 1.83773232\n",
      "iteration: 151 loss: 1.43256092\n",
      "iteration: 152 loss: 3.17535758\n",
      "iteration: 153 loss: 3.53099942\n",
      "iteration: 154 loss: 1.42756224\n",
      "iteration: 155 loss: 2.85193634\n",
      "iteration: 156 loss: 0.35519850\n",
      "iteration: 157 loss: 0.84554207\n",
      "iteration: 158 loss: 0.84906471\n",
      "iteration: 159 loss: 0.41022575\n",
      "iteration: 160 loss: 0.53115624\n",
      "iteration: 161 loss: 1.67928922\n",
      "iteration: 162 loss: 0.37362471\n",
      "iteration: 163 loss: 2.89958906\n",
      "iteration: 164 loss: 3.38444996\n",
      "iteration: 165 loss: 1.51852560\n",
      "iteration: 166 loss: 0.67162496\n",
      "iteration: 167 loss: 0.27404413\n",
      "iteration: 168 loss: 0.64032459\n",
      "iteration: 169 loss: 1.62807393\n",
      "iteration: 170 loss: 2.59344912\n",
      "iteration: 171 loss: 0.93773174\n",
      "iteration: 172 loss: 0.49214265\n",
      "iteration: 173 loss: 0.43996987\n",
      "iteration: 174 loss: 3.53005838\n",
      "iteration: 175 loss: 2.02930689\n",
      "iteration: 176 loss: 1.77309632\n",
      "iteration: 177 loss: 0.97434944\n",
      "iteration: 178 loss: 0.60656077\n",
      "iteration: 179 loss: 0.36061025\n",
      "iteration: 180 loss: 0.59957802\n",
      "iteration: 181 loss: 0.95641178\n",
      "iteration: 182 loss: 0.41203707\n",
      "iteration: 183 loss: 2.40840077\n",
      "iteration: 184 loss: 0.89450133\n",
      "iteration: 185 loss: 1.94910967\n",
      "iteration: 186 loss: 1.69589138\n",
      "iteration: 187 loss: 0.80625516\n",
      "iteration: 188 loss: 3.66391063\n",
      "iteration: 189 loss: 2.14495945\n",
      "iteration: 190 loss: 2.86156893\n",
      "iteration: 191 loss: 1.31036294\n",
      "iteration: 192 loss: 0.95348257\n",
      "iteration: 193 loss: 1.44094861\n",
      "iteration: 194 loss: 0.90214038\n",
      "iteration: 195 loss: 3.11748862\n",
      "iteration: 196 loss: 3.14877534\n",
      "iteration: 197 loss: 0.78407937\n",
      "iteration: 198 loss: 0.81510961\n",
      "iteration: 199 loss: 1.31401682\n",
      "epoch:  32 mean loss training: 1.47753310\n",
      "epoch:  32 mean loss validation: 1.56513417\n",
      "iteration:   0 loss: 2.95709157\n",
      "iteration:   1 loss: 2.50789165\n",
      "iteration:   2 loss: 1.50514150\n",
      "iteration:   3 loss: 0.84391707\n",
      "iteration:   4 loss: 3.11848140\n",
      "iteration:   5 loss: 2.69009185\n",
      "iteration:   6 loss: 2.11008573\n",
      "iteration:   7 loss: 1.59828210\n",
      "iteration:   8 loss: 3.32637525\n",
      "iteration:   9 loss: 3.50213981\n",
      "iteration:  10 loss: 1.01991117\n",
      "iteration:  11 loss: 0.52801520\n",
      "iteration:  12 loss: 1.54282367\n",
      "iteration:  13 loss: 1.31756234\n",
      "iteration:  14 loss: 1.03500938\n",
      "iteration:  15 loss: 0.75329643\n",
      "iteration:  16 loss: 1.25545788\n",
      "iteration:  17 loss: 1.24593079\n",
      "iteration:  18 loss: 1.89989519\n",
      "iteration:  19 loss: 0.69647509\n",
      "iteration:  20 loss: 1.20049870\n",
      "iteration:  21 loss: 2.51031184\n",
      "iteration:  22 loss: 0.42440215\n",
      "iteration:  23 loss: 1.15791118\n",
      "iteration:  24 loss: 0.34687972\n",
      "iteration:  25 loss: 2.15296602\n",
      "iteration:  26 loss: 1.55247974\n",
      "iteration:  27 loss: 2.54981661\n",
      "iteration:  28 loss: 0.45963562\n",
      "iteration:  29 loss: 1.11526930\n",
      "iteration:  30 loss: 1.30335379\n",
      "iteration:  31 loss: 0.71898705\n",
      "iteration:  32 loss: 0.50709516\n",
      "iteration:  33 loss: 0.78522396\n",
      "iteration:  34 loss: 1.92915678\n",
      "iteration:  35 loss: 1.18904305\n",
      "iteration:  36 loss: 0.20982431\n",
      "iteration:  37 loss: 0.38098308\n",
      "iteration:  38 loss: 1.14324737\n",
      "iteration:  39 loss: 3.30623746\n",
      "iteration:  40 loss: 2.25242496\n",
      "iteration:  41 loss: 0.87644672\n",
      "iteration:  42 loss: 1.08850563\n",
      "iteration:  43 loss: 3.36785197\n",
      "iteration:  44 loss: 0.88376236\n",
      "iteration:  45 loss: 0.47529757\n",
      "iteration:  46 loss: 1.66153061\n",
      "iteration:  47 loss: 1.61131155\n",
      "iteration:  48 loss: 0.83980489\n",
      "iteration:  49 loss: 0.91021377\n",
      "iteration:  50 loss: 1.79763353\n",
      "iteration:  51 loss: 0.48701262\n",
      "iteration:  52 loss: 1.43794513\n",
      "iteration:  53 loss: 0.43600395\n",
      "iteration:  54 loss: 0.90383422\n",
      "iteration:  55 loss: 2.96084690\n",
      "iteration:  56 loss: 1.05656302\n",
      "iteration:  57 loss: 1.29214096\n",
      "iteration:  58 loss: 3.65934682\n",
      "iteration:  59 loss: 1.66416800\n",
      "iteration:  60 loss: 0.57002217\n",
      "iteration:  61 loss: 0.70340425\n",
      "iteration:  62 loss: 0.89488924\n",
      "iteration:  63 loss: 1.27887070\n",
      "iteration:  64 loss: 0.58827424\n",
      "iteration:  65 loss: 1.97772586\n",
      "iteration:  66 loss: 0.91414797\n",
      "iteration:  67 loss: 0.61024332\n",
      "iteration:  68 loss: 0.53674471\n",
      "iteration:  69 loss: 0.95851231\n",
      "iteration:  70 loss: 0.51009315\n",
      "iteration:  71 loss: 0.78160453\n",
      "iteration:  72 loss: 1.49191141\n",
      "iteration:  73 loss: 2.06476450\n",
      "iteration:  74 loss: 0.89079338\n",
      "iteration:  75 loss: 0.60563958\n",
      "iteration:  76 loss: 0.71900529\n",
      "iteration:  77 loss: 1.04047489\n",
      "iteration:  78 loss: 1.54016209\n",
      "iteration:  79 loss: 0.70984262\n",
      "iteration:  80 loss: 1.74772608\n",
      "iteration:  81 loss: 0.28231212\n",
      "iteration:  82 loss: 0.79370266\n",
      "iteration:  83 loss: 1.19845259\n",
      "iteration:  84 loss: 2.89331722\n",
      "iteration:  85 loss: 0.33792308\n",
      "iteration:  86 loss: 0.58515054\n",
      "iteration:  87 loss: 3.12497902\n",
      "iteration:  88 loss: 0.32364032\n",
      "iteration:  89 loss: 2.88199806\n",
      "iteration:  90 loss: 2.64091706\n",
      "iteration:  91 loss: 0.58325011\n",
      "iteration:  92 loss: 0.99372727\n",
      "iteration:  93 loss: 2.11637712\n",
      "iteration:  94 loss: 1.48935831\n",
      "iteration:  95 loss: 1.29482079\n",
      "iteration:  96 loss: 3.42843342\n",
      "iteration:  97 loss: 1.19512606\n",
      "iteration:  98 loss: 0.67426407\n",
      "iteration:  99 loss: 0.68169332\n",
      "iteration: 100 loss: 1.55460501\n",
      "iteration: 101 loss: 0.54967457\n",
      "iteration: 102 loss: 1.66419971\n",
      "iteration: 103 loss: 2.71476722\n",
      "iteration: 104 loss: 0.51273227\n",
      "iteration: 105 loss: 0.95741564\n",
      "iteration: 106 loss: 0.45267990\n",
      "iteration: 107 loss: 1.46661079\n",
      "iteration: 108 loss: 1.48842442\n",
      "iteration: 109 loss: 0.92122513\n",
      "iteration: 110 loss: 2.77452421\n",
      "iteration: 111 loss: 3.02095938\n",
      "iteration: 112 loss: 2.89227891\n",
      "iteration: 113 loss: 1.58475053\n",
      "iteration: 114 loss: 2.51047087\n",
      "iteration: 115 loss: 3.25872898\n",
      "iteration: 116 loss: 1.34350741\n",
      "iteration: 117 loss: 0.29186127\n",
      "iteration: 118 loss: 0.62684250\n",
      "iteration: 119 loss: 1.31652308\n",
      "iteration: 120 loss: 0.31706747\n",
      "iteration: 121 loss: 0.74131083\n",
      "iteration: 122 loss: 0.77430069\n",
      "iteration: 123 loss: 2.43102694\n",
      "iteration: 124 loss: 2.55166721\n",
      "iteration: 125 loss: 2.68535399\n",
      "iteration: 126 loss: 3.15152860\n",
      "iteration: 127 loss: 1.15068197\n",
      "iteration: 128 loss: 2.17485595\n",
      "iteration: 129 loss: 0.50051206\n",
      "iteration: 130 loss: 2.68216062\n",
      "iteration: 131 loss: 2.33370948\n",
      "iteration: 132 loss: 2.74198341\n",
      "iteration: 133 loss: 0.87124443\n",
      "iteration: 134 loss: 0.88338339\n",
      "iteration: 135 loss: 1.22112668\n",
      "iteration: 136 loss: 1.00190711\n",
      "iteration: 137 loss: 1.39244509\n",
      "iteration: 138 loss: 0.91038722\n",
      "iteration: 139 loss: 1.01702130\n",
      "iteration: 140 loss: 2.40817809\n",
      "iteration: 141 loss: 1.13536990\n",
      "iteration: 142 loss: 0.82941616\n",
      "iteration: 143 loss: 1.20271063\n",
      "iteration: 144 loss: 3.39134216\n",
      "iteration: 145 loss: 2.11941957\n",
      "iteration: 146 loss: 0.57346362\n",
      "iteration: 147 loss: 0.96289837\n",
      "iteration: 148 loss: 1.04304898\n",
      "iteration: 149 loss: 1.88939786\n",
      "iteration: 150 loss: 1.82620895\n",
      "iteration: 151 loss: 1.40643644\n",
      "iteration: 152 loss: 3.27153039\n",
      "iteration: 153 loss: 3.46489716\n",
      "iteration: 154 loss: 1.43243408\n",
      "iteration: 155 loss: 2.80103254\n",
      "iteration: 156 loss: 0.42167881\n",
      "iteration: 157 loss: 0.90964341\n",
      "iteration: 158 loss: 0.70706040\n",
      "iteration: 159 loss: 0.38272268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 160 loss: 0.78901744\n",
      "iteration: 161 loss: 1.66414523\n",
      "iteration: 162 loss: 0.40759569\n",
      "iteration: 163 loss: 2.77467299\n",
      "iteration: 164 loss: 3.38175225\n",
      "iteration: 165 loss: 1.98212457\n",
      "iteration: 166 loss: 0.64401484\n",
      "iteration: 167 loss: 0.27928928\n",
      "iteration: 168 loss: 0.70188862\n",
      "iteration: 169 loss: 1.64778948\n",
      "iteration: 170 loss: 3.22051001\n",
      "iteration: 171 loss: 1.23033404\n",
      "iteration: 172 loss: 0.53122050\n",
      "iteration: 173 loss: 0.40253097\n",
      "iteration: 174 loss: 3.50793147\n",
      "iteration: 175 loss: 2.60154510\n",
      "iteration: 176 loss: 1.99290717\n",
      "iteration: 177 loss: 0.96129483\n",
      "iteration: 178 loss: 0.62406266\n",
      "iteration: 179 loss: 0.31690085\n",
      "iteration: 180 loss: 0.47575441\n",
      "iteration: 181 loss: 0.96911389\n",
      "iteration: 182 loss: 0.37507802\n",
      "iteration: 183 loss: 2.13811851\n",
      "iteration: 184 loss: 0.82829469\n",
      "iteration: 185 loss: 1.73579729\n",
      "iteration: 186 loss: 1.88888681\n",
      "iteration: 187 loss: 0.85686749\n",
      "iteration: 188 loss: 3.26418304\n",
      "iteration: 189 loss: 2.06533074\n",
      "iteration: 190 loss: 2.34927154\n",
      "iteration: 191 loss: 1.21248686\n",
      "iteration: 192 loss: 1.21116459\n",
      "iteration: 193 loss: 1.60749710\n",
      "iteration: 194 loss: 0.77062249\n",
      "iteration: 195 loss: 3.24237466\n",
      "iteration: 196 loss: 3.13022876\n",
      "iteration: 197 loss: 0.79523563\n",
      "iteration: 198 loss: 0.82415682\n",
      "iteration: 199 loss: 0.98851389\n",
      "epoch:  33 mean loss training: 1.47359991\n",
      "epoch:  33 mean loss validation: 1.49886012\n",
      "iteration:   0 loss: 2.79658079\n",
      "iteration:   1 loss: 2.46702719\n",
      "iteration:   2 loss: 1.46548414\n",
      "iteration:   3 loss: 0.72365385\n",
      "iteration:   4 loss: 3.49596834\n",
      "iteration:   5 loss: 2.74488664\n",
      "iteration:   6 loss: 1.96415007\n",
      "iteration:   7 loss: 1.41005933\n",
      "iteration:   8 loss: 3.50715399\n",
      "iteration:   9 loss: 3.53165340\n",
      "iteration:  10 loss: 0.84906912\n",
      "iteration:  11 loss: 0.61075830\n",
      "iteration:  12 loss: 1.84766209\n",
      "iteration:  13 loss: 1.14634109\n",
      "iteration:  14 loss: 0.96998048\n",
      "iteration:  15 loss: 0.71984506\n",
      "iteration:  16 loss: 1.14334750\n",
      "iteration:  17 loss: 1.09717965\n",
      "iteration:  18 loss: 1.70309412\n",
      "iteration:  19 loss: 1.12516499\n",
      "iteration:  20 loss: 0.96375221\n",
      "iteration:  21 loss: 2.22457147\n",
      "iteration:  22 loss: 0.43486249\n",
      "iteration:  23 loss: 1.11947477\n",
      "iteration:  24 loss: 0.67280865\n",
      "iteration:  25 loss: 2.50907207\n",
      "iteration:  26 loss: 1.51713371\n",
      "iteration:  27 loss: 2.55960107\n",
      "iteration:  28 loss: 0.52093428\n",
      "iteration:  29 loss: 1.49360096\n",
      "iteration:  30 loss: 0.68194884\n",
      "iteration:  31 loss: 0.44892117\n",
      "iteration:  32 loss: 0.41845673\n",
      "iteration:  33 loss: 0.80103910\n",
      "iteration:  34 loss: 1.67655277\n",
      "iteration:  35 loss: 1.12027323\n",
      "iteration:  36 loss: 0.24077624\n",
      "iteration:  37 loss: 0.79432535\n",
      "iteration:  38 loss: 1.48488069\n",
      "iteration:  39 loss: 2.93136573\n",
      "iteration:  40 loss: 2.23882675\n",
      "iteration:  41 loss: 1.42229164\n",
      "iteration:  42 loss: 0.95358151\n",
      "iteration:  43 loss: 2.08589101\n",
      "iteration:  44 loss: 1.02115941\n",
      "iteration:  45 loss: 0.63608950\n",
      "iteration:  46 loss: 1.50138497\n",
      "iteration:  47 loss: 1.78614402\n",
      "iteration:  48 loss: 1.05027211\n",
      "iteration:  49 loss: 1.13748157\n",
      "iteration:  50 loss: 2.64234614\n",
      "iteration:  51 loss: 0.62770057\n",
      "iteration:  52 loss: 1.35108018\n",
      "iteration:  53 loss: 0.47264788\n",
      "iteration:  54 loss: 0.83928698\n",
      "iteration:  55 loss: 3.17569613\n",
      "iteration:  56 loss: 1.05706358\n",
      "iteration:  57 loss: 1.29048371\n",
      "iteration:  58 loss: 3.79846430\n",
      "iteration:  59 loss: 1.77723694\n",
      "iteration:  60 loss: 0.56261224\n",
      "iteration:  61 loss: 0.65422165\n",
      "iteration:  62 loss: 0.98015064\n",
      "iteration:  63 loss: 1.43671858\n",
      "iteration:  64 loss: 0.55978197\n",
      "iteration:  65 loss: 2.04769778\n",
      "iteration:  66 loss: 1.05629742\n",
      "iteration:  67 loss: 0.73027867\n",
      "iteration:  68 loss: 0.68603832\n",
      "iteration:  69 loss: 1.32913303\n",
      "iteration:  70 loss: 0.64606398\n",
      "iteration:  71 loss: 0.57461160\n",
      "iteration:  72 loss: 1.37455118\n",
      "iteration:  73 loss: 2.19971061\n",
      "iteration:  74 loss: 0.90015429\n",
      "iteration:  75 loss: 0.73257715\n",
      "iteration:  76 loss: 0.65249026\n",
      "iteration:  77 loss: 1.04921150\n",
      "iteration:  78 loss: 1.59817719\n",
      "iteration:  79 loss: 0.65294904\n",
      "iteration:  80 loss: 1.87662363\n",
      "iteration:  81 loss: 0.33830106\n",
      "iteration:  82 loss: 0.66922700\n",
      "iteration:  83 loss: 1.05893075\n",
      "iteration:  84 loss: 3.23118448\n",
      "iteration:  85 loss: 0.32867888\n",
      "iteration:  86 loss: 0.71994865\n",
      "iteration:  87 loss: 3.26855755\n",
      "iteration:  88 loss: 0.34152785\n",
      "iteration:  89 loss: 2.51137114\n",
      "iteration:  90 loss: 2.54904127\n",
      "iteration:  91 loss: 0.51507664\n",
      "iteration:  92 loss: 0.86988604\n",
      "iteration:  93 loss: 1.64997733\n",
      "iteration:  94 loss: 1.54833019\n",
      "iteration:  95 loss: 0.73636365\n",
      "iteration:  96 loss: 3.92243147\n",
      "iteration:  97 loss: 1.08882141\n",
      "iteration:  98 loss: 0.57220310\n",
      "iteration:  99 loss: 0.59890205\n",
      "iteration: 100 loss: 1.72131670\n",
      "iteration: 101 loss: 0.55129319\n",
      "iteration: 102 loss: 0.90848029\n",
      "iteration: 103 loss: 2.52629590\n",
      "iteration: 104 loss: 0.55081385\n",
      "iteration: 105 loss: 0.86999476\n",
      "iteration: 106 loss: 0.56330007\n",
      "iteration: 107 loss: 1.53106833\n",
      "iteration: 108 loss: 1.42374992\n",
      "iteration: 109 loss: 0.75338101\n",
      "iteration: 110 loss: 2.85602522\n",
      "iteration: 111 loss: 2.78366947\n",
      "iteration: 112 loss: 3.41667008\n",
      "iteration: 113 loss: 2.64178133\n",
      "iteration: 114 loss: 2.19478226\n",
      "iteration: 115 loss: 3.42059231\n",
      "iteration: 116 loss: 1.50316095\n",
      "iteration: 117 loss: 0.42872488\n",
      "iteration: 118 loss: 0.70250225\n",
      "iteration: 119 loss: 1.37836909\n",
      "iteration: 120 loss: 0.34865233\n",
      "iteration: 121 loss: 0.77975565\n",
      "iteration: 122 loss: 0.60486478\n",
      "iteration: 123 loss: 2.48672795\n",
      "iteration: 124 loss: 2.33570504\n",
      "iteration: 125 loss: 2.69326091\n",
      "iteration: 126 loss: 3.02839661\n",
      "iteration: 127 loss: 1.08449662\n",
      "iteration: 128 loss: 2.05707073\n",
      "iteration: 129 loss: 0.56344950\n",
      "iteration: 130 loss: 2.48817277\n",
      "iteration: 131 loss: 2.37761521\n",
      "iteration: 132 loss: 2.70241427\n",
      "iteration: 133 loss: 1.42620289\n",
      "iteration: 134 loss: 1.45788133\n",
      "iteration: 135 loss: 1.35060203\n",
      "iteration: 136 loss: 0.96133274\n",
      "iteration: 137 loss: 1.63595390\n",
      "iteration: 138 loss: 0.95244867\n",
      "iteration: 139 loss: 0.84948242\n",
      "iteration: 140 loss: 2.57942080\n",
      "iteration: 141 loss: 0.92469412\n",
      "iteration: 142 loss: 0.90131485\n",
      "iteration: 143 loss: 1.75084054\n",
      "iteration: 144 loss: 3.40288997\n",
      "iteration: 145 loss: 2.06674767\n",
      "iteration: 146 loss: 1.55430830\n",
      "iteration: 147 loss: 1.05037844\n",
      "iteration: 148 loss: 1.10686564\n",
      "iteration: 149 loss: 2.11484337\n",
      "iteration: 150 loss: 1.68362188\n",
      "iteration: 151 loss: 1.42245007\n",
      "iteration: 152 loss: 3.01391649\n",
      "iteration: 153 loss: 3.41749287\n",
      "iteration: 154 loss: 1.33614504\n",
      "iteration: 155 loss: 2.80933404\n",
      "iteration: 156 loss: 0.55775207\n",
      "iteration: 157 loss: 0.83108425\n",
      "iteration: 158 loss: 0.90949827\n",
      "iteration: 159 loss: 0.63885641\n",
      "iteration: 160 loss: 0.62058169\n",
      "iteration: 161 loss: 1.57268548\n",
      "iteration: 162 loss: 0.69754612\n",
      "iteration: 163 loss: 2.95225716\n",
      "iteration: 164 loss: 2.88565016\n",
      "iteration: 165 loss: 2.62878704\n",
      "iteration: 166 loss: 1.67207205\n",
      "iteration: 167 loss: 1.13912165\n",
      "iteration: 168 loss: 0.87587833\n",
      "iteration: 169 loss: 1.71121335\n",
      "iteration: 170 loss: 3.18104577\n",
      "iteration: 171 loss: 1.49817705\n",
      "iteration: 172 loss: 0.60026997\n",
      "iteration: 173 loss: 0.32398710\n",
      "iteration: 174 loss: 3.38455963\n",
      "iteration: 175 loss: 2.76636410\n",
      "iteration: 176 loss: 2.43526721\n",
      "iteration: 177 loss: 0.98503590\n",
      "iteration: 178 loss: 1.53970218\n",
      "iteration: 179 loss: 0.56490701\n",
      "iteration: 180 loss: 1.88898945\n",
      "iteration: 181 loss: 1.16105247\n",
      "iteration: 182 loss: 0.44495115\n",
      "iteration: 183 loss: 2.73015118\n",
      "iteration: 184 loss: 1.13588989\n",
      "iteration: 185 loss: 1.98366678\n",
      "iteration: 186 loss: 2.22376823\n",
      "iteration: 187 loss: 0.68447334\n",
      "iteration: 188 loss: 2.78338456\n",
      "iteration: 189 loss: 2.12600636\n",
      "iteration: 190 loss: 2.71777344\n",
      "iteration: 191 loss: 1.48886013\n",
      "iteration: 192 loss: 0.76816350\n",
      "iteration: 193 loss: 1.69118321\n",
      "iteration: 194 loss: 0.95120710\n",
      "iteration: 195 loss: 3.13666844\n",
      "iteration: 196 loss: 3.13723397\n",
      "iteration: 197 loss: 1.00286114\n",
      "iteration: 198 loss: 0.78118426\n",
      "iteration: 199 loss: 0.93313628\n",
      "epoch:  34 mean loss training: 1.53202987\n",
      "epoch:  34 mean loss validation: 1.46829748\n",
      "iteration:   0 loss: 2.39995980\n",
      "iteration:   1 loss: 1.21152925\n",
      "iteration:   2 loss: 1.28389132\n",
      "iteration:   3 loss: 0.48631939\n",
      "iteration:   4 loss: 3.60043120\n",
      "iteration:   5 loss: 3.55135942\n",
      "iteration:   6 loss: 2.32119417\n",
      "iteration:   7 loss: 1.62170732\n",
      "iteration:   8 loss: 2.86440301\n",
      "iteration:   9 loss: 3.46729040\n",
      "iteration:  10 loss: 0.50137454\n",
      "iteration:  11 loss: 1.30424440\n",
      "iteration:  12 loss: 2.14077425\n",
      "iteration:  13 loss: 0.66167873\n",
      "iteration:  14 loss: 1.06930113\n",
      "iteration:  15 loss: 0.67912602\n",
      "iteration:  16 loss: 0.50116843\n",
      "iteration:  17 loss: 0.55925149\n",
      "iteration:  18 loss: 1.57717466\n",
      "iteration:  19 loss: 0.60107684\n",
      "iteration:  20 loss: 0.47778597\n",
      "iteration:  21 loss: 2.29928613\n",
      "iteration:  22 loss: 0.30772981\n",
      "iteration:  23 loss: 1.17907059\n",
      "iteration:  24 loss: 0.55390424\n",
      "iteration:  25 loss: 2.98220110\n",
      "iteration:  26 loss: 1.30716956\n",
      "iteration:  27 loss: 2.69613957\n",
      "iteration:  28 loss: 0.28036797\n",
      "iteration:  29 loss: 1.49400342\n",
      "iteration:  30 loss: 1.49540114\n",
      "iteration:  31 loss: 0.46468455\n",
      "iteration:  32 loss: 0.34463358\n",
      "iteration:  33 loss: 0.67807919\n",
      "iteration:  34 loss: 2.61134553\n",
      "iteration:  35 loss: 0.99146515\n",
      "iteration:  36 loss: 0.30341405\n",
      "iteration:  37 loss: 0.39098161\n",
      "iteration:  38 loss: 1.18469226\n",
      "iteration:  39 loss: 4.10055304\n",
      "iteration:  40 loss: 2.24455905\n",
      "iteration:  41 loss: 0.66817594\n",
      "iteration:  42 loss: 0.91174126\n",
      "iteration:  43 loss: 3.77276897\n",
      "iteration:  44 loss: 0.50663716\n",
      "iteration:  45 loss: 0.59620613\n",
      "iteration:  46 loss: 1.76505494\n",
      "iteration:  47 loss: 1.51410782\n",
      "iteration:  48 loss: 0.77082646\n",
      "iteration:  49 loss: 0.65323228\n",
      "iteration:  50 loss: 2.62515235\n",
      "iteration:  51 loss: 0.46107996\n",
      "iteration:  52 loss: 1.33728647\n",
      "iteration:  53 loss: 0.34012711\n",
      "iteration:  54 loss: 0.99802214\n",
      "iteration:  55 loss: 3.59443569\n",
      "iteration:  56 loss: 1.03640831\n",
      "iteration:  57 loss: 2.65687728\n",
      "iteration:  58 loss: 3.62856913\n",
      "iteration:  59 loss: 1.54600906\n",
      "iteration:  60 loss: 0.34660947\n",
      "iteration:  61 loss: 0.42557418\n",
      "iteration:  62 loss: 1.10750318\n",
      "iteration:  63 loss: 0.88340944\n",
      "iteration:  64 loss: 0.54773891\n",
      "iteration:  65 loss: 1.68995607\n",
      "iteration:  66 loss: 0.72244298\n",
      "iteration:  67 loss: 0.23598906\n",
      "iteration:  68 loss: 0.44368085\n",
      "iteration:  69 loss: 0.60998446\n",
      "iteration:  70 loss: 0.52507484\n",
      "iteration:  71 loss: 0.58551270\n",
      "iteration:  72 loss: 1.32243359\n",
      "iteration:  73 loss: 2.38406706\n",
      "iteration:  74 loss: 0.91577905\n",
      "iteration:  75 loss: 0.79973507\n",
      "iteration:  76 loss: 0.58222342\n",
      "iteration:  77 loss: 0.92337412\n",
      "iteration:  78 loss: 1.69113743\n",
      "iteration:  79 loss: 0.67595232\n",
      "iteration:  80 loss: 1.81566715\n",
      "iteration:  81 loss: 0.33352962\n",
      "iteration:  82 loss: 0.84333998\n",
      "iteration:  83 loss: 0.91998255\n",
      "iteration:  84 loss: 2.98992395\n",
      "iteration:  85 loss: 0.27082494\n",
      "iteration:  86 loss: 0.72369415\n",
      "iteration:  87 loss: 3.15138793\n",
      "iteration:  88 loss: 0.35939717\n",
      "iteration:  89 loss: 2.61732173\n",
      "iteration:  90 loss: 2.61253572\n",
      "iteration:  91 loss: 0.57658416\n",
      "iteration:  92 loss: 1.06189561\n",
      "iteration:  93 loss: 1.88636708\n",
      "iteration:  94 loss: 1.14133358\n",
      "iteration:  95 loss: 0.55351716\n",
      "iteration:  96 loss: 3.92789674\n",
      "iteration:  97 loss: 0.93404645\n",
      "iteration:  98 loss: 0.55194509\n",
      "iteration:  99 loss: 0.47612688\n",
      "iteration: 100 loss: 1.99059093\n",
      "iteration: 101 loss: 0.53062487\n",
      "iteration: 102 loss: 0.94943410\n",
      "iteration: 103 loss: 2.98484850\n",
      "iteration: 104 loss: 0.45870885\n",
      "iteration: 105 loss: 0.87688333\n",
      "iteration: 106 loss: 0.43442181\n",
      "iteration: 107 loss: 0.96279818\n",
      "iteration: 108 loss: 1.43149757\n",
      "iteration: 109 loss: 0.69749790\n",
      "iteration: 110 loss: 2.84552813\n",
      "iteration: 111 loss: 3.08608246\n",
      "iteration: 112 loss: 3.18845725\n",
      "iteration: 113 loss: 0.99556082\n",
      "iteration: 114 loss: 1.93973207\n",
      "iteration: 115 loss: 3.80887747\n",
      "iteration: 116 loss: 1.94579899\n",
      "iteration: 117 loss: 0.34748662\n",
      "iteration: 118 loss: 0.62484342\n",
      "iteration: 119 loss: 0.96140170\n",
      "iteration: 120 loss: 0.32848418\n",
      "iteration: 121 loss: 0.63183409\n",
      "iteration: 122 loss: 0.51323521\n",
      "iteration: 123 loss: 2.55201101\n",
      "iteration: 124 loss: 1.86053205\n",
      "iteration: 125 loss: 2.31801653\n",
      "iteration: 126 loss: 3.31497836\n",
      "iteration: 127 loss: 0.98762131\n",
      "iteration: 128 loss: 1.90685344\n",
      "iteration: 129 loss: 0.46549878\n",
      "iteration: 130 loss: 2.54066348\n",
      "iteration: 131 loss: 2.13705850\n",
      "iteration: 132 loss: 1.79381073\n",
      "iteration: 133 loss: 0.59173238\n",
      "iteration: 134 loss: 0.80767304\n",
      "iteration: 135 loss: 1.31433034\n",
      "iteration: 136 loss: 1.09699667\n",
      "iteration: 137 loss: 1.47402346\n",
      "iteration: 138 loss: 1.02083814\n",
      "iteration: 139 loss: 1.06699324\n",
      "iteration: 140 loss: 2.02513862\n",
      "iteration: 141 loss: 1.31596744\n",
      "iteration: 142 loss: 0.79045171\n",
      "iteration: 143 loss: 1.29170823\n",
      "iteration: 144 loss: 3.45710325\n",
      "iteration: 145 loss: 2.14220738\n",
      "iteration: 146 loss: 0.51532817\n",
      "iteration: 147 loss: 1.16171610\n",
      "iteration: 148 loss: 1.29919291\n",
      "iteration: 149 loss: 1.31407332\n",
      "iteration: 150 loss: 2.23332500\n",
      "iteration: 151 loss: 1.56135285\n",
      "iteration: 152 loss: 3.96625471\n",
      "iteration: 153 loss: 3.00265694\n",
      "iteration: 154 loss: 2.39922190\n",
      "iteration: 155 loss: 3.09879756\n",
      "iteration: 156 loss: 0.64002019\n",
      "iteration: 157 loss: 1.33761179\n",
      "iteration: 158 loss: 1.15553141\n",
      "iteration: 159 loss: 0.52280301\n",
      "iteration: 160 loss: 0.74982673\n",
      "iteration: 161 loss: 1.01668596\n",
      "iteration: 162 loss: 0.38100055\n",
      "iteration: 163 loss: 2.50055122\n",
      "iteration: 164 loss: 3.05368686\n",
      "iteration: 165 loss: 1.80142248\n",
      "iteration: 166 loss: 0.55685025\n",
      "iteration: 167 loss: 0.28544357\n",
      "iteration: 168 loss: 1.00328696\n",
      "iteration: 169 loss: 0.59784269\n",
      "iteration: 170 loss: 3.41048145\n",
      "iteration: 171 loss: 0.85215473\n",
      "iteration: 172 loss: 0.60551053\n",
      "iteration: 173 loss: 0.36721587\n",
      "iteration: 174 loss: 3.73231316\n",
      "iteration: 175 loss: 2.77819681\n",
      "iteration: 176 loss: 2.29099941\n",
      "iteration: 177 loss: 0.83767492\n",
      "iteration: 178 loss: 1.16830075\n",
      "iteration: 179 loss: 0.30691242\n",
      "iteration: 180 loss: 1.15941739\n",
      "iteration: 181 loss: 0.97632521\n",
      "iteration: 182 loss: 0.50809401\n",
      "iteration: 183 loss: 2.32756186\n",
      "iteration: 184 loss: 0.85843486\n",
      "iteration: 185 loss: 2.16293645\n",
      "iteration: 186 loss: 1.70739305\n",
      "iteration: 187 loss: 0.84611070\n",
      "iteration: 188 loss: 3.09447265\n",
      "iteration: 189 loss: 2.11767960\n",
      "iteration: 190 loss: 1.82837927\n",
      "iteration: 191 loss: 1.12972629\n",
      "iteration: 192 loss: 1.29523349\n",
      "iteration: 193 loss: 1.22518981\n",
      "iteration: 194 loss: 0.70951772\n",
      "iteration: 195 loss: 3.08369875\n",
      "iteration: 196 loss: 3.17468548\n",
      "iteration: 197 loss: 0.91709775\n",
      "iteration: 198 loss: 0.89343679\n",
      "iteration: 199 loss: 0.90648180\n",
      "epoch:  35 mean loss training: 1.44785929\n",
      "epoch:  35 mean loss validation: 1.47960818\n",
      "iteration:   0 loss: 2.96928978\n",
      "iteration:   1 loss: 1.96323323\n",
      "iteration:   2 loss: 1.49599695\n",
      "iteration:   3 loss: 0.78316480\n",
      "iteration:   4 loss: 2.89057422\n",
      "iteration:   5 loss: 3.25870442\n",
      "iteration:   6 loss: 1.95152998\n",
      "iteration:   7 loss: 1.41785109\n",
      "iteration:   8 loss: 2.93983889\n",
      "iteration:   9 loss: 3.74030638\n",
      "iteration:  10 loss: 0.86401391\n",
      "iteration:  11 loss: 0.48769221\n",
      "iteration:  12 loss: 1.85609651\n",
      "iteration:  13 loss: 0.82727599\n",
      "iteration:  14 loss: 1.04409385\n",
      "iteration:  15 loss: 0.73526788\n",
      "iteration:  16 loss: 1.04567087\n",
      "iteration:  17 loss: 0.87699717\n",
      "iteration:  18 loss: 1.64434493\n",
      "iteration:  19 loss: 0.84598422\n",
      "iteration:  20 loss: 0.56115782\n",
      "iteration:  21 loss: 2.22964883\n",
      "iteration:  22 loss: 0.35180801\n",
      "iteration:  23 loss: 1.08783376\n",
      "iteration:  24 loss: 0.75317270\n",
      "iteration:  25 loss: 2.51806712\n",
      "iteration:  26 loss: 1.39068615\n",
      "iteration:  27 loss: 2.35347939\n",
      "iteration:  28 loss: 0.28662685\n",
      "iteration:  29 loss: 1.02062738\n",
      "iteration:  30 loss: 1.46641493\n",
      "iteration:  31 loss: 0.44786400\n",
      "iteration:  32 loss: 0.46975687\n",
      "iteration:  33 loss: 0.59557474\n",
      "iteration:  34 loss: 2.71271753\n",
      "iteration:  35 loss: 1.09961307\n",
      "iteration:  36 loss: 0.18649675\n",
      "iteration:  37 loss: 0.38906509\n",
      "iteration:  38 loss: 1.14713740\n",
      "iteration:  39 loss: 3.91687489\n",
      "iteration:  40 loss: 2.01355839\n",
      "iteration:  41 loss: 0.62179989\n",
      "iteration:  42 loss: 1.13042986\n",
      "iteration:  43 loss: 3.23525691\n",
      "iteration:  44 loss: 0.81474245\n",
      "iteration:  45 loss: 0.36528203\n",
      "iteration:  46 loss: 1.21673822\n",
      "iteration:  47 loss: 1.33943295\n",
      "iteration:  48 loss: 0.89966488\n",
      "iteration:  49 loss: 0.88979220\n",
      "iteration:  50 loss: 2.13098812\n",
      "iteration:  51 loss: 0.42013791\n",
      "iteration:  52 loss: 1.36382294\n",
      "iteration:  53 loss: 0.60798854\n",
      "iteration:  54 loss: 0.86274052\n",
      "iteration:  55 loss: 3.17063117\n",
      "iteration:  56 loss: 1.02504444\n",
      "iteration:  57 loss: 1.82451499\n",
      "iteration:  58 loss: 3.68752766\n",
      "iteration:  59 loss: 2.27454805\n",
      "iteration:  60 loss: 0.71532708\n",
      "iteration:  61 loss: 0.63146943\n",
      "iteration:  62 loss: 1.32568622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  63 loss: 1.86418211\n",
      "iteration:  64 loss: 1.18088353\n",
      "iteration:  65 loss: 1.59141397\n",
      "iteration:  66 loss: 1.71450639\n",
      "iteration:  67 loss: 1.44078922\n",
      "iteration:  68 loss: 1.37416863\n",
      "iteration:  69 loss: 2.19957209\n",
      "iteration:  70 loss: 1.10449290\n",
      "iteration:  71 loss: 1.19400597\n",
      "iteration:  72 loss: 1.53384066\n",
      "iteration:  73 loss: 3.13489890\n",
      "iteration:  74 loss: 0.87492454\n",
      "iteration:  75 loss: 0.46117505\n",
      "iteration:  76 loss: 0.57568365\n",
      "iteration:  77 loss: 1.09200656\n",
      "iteration:  78 loss: 1.53983450\n",
      "iteration:  79 loss: 0.64345026\n",
      "iteration:  80 loss: 1.92176664\n",
      "iteration:  81 loss: 0.23098913\n",
      "iteration:  82 loss: 0.83820564\n",
      "iteration:  83 loss: 1.48919022\n",
      "iteration:  84 loss: 2.92334342\n",
      "iteration:  85 loss: 0.23659898\n",
      "iteration:  86 loss: 0.53200245\n",
      "iteration:  87 loss: 3.27562714\n",
      "iteration:  88 loss: 0.39701989\n",
      "iteration:  89 loss: 2.81360745\n",
      "iteration:  90 loss: 3.00749445\n",
      "iteration:  91 loss: 0.51753813\n",
      "iteration:  92 loss: 0.85098457\n",
      "iteration:  93 loss: 2.44472957\n",
      "iteration:  94 loss: 1.64258432\n",
      "iteration:  95 loss: 0.80516016\n",
      "iteration:  96 loss: 3.33240771\n",
      "iteration:  97 loss: 1.35496616\n",
      "iteration:  98 loss: 0.57708114\n",
      "iteration:  99 loss: 0.42757285\n",
      "iteration: 100 loss: 1.19488001\n",
      "iteration: 101 loss: 0.46474397\n",
      "iteration: 102 loss: 1.61003995\n",
      "iteration: 103 loss: 2.51081634\n",
      "iteration: 104 loss: 0.36285079\n",
      "iteration: 105 loss: 0.75133741\n",
      "iteration: 106 loss: 0.33182400\n",
      "iteration: 107 loss: 1.23353267\n",
      "iteration: 108 loss: 1.33918452\n",
      "iteration: 109 loss: 0.44636723\n",
      "iteration: 110 loss: 3.31226587\n",
      "iteration: 111 loss: 3.07279396\n",
      "iteration: 112 loss: 2.95042515\n",
      "iteration: 113 loss: 1.68893504\n",
      "iteration: 114 loss: 2.51387644\n",
      "iteration: 115 loss: 3.81529045\n",
      "iteration: 116 loss: 1.43988156\n",
      "iteration: 117 loss: 0.31326181\n",
      "iteration: 118 loss: 0.60229260\n",
      "iteration: 119 loss: 1.51671243\n",
      "iteration: 120 loss: 0.33206853\n",
      "iteration: 121 loss: 0.65451950\n",
      "iteration: 122 loss: 0.60970747\n",
      "iteration: 123 loss: 2.61156964\n",
      "iteration: 124 loss: 2.06312704\n",
      "iteration: 125 loss: 2.29684186\n",
      "iteration: 126 loss: 3.04950571\n",
      "iteration: 127 loss: 0.89995092\n",
      "iteration: 128 loss: 1.87449419\n",
      "iteration: 129 loss: 0.49709487\n",
      "iteration: 130 loss: 2.73190880\n",
      "iteration: 131 loss: 1.89679897\n",
      "iteration: 132 loss: 1.72871983\n",
      "iteration: 133 loss: 0.88005340\n",
      "iteration: 134 loss: 0.72226143\n",
      "iteration: 135 loss: 1.20251572\n",
      "iteration: 136 loss: 1.20538354\n",
      "iteration: 137 loss: 1.48124003\n",
      "iteration: 138 loss: 1.22122157\n",
      "iteration: 139 loss: 1.19881094\n",
      "iteration: 140 loss: 1.65582383\n",
      "iteration: 141 loss: 1.35771871\n",
      "iteration: 142 loss: 0.95145410\n",
      "iteration: 143 loss: 1.38635159\n",
      "iteration: 144 loss: 3.49981689\n",
      "iteration: 145 loss: 2.04542923\n",
      "iteration: 146 loss: 0.69984603\n",
      "iteration: 147 loss: 1.19400215\n",
      "iteration: 148 loss: 1.28310049\n",
      "iteration: 149 loss: 1.42733908\n",
      "iteration: 150 loss: 2.04763794\n",
      "iteration: 151 loss: 1.41820848\n",
      "iteration: 152 loss: 3.68465924\n",
      "iteration: 153 loss: 3.31782913\n",
      "iteration: 154 loss: 1.95252216\n",
      "iteration: 155 loss: 3.03145337\n",
      "iteration: 156 loss: 0.54506069\n",
      "iteration: 157 loss: 0.96624428\n",
      "iteration: 158 loss: 0.98955184\n",
      "iteration: 159 loss: 0.52177328\n",
      "iteration: 160 loss: 0.78491235\n",
      "iteration: 161 loss: 1.01041710\n",
      "iteration: 162 loss: 0.39181569\n",
      "iteration: 163 loss: 3.15123487\n",
      "iteration: 164 loss: 3.43227601\n",
      "iteration: 165 loss: 1.56956327\n",
      "iteration: 166 loss: 0.58984673\n",
      "iteration: 167 loss: 0.24716581\n",
      "iteration: 168 loss: 0.76094890\n",
      "iteration: 169 loss: 1.09282911\n",
      "iteration: 170 loss: 3.35366797\n",
      "iteration: 171 loss: 1.00647688\n",
      "iteration: 172 loss: 0.53474253\n",
      "iteration: 173 loss: 0.47798946\n",
      "iteration: 174 loss: 3.46631455\n",
      "iteration: 175 loss: 2.26774883\n",
      "iteration: 176 loss: 1.82938313\n",
      "iteration: 177 loss: 0.98053789\n",
      "iteration: 178 loss: 0.54796439\n",
      "iteration: 179 loss: 0.21158054\n",
      "iteration: 180 loss: 0.37511840\n",
      "iteration: 181 loss: 0.91314095\n",
      "iteration: 182 loss: 0.36878461\n",
      "iteration: 183 loss: 1.94667006\n",
      "iteration: 184 loss: 0.83766395\n",
      "iteration: 185 loss: 2.05852652\n",
      "iteration: 186 loss: 1.68784201\n",
      "iteration: 187 loss: 0.98381138\n",
      "iteration: 188 loss: 3.73055553\n",
      "iteration: 189 loss: 2.16864061\n",
      "iteration: 190 loss: 2.86782598\n",
      "iteration: 191 loss: 1.29508507\n",
      "iteration: 192 loss: 0.84403783\n",
      "iteration: 193 loss: 1.76676369\n",
      "iteration: 194 loss: 0.64276433\n",
      "iteration: 195 loss: 3.52083921\n",
      "iteration: 196 loss: 3.17005658\n",
      "iteration: 197 loss: 0.83314168\n",
      "iteration: 198 loss: 0.83050066\n",
      "iteration: 199 loss: 0.98281533\n",
      "epoch:  36 mean loss training: 1.49232638\n",
      "epoch:  36 mean loss validation: 1.60625792\n",
      "iteration:   0 loss: 3.47090101\n",
      "iteration:   1 loss: 2.57947755\n",
      "iteration:   2 loss: 1.34893799\n",
      "iteration:   3 loss: 1.06816602\n",
      "iteration:   4 loss: 2.98839712\n",
      "iteration:   5 loss: 2.02210212\n",
      "iteration:   6 loss: 2.24379516\n",
      "iteration:   7 loss: 1.27716041\n",
      "iteration:   8 loss: 3.46512008\n",
      "iteration:   9 loss: 3.70946074\n",
      "iteration:  10 loss: 1.23925745\n",
      "iteration:  11 loss: 0.56163031\n",
      "iteration:  12 loss: 1.23741937\n",
      "iteration:  13 loss: 1.29450917\n",
      "iteration:  14 loss: 0.94850981\n",
      "iteration:  15 loss: 0.75589788\n",
      "iteration:  16 loss: 1.24225533\n",
      "iteration:  17 loss: 1.10351813\n",
      "iteration:  18 loss: 1.63585413\n",
      "iteration:  19 loss: 0.73978013\n",
      "iteration:  20 loss: 0.62352520\n",
      "iteration:  21 loss: 1.93809485\n",
      "iteration:  22 loss: 0.41546524\n",
      "iteration:  23 loss: 1.21180844\n",
      "iteration:  24 loss: 0.37306359\n",
      "iteration:  25 loss: 1.71228135\n",
      "iteration:  26 loss: 1.47004235\n",
      "iteration:  27 loss: 2.29208779\n",
      "iteration:  28 loss: 0.54783541\n",
      "iteration:  29 loss: 0.84882605\n",
      "iteration:  30 loss: 0.61364442\n",
      "iteration:  31 loss: 0.40313858\n",
      "iteration:  32 loss: 0.63982785\n",
      "iteration:  33 loss: 0.84239078\n",
      "iteration:  34 loss: 1.96021497\n",
      "iteration:  35 loss: 1.17481649\n",
      "iteration:  36 loss: 0.15102068\n",
      "iteration:  37 loss: 0.35363644\n",
      "iteration:  38 loss: 1.53258491\n",
      "iteration:  39 loss: 3.29211831\n",
      "iteration:  40 loss: 2.45043540\n",
      "iteration:  41 loss: 1.37618792\n",
      "iteration:  42 loss: 1.43990040\n",
      "iteration:  43 loss: 1.85381401\n",
      "iteration:  44 loss: 0.90330714\n",
      "iteration:  45 loss: 0.40477040\n",
      "iteration:  46 loss: 1.93192422\n",
      "iteration:  47 loss: 1.56395316\n",
      "iteration:  48 loss: 1.32401443\n",
      "iteration:  49 loss: 1.18288875\n",
      "iteration:  50 loss: 1.39477980\n",
      "iteration:  51 loss: 0.78114462\n",
      "iteration:  52 loss: 1.87373722\n",
      "iteration:  53 loss: 0.66166687\n",
      "iteration:  54 loss: 0.88523120\n",
      "iteration:  55 loss: 2.57751322\n",
      "iteration:  56 loss: 1.44321191\n",
      "iteration:  57 loss: 1.47723389\n",
      "iteration:  58 loss: 1.95727932\n",
      "iteration:  59 loss: 1.93543422\n",
      "iteration:  60 loss: 0.63958853\n",
      "iteration:  61 loss: 0.79183245\n",
      "iteration:  62 loss: 0.82210892\n",
      "iteration:  63 loss: 1.49035132\n",
      "iteration:  64 loss: 0.79769337\n",
      "iteration:  65 loss: 2.35659432\n",
      "iteration:  66 loss: 1.50298607\n",
      "iteration:  67 loss: 1.00630438\n",
      "iteration:  68 loss: 1.28447163\n",
      "iteration:  69 loss: 1.12474525\n",
      "iteration:  70 loss: 0.79874736\n",
      "iteration:  71 loss: 0.83196318\n",
      "iteration:  72 loss: 1.26685464\n",
      "iteration:  73 loss: 2.05999994\n",
      "iteration:  74 loss: 0.70689046\n",
      "iteration:  75 loss: 0.55595535\n",
      "iteration:  76 loss: 0.93318307\n",
      "iteration:  77 loss: 1.18577838\n",
      "iteration:  78 loss: 1.40960860\n",
      "iteration:  79 loss: 0.67589062\n",
      "iteration:  80 loss: 1.72514153\n",
      "iteration:  81 loss: 0.32524523\n",
      "iteration:  82 loss: 0.72142458\n",
      "iteration:  83 loss: 0.76671082\n",
      "iteration:  84 loss: 2.94132924\n",
      "iteration:  85 loss: 0.27736703\n",
      "iteration:  86 loss: 0.70293522\n",
      "iteration:  87 loss: 3.25821257\n",
      "iteration:  88 loss: 0.28032851\n",
      "iteration:  89 loss: 2.93411040\n",
      "iteration:  90 loss: 2.38419366\n",
      "iteration:  91 loss: 0.53961307\n",
      "iteration:  92 loss: 0.91055274\n",
      "iteration:  93 loss: 2.14194036\n",
      "iteration:  94 loss: 1.42964673\n",
      "iteration:  95 loss: 0.89761084\n",
      "iteration:  96 loss: 4.38943100\n",
      "iteration:  97 loss: 1.02632928\n",
      "iteration:  98 loss: 0.50569367\n",
      "iteration:  99 loss: 0.47752485\n",
      "iteration: 100 loss: 2.43697834\n",
      "iteration: 101 loss: 0.49600324\n",
      "iteration: 102 loss: 0.99067861\n",
      "iteration: 103 loss: 2.86777520\n",
      "iteration: 104 loss: 0.67076856\n",
      "iteration: 105 loss: 0.82086933\n",
      "iteration: 106 loss: 0.35770991\n",
      "iteration: 107 loss: 0.85322642\n",
      "iteration: 108 loss: 1.23781264\n",
      "iteration: 109 loss: 0.62325966\n",
      "iteration: 110 loss: 2.89661741\n",
      "iteration: 111 loss: 3.07894015\n",
      "iteration: 112 loss: 3.13723969\n",
      "iteration: 113 loss: 1.99107373\n",
      "iteration: 114 loss: 2.61005688\n",
      "iteration: 115 loss: 3.85888505\n",
      "iteration: 116 loss: 2.11123157\n",
      "iteration: 117 loss: 0.22088744\n",
      "iteration: 118 loss: 0.52992946\n",
      "iteration: 119 loss: 0.87282836\n",
      "iteration: 120 loss: 0.27269644\n",
      "iteration: 121 loss: 0.65107954\n",
      "iteration: 122 loss: 0.28490499\n",
      "iteration: 123 loss: 2.51151013\n",
      "iteration: 124 loss: 1.91406894\n",
      "iteration: 125 loss: 2.97885799\n",
      "iteration: 126 loss: 3.29411912\n",
      "iteration: 127 loss: 1.01809573\n",
      "iteration: 128 loss: 2.42953658\n",
      "iteration: 129 loss: 0.39249414\n",
      "iteration: 130 loss: 2.61109686\n",
      "iteration: 131 loss: 2.26551819\n",
      "iteration: 132 loss: 2.76506495\n",
      "iteration: 133 loss: 0.60208184\n",
      "iteration: 134 loss: 0.73669153\n",
      "iteration: 135 loss: 0.69460702\n",
      "iteration: 136 loss: 1.04030621\n",
      "iteration: 137 loss: 1.17370474\n",
      "iteration: 138 loss: 0.84918833\n",
      "iteration: 139 loss: 0.90729332\n",
      "iteration: 140 loss: 2.21225524\n",
      "iteration: 141 loss: 0.83322644\n",
      "iteration: 142 loss: 0.85894400\n",
      "iteration: 143 loss: 1.46684706\n",
      "iteration: 144 loss: 3.51632810\n",
      "iteration: 145 loss: 2.40736985\n",
      "iteration: 146 loss: 0.68632162\n",
      "iteration: 147 loss: 0.99607110\n",
      "iteration: 148 loss: 1.27217078\n",
      "iteration: 149 loss: 1.67359030\n",
      "iteration: 150 loss: 1.94257796\n",
      "iteration: 151 loss: 1.84295785\n",
      "iteration: 152 loss: 3.71265674\n",
      "iteration: 153 loss: 3.16551566\n",
      "iteration: 154 loss: 1.73829424\n",
      "iteration: 155 loss: 3.14282537\n",
      "iteration: 156 loss: 0.66870141\n",
      "iteration: 157 loss: 1.19638503\n",
      "iteration: 158 loss: 1.09523976\n",
      "iteration: 159 loss: 0.59492499\n",
      "iteration: 160 loss: 0.81482637\n",
      "iteration: 161 loss: 1.53381705\n",
      "iteration: 162 loss: 0.41972581\n",
      "iteration: 163 loss: 3.04648948\n",
      "iteration: 164 loss: 3.46758437\n",
      "iteration: 165 loss: 0.53709757\n",
      "iteration: 166 loss: 0.78193951\n",
      "iteration: 167 loss: 0.26029772\n",
      "iteration: 168 loss: 0.92277378\n",
      "iteration: 169 loss: 1.84995699\n",
      "iteration: 170 loss: 3.20288920\n",
      "iteration: 171 loss: 0.77272683\n",
      "iteration: 172 loss: 0.75961643\n",
      "iteration: 173 loss: 0.37324560\n",
      "iteration: 174 loss: 3.52706552\n",
      "iteration: 175 loss: 2.03498864\n",
      "iteration: 176 loss: 1.85787845\n",
      "iteration: 177 loss: 1.11215806\n",
      "iteration: 178 loss: 0.46292895\n",
      "iteration: 179 loss: 0.20837925\n",
      "iteration: 180 loss: 0.55554444\n",
      "iteration: 181 loss: 0.87386894\n",
      "iteration: 182 loss: 0.33297524\n",
      "iteration: 183 loss: 1.85177863\n",
      "iteration: 184 loss: 0.98551488\n",
      "iteration: 185 loss: 1.93610537\n",
      "iteration: 186 loss: 2.12790871\n",
      "iteration: 187 loss: 0.81204689\n",
      "iteration: 188 loss: 3.78036880\n",
      "iteration: 189 loss: 2.08011794\n",
      "iteration: 190 loss: 3.07878280\n",
      "iteration: 191 loss: 1.39008713\n",
      "iteration: 192 loss: 0.68629295\n",
      "iteration: 193 loss: 2.14637208\n",
      "iteration: 194 loss: 0.58893311\n",
      "iteration: 195 loss: 3.60740352\n",
      "iteration: 196 loss: 2.97111559\n",
      "iteration: 197 loss: 1.39904201\n",
      "iteration: 198 loss: 0.45935348\n",
      "iteration: 199 loss: 0.44917551\n",
      "epoch:  37 mean loss training: 1.47380972\n",
      "epoch:  37 mean loss validation: 1.46161556\n",
      "iteration:   0 loss: 2.90917230\n",
      "iteration:   1 loss: 2.33417130\n",
      "iteration:   2 loss: 1.08737838\n",
      "iteration:   3 loss: 0.81681979\n",
      "iteration:   4 loss: 2.72670245\n",
      "iteration:   5 loss: 3.18216801\n",
      "iteration:   6 loss: 1.86219251\n",
      "iteration:   7 loss: 1.27822328\n",
      "iteration:   8 loss: 3.86691189\n",
      "iteration:   9 loss: 3.59164786\n",
      "iteration:  10 loss: 0.57473660\n",
      "iteration:  11 loss: 0.49057832\n",
      "iteration:  12 loss: 1.35610390\n",
      "iteration:  13 loss: 0.79281622\n",
      "iteration:  14 loss: 0.97570503\n",
      "iteration:  15 loss: 0.65538502\n",
      "iteration:  16 loss: 0.82799494\n",
      "iteration:  17 loss: 0.59755558\n",
      "iteration:  18 loss: 1.58535171\n",
      "iteration:  19 loss: 1.03043699\n",
      "iteration:  20 loss: 0.76153183\n",
      "iteration:  21 loss: 1.75379777\n",
      "iteration:  22 loss: 0.35722125\n",
      "iteration:  23 loss: 1.24836040\n",
      "iteration:  24 loss: 0.38096726\n",
      "iteration:  25 loss: 2.16540766\n",
      "iteration:  26 loss: 1.35818064\n",
      "iteration:  27 loss: 2.49424314\n",
      "iteration:  28 loss: 0.53094262\n",
      "iteration:  29 loss: 1.05974948\n",
      "iteration:  30 loss: 0.59476942\n",
      "iteration:  31 loss: 0.33390749\n",
      "iteration:  32 loss: 0.48760882\n",
      "iteration:  33 loss: 0.72530067\n",
      "iteration:  34 loss: 1.61855721\n",
      "iteration:  35 loss: 1.23742902\n",
      "iteration:  36 loss: 0.16603208\n",
      "iteration:  37 loss: 0.33567131\n",
      "iteration:  38 loss: 1.48270535\n",
      "iteration:  39 loss: 3.28363061\n",
      "iteration:  40 loss: 2.32214832\n",
      "iteration:  41 loss: 1.43855059\n",
      "iteration:  42 loss: 1.41490316\n",
      "iteration:  43 loss: 3.16362071\n",
      "iteration:  44 loss: 0.91385382\n",
      "iteration:  45 loss: 0.27073663\n",
      "iteration:  46 loss: 2.09564638\n",
      "iteration:  47 loss: 1.33212984\n",
      "iteration:  48 loss: 1.38039041\n",
      "iteration:  49 loss: 1.11073363\n",
      "iteration:  50 loss: 1.39902449\n",
      "iteration:  51 loss: 0.68649369\n",
      "iteration:  52 loss: 1.73434436\n",
      "iteration:  53 loss: 0.39323378\n",
      "iteration:  54 loss: 0.70224625\n",
      "iteration:  55 loss: 2.91925955\n",
      "iteration:  56 loss: 1.15409064\n",
      "iteration:  57 loss: 1.37143040\n",
      "iteration:  58 loss: 2.22406149\n",
      "iteration:  59 loss: 1.64879370\n",
      "iteration:  60 loss: 0.56987405\n",
      "iteration:  61 loss: 0.65931445\n",
      "iteration:  62 loss: 0.84805429\n",
      "iteration:  63 loss: 1.23800337\n",
      "iteration:  64 loss: 0.82301784\n",
      "iteration:  65 loss: 2.55055761\n",
      "iteration:  66 loss: 1.57516873\n",
      "iteration:  67 loss: 1.15208435\n",
      "iteration:  68 loss: 1.50103569\n",
      "iteration:  69 loss: 1.49742603\n",
      "iteration:  70 loss: 0.79743671\n",
      "iteration:  71 loss: 1.09130931\n",
      "iteration:  72 loss: 1.30795026\n",
      "iteration:  73 loss: 2.00982285\n",
      "iteration:  74 loss: 1.34749722\n",
      "iteration:  75 loss: 0.76834893\n",
      "iteration:  76 loss: 0.95918351\n",
      "iteration:  77 loss: 0.88284516\n",
      "iteration:  78 loss: 1.69284904\n",
      "iteration:  79 loss: 0.70804226\n",
      "iteration:  80 loss: 2.10155177\n",
      "iteration:  81 loss: 0.35941887\n",
      "iteration:  82 loss: 1.09773827\n",
      "iteration:  83 loss: 0.85315508\n",
      "iteration:  84 loss: 3.30949044\n",
      "iteration:  85 loss: 0.26408660\n",
      "iteration:  86 loss: 0.80779290\n",
      "iteration:  87 loss: 3.35395980\n",
      "iteration:  88 loss: 0.25791648\n",
      "iteration:  89 loss: 2.81246567\n",
      "iteration:  90 loss: 1.44880450\n",
      "iteration:  91 loss: 0.54556435\n",
      "iteration:  92 loss: 1.12391675\n",
      "iteration:  93 loss: 2.17540264\n",
      "iteration:  94 loss: 1.12733829\n",
      "iteration:  95 loss: 0.79538590\n",
      "iteration:  96 loss: 4.38845348\n",
      "iteration:  97 loss: 0.75467515\n",
      "iteration:  98 loss: 0.50834465\n",
      "iteration:  99 loss: 0.43012995\n",
      "iteration: 100 loss: 0.70471638\n",
      "iteration: 101 loss: 0.31373054\n",
      "iteration: 102 loss: 0.93422669\n",
      "iteration: 103 loss: 2.73590803\n",
      "iteration: 104 loss: 0.67928457\n",
      "iteration: 105 loss: 0.84434831\n",
      "iteration: 106 loss: 0.41347510\n",
      "iteration: 107 loss: 0.96327090\n",
      "iteration: 108 loss: 1.56618512\n",
      "iteration: 109 loss: 0.59934348\n",
      "iteration: 110 loss: 2.84770012\n",
      "iteration: 111 loss: 3.06988931\n",
      "iteration: 112 loss: 3.32973409\n",
      "iteration: 113 loss: 1.98195398\n",
      "iteration: 114 loss: 2.59082603\n",
      "iteration: 115 loss: 4.34631777\n",
      "iteration: 116 loss: 2.22211313\n",
      "iteration: 117 loss: 0.18012568\n",
      "iteration: 118 loss: 0.51180053\n",
      "iteration: 119 loss: 0.91280425\n",
      "iteration: 120 loss: 0.42361081\n",
      "iteration: 121 loss: 0.65101498\n",
      "iteration: 122 loss: 0.26262465\n",
      "iteration: 123 loss: 2.30507421\n",
      "iteration: 124 loss: 1.89051580\n",
      "iteration: 125 loss: 3.38258934\n",
      "iteration: 126 loss: 3.14133382\n",
      "iteration: 127 loss: 1.00382555\n",
      "iteration: 128 loss: 2.46080184\n",
      "iteration: 129 loss: 0.32863387\n",
      "iteration: 130 loss: 2.54993248\n",
      "iteration: 131 loss: 2.16237855\n",
      "iteration: 132 loss: 2.54713249\n",
      "iteration: 133 loss: 0.56941491\n",
      "iteration: 134 loss: 0.70084798\n",
      "iteration: 135 loss: 0.84549236\n",
      "iteration: 136 loss: 1.00836813\n",
      "iteration: 137 loss: 2.16725659\n",
      "iteration: 138 loss: 0.77857953\n",
      "iteration: 139 loss: 0.97502524\n",
      "iteration: 140 loss: 2.11585784\n",
      "iteration: 141 loss: 0.77717698\n",
      "iteration: 142 loss: 0.76000482\n",
      "iteration: 143 loss: 0.82413089\n",
      "iteration: 144 loss: 3.62444139\n",
      "iteration: 145 loss: 2.67846370\n",
      "iteration: 146 loss: 0.54120362\n",
      "iteration: 147 loss: 0.96899509\n",
      "iteration: 148 loss: 0.87207788\n",
      "iteration: 149 loss: 1.32993734\n",
      "iteration: 150 loss: 1.47492647\n",
      "iteration: 151 loss: 1.27441883\n",
      "iteration: 152 loss: 3.06934023\n",
      "iteration: 153 loss: 3.44019556\n",
      "iteration: 154 loss: 1.08917582\n",
      "iteration: 155 loss: 2.89380622\n",
      "iteration: 156 loss: 0.27572235\n",
      "iteration: 157 loss: 0.95855308\n",
      "iteration: 158 loss: 0.90407676\n",
      "iteration: 159 loss: 0.34263226\n",
      "iteration: 160 loss: 0.73021126\n",
      "iteration: 161 loss: 1.74514532\n",
      "iteration: 162 loss: 0.36571723\n",
      "iteration: 163 loss: 3.24939466\n",
      "iteration: 164 loss: 3.37769318\n",
      "iteration: 165 loss: 0.91333693\n",
      "iteration: 166 loss: 0.65521264\n",
      "iteration: 167 loss: 0.23873983\n",
      "iteration: 168 loss: 0.79011112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 169 loss: 1.27903855\n",
      "iteration: 170 loss: 3.24223495\n",
      "iteration: 171 loss: 0.76571685\n",
      "iteration: 172 loss: 0.80995280\n",
      "iteration: 173 loss: 0.40707463\n",
      "iteration: 174 loss: 3.49275875\n",
      "iteration: 175 loss: 1.89107823\n",
      "iteration: 176 loss: 1.83186853\n",
      "iteration: 177 loss: 1.04910362\n",
      "iteration: 178 loss: 0.52729815\n",
      "iteration: 179 loss: 0.19703755\n",
      "iteration: 180 loss: 0.44566742\n",
      "iteration: 181 loss: 0.91185659\n",
      "iteration: 182 loss: 0.38431948\n",
      "iteration: 183 loss: 1.50217676\n",
      "iteration: 184 loss: 0.78512979\n",
      "iteration: 185 loss: 1.92511225\n",
      "iteration: 186 loss: 1.77772331\n",
      "iteration: 187 loss: 0.96115500\n",
      "iteration: 188 loss: 3.84610891\n",
      "iteration: 189 loss: 2.14424562\n",
      "iteration: 190 loss: 2.58808994\n",
      "iteration: 191 loss: 1.04417586\n",
      "iteration: 192 loss: 0.89947367\n",
      "iteration: 193 loss: 1.62139583\n",
      "iteration: 194 loss: 0.64416736\n",
      "iteration: 195 loss: 3.17123580\n",
      "iteration: 196 loss: 3.17323732\n",
      "iteration: 197 loss: 0.77265882\n",
      "iteration: 198 loss: 0.90643293\n",
      "iteration: 199 loss: 0.86883384\n",
      "epoch:  38 mean loss training: 1.43569219\n",
      "epoch:  38 mean loss validation: 1.50266838\n",
      "iteration:   0 loss: 2.99169540\n",
      "iteration:   1 loss: 2.50961995\n",
      "iteration:   2 loss: 1.35903311\n",
      "iteration:   3 loss: 0.90668833\n",
      "iteration:   4 loss: 3.24445176\n",
      "iteration:   5 loss: 2.27245498\n",
      "iteration:   6 loss: 1.76919353\n",
      "iteration:   7 loss: 1.32791281\n",
      "iteration:   8 loss: 3.56920743\n",
      "iteration:   9 loss: 3.57139754\n",
      "iteration:  10 loss: 0.97726101\n",
      "iteration:  11 loss: 0.84108180\n",
      "iteration:  12 loss: 1.60325074\n",
      "iteration:  13 loss: 1.11318386\n",
      "iteration:  14 loss: 0.96868014\n",
      "iteration:  15 loss: 0.65704280\n",
      "iteration:  16 loss: 1.10170150\n",
      "iteration:  17 loss: 1.13405383\n",
      "iteration:  18 loss: 1.95058882\n",
      "iteration:  19 loss: 1.11070848\n",
      "iteration:  20 loss: 0.95236832\n",
      "iteration:  21 loss: 1.42516017\n",
      "iteration:  22 loss: 0.45348254\n",
      "iteration:  23 loss: 1.15968251\n",
      "iteration:  24 loss: 0.40603873\n",
      "iteration:  25 loss: 2.07209015\n",
      "iteration:  26 loss: 1.44592905\n",
      "iteration:  27 loss: 2.45297408\n",
      "iteration:  28 loss: 0.59994465\n",
      "iteration:  29 loss: 1.21851027\n",
      "iteration:  30 loss: 0.65323949\n",
      "iteration:  31 loss: 0.45929143\n",
      "iteration:  32 loss: 0.54105330\n",
      "iteration:  33 loss: 0.78256708\n",
      "iteration:  34 loss: 1.72975802\n",
      "iteration:  35 loss: 1.23677838\n",
      "iteration:  36 loss: 0.15070336\n",
      "iteration:  37 loss: 0.31380722\n",
      "iteration:  38 loss: 1.45792258\n",
      "iteration:  39 loss: 3.26965904\n",
      "iteration:  40 loss: 2.11605859\n",
      "iteration:  41 loss: 1.84777653\n",
      "iteration:  42 loss: 1.33718503\n",
      "iteration:  43 loss: 3.13926101\n",
      "iteration:  44 loss: 0.57684761\n",
      "iteration:  45 loss: 0.27800229\n",
      "iteration:  46 loss: 2.07878232\n",
      "iteration:  47 loss: 1.18546045\n",
      "iteration:  48 loss: 1.08322430\n",
      "iteration:  49 loss: 0.88748467\n",
      "iteration:  50 loss: 2.04169774\n",
      "iteration:  51 loss: 0.44062483\n",
      "iteration:  52 loss: 1.49772000\n",
      "iteration:  53 loss: 0.30008176\n",
      "iteration:  54 loss: 0.61993676\n",
      "iteration:  55 loss: 2.83172441\n",
      "iteration:  56 loss: 1.03956699\n",
      "iteration:  57 loss: 1.47273552\n",
      "iteration:  58 loss: 3.35922885\n",
      "iteration:  59 loss: 1.59641886\n",
      "iteration:  60 loss: 0.61672109\n",
      "iteration:  61 loss: 0.57982093\n",
      "iteration:  62 loss: 0.87275553\n",
      "iteration:  63 loss: 1.32906640\n",
      "iteration:  64 loss: 0.68668854\n",
      "iteration:  65 loss: 2.24169540\n",
      "iteration:  66 loss: 1.15104914\n",
      "iteration:  67 loss: 1.12395239\n",
      "iteration:  68 loss: 1.21189237\n",
      "iteration:  69 loss: 1.90920830\n",
      "iteration:  70 loss: 1.18475378\n",
      "iteration:  71 loss: 0.93129587\n",
      "iteration:  72 loss: 1.30651546\n",
      "iteration:  73 loss: 2.31984019\n",
      "iteration:  74 loss: 0.94501644\n",
      "iteration:  75 loss: 0.59123600\n",
      "iteration:  76 loss: 0.91177052\n",
      "iteration:  77 loss: 1.05758142\n",
      "iteration:  78 loss: 1.72983623\n",
      "iteration:  79 loss: 0.63367331\n",
      "iteration:  80 loss: 1.72816730\n",
      "iteration:  81 loss: 0.33013701\n",
      "iteration:  82 loss: 0.93622690\n",
      "iteration:  83 loss: 1.10469103\n",
      "iteration:  84 loss: 3.22072434\n",
      "iteration:  85 loss: 0.26053300\n",
      "iteration:  86 loss: 0.65498573\n",
      "iteration:  87 loss: 3.20859694\n",
      "iteration:  88 loss: 0.27354124\n",
      "iteration:  89 loss: 3.00780463\n",
      "iteration:  90 loss: 1.48074305\n",
      "iteration:  91 loss: 0.66944641\n",
      "iteration:  92 loss: 1.09830987\n",
      "iteration:  93 loss: 2.17567039\n",
      "iteration:  94 loss: 1.24890935\n",
      "iteration:  95 loss: 0.81148100\n",
      "iteration:  96 loss: 4.60993481\n",
      "iteration:  97 loss: 1.07009363\n",
      "iteration:  98 loss: 0.44556925\n",
      "iteration:  99 loss: 0.40253222\n",
      "iteration: 100 loss: 0.59260845\n",
      "iteration: 101 loss: 0.47409198\n",
      "iteration: 102 loss: 0.98662961\n",
      "iteration: 103 loss: 3.01886916\n",
      "iteration: 104 loss: 0.50033301\n",
      "iteration: 105 loss: 0.88165444\n",
      "iteration: 106 loss: 0.37357122\n",
      "iteration: 107 loss: 0.87637979\n",
      "iteration: 108 loss: 1.67322826\n",
      "iteration: 109 loss: 0.57062835\n",
      "iteration: 110 loss: 3.02963519\n",
      "iteration: 111 loss: 3.08869982\n",
      "iteration: 112 loss: 3.04435372\n",
      "iteration: 113 loss: 1.97853792\n",
      "iteration: 114 loss: 2.50632572\n",
      "iteration: 115 loss: 3.75554347\n",
      "iteration: 116 loss: 1.37351656\n",
      "iteration: 117 loss: 0.18962318\n",
      "iteration: 118 loss: 0.53478283\n",
      "iteration: 119 loss: 1.41225898\n",
      "iteration: 120 loss: 0.24562673\n",
      "iteration: 121 loss: 0.61713749\n",
      "iteration: 122 loss: 0.31959599\n",
      "iteration: 123 loss: 2.41858888\n",
      "iteration: 124 loss: 2.20902681\n",
      "iteration: 125 loss: 2.79011774\n",
      "iteration: 126 loss: 3.16837883\n",
      "iteration: 127 loss: 1.10010362\n",
      "iteration: 128 loss: 1.87950420\n",
      "iteration: 129 loss: 0.40664238\n",
      "iteration: 130 loss: 2.69180942\n",
      "iteration: 131 loss: 2.09202909\n",
      "iteration: 132 loss: 2.74098492\n",
      "iteration: 133 loss: 0.73419410\n",
      "iteration: 134 loss: 0.74249655\n",
      "iteration: 135 loss: 1.55535746\n",
      "iteration: 136 loss: 0.94093561\n",
      "iteration: 137 loss: 1.27757990\n",
      "iteration: 138 loss: 0.91592312\n",
      "iteration: 139 loss: 0.99859953\n",
      "iteration: 140 loss: 2.09400320\n",
      "iteration: 141 loss: 0.84173495\n",
      "iteration: 142 loss: 0.84925205\n",
      "iteration: 143 loss: 1.35974991\n",
      "iteration: 144 loss: 3.43895030\n",
      "iteration: 145 loss: 2.36540818\n",
      "iteration: 146 loss: 1.16362023\n",
      "iteration: 147 loss: 0.97327113\n",
      "iteration: 148 loss: 1.11293244\n",
      "iteration: 149 loss: 1.89785278\n",
      "iteration: 150 loss: 1.89214945\n",
      "iteration: 151 loss: 1.35823035\n",
      "iteration: 152 loss: 3.42544484\n",
      "iteration: 153 loss: 3.43831706\n",
      "iteration: 154 loss: 1.66254735\n",
      "iteration: 155 loss: 2.95040607\n",
      "iteration: 156 loss: 0.42088735\n",
      "iteration: 157 loss: 1.05587542\n",
      "iteration: 158 loss: 0.90870595\n",
      "iteration: 159 loss: 0.45279264\n",
      "iteration: 160 loss: 0.73555285\n",
      "iteration: 161 loss: 1.48036027\n",
      "iteration: 162 loss: 0.36514378\n",
      "iteration: 163 loss: 2.87788963\n",
      "iteration: 164 loss: 3.41275573\n",
      "iteration: 165 loss: 0.45268682\n",
      "iteration: 166 loss: 0.62019444\n",
      "iteration: 167 loss: 0.23457947\n",
      "iteration: 168 loss: 0.80635226\n",
      "iteration: 169 loss: 1.62536919\n",
      "iteration: 170 loss: 3.18558455\n",
      "iteration: 171 loss: 0.75784391\n",
      "iteration: 172 loss: 0.70666289\n",
      "iteration: 173 loss: 0.38050133\n",
      "iteration: 174 loss: 3.52195978\n",
      "iteration: 175 loss: 1.79872942\n",
      "iteration: 176 loss: 1.94064844\n",
      "iteration: 177 loss: 0.96399361\n",
      "iteration: 178 loss: 0.45890558\n",
      "iteration: 179 loss: 0.18437187\n",
      "iteration: 180 loss: 0.39759344\n",
      "iteration: 181 loss: 0.94194978\n",
      "iteration: 182 loss: 0.31274807\n",
      "iteration: 183 loss: 1.40126967\n",
      "iteration: 184 loss: 0.90775806\n",
      "iteration: 185 loss: 1.86741865\n",
      "iteration: 186 loss: 1.77534902\n",
      "iteration: 187 loss: 0.84845775\n",
      "iteration: 188 loss: 3.38105631\n",
      "iteration: 189 loss: 2.09644747\n",
      "iteration: 190 loss: 2.50432897\n",
      "iteration: 191 loss: 1.00869036\n",
      "iteration: 192 loss: 1.11736143\n",
      "iteration: 193 loss: 1.66735852\n",
      "iteration: 194 loss: 0.67276865\n",
      "iteration: 195 loss: 3.25618505\n",
      "iteration: 196 loss: 3.17878890\n",
      "iteration: 197 loss: 0.74028510\n",
      "iteration: 198 loss: 0.81361622\n",
      "iteration: 199 loss: 0.89569890\n",
      "epoch:  39 mean loss training: 1.45112848\n",
      "epoch:  39 mean loss validation: 1.49881792\n",
      "iteration:   0 loss: 2.95788288\n",
      "iteration:   1 loss: 2.53754854\n",
      "iteration:   2 loss: 1.44243264\n",
      "iteration:   3 loss: 0.81565863\n",
      "iteration:   4 loss: 3.59263682\n",
      "iteration:   5 loss: 2.06906915\n",
      "iteration:   6 loss: 2.05301452\n",
      "iteration:   7 loss: 1.29155242\n",
      "iteration:   8 loss: 3.32494569\n",
      "iteration:   9 loss: 3.63420224\n",
      "iteration:  10 loss: 0.82448417\n",
      "iteration:  11 loss: 0.65470469\n",
      "iteration:  12 loss: 1.70481408\n",
      "iteration:  13 loss: 1.06767011\n",
      "iteration:  14 loss: 0.96221727\n",
      "iteration:  15 loss: 0.71669543\n",
      "iteration:  16 loss: 0.95809746\n",
      "iteration:  17 loss: 1.08343685\n",
      "iteration:  18 loss: 1.58554995\n",
      "iteration:  19 loss: 0.82787716\n",
      "iteration:  20 loss: 0.76957536\n",
      "iteration:  21 loss: 2.25871849\n",
      "iteration:  22 loss: 0.46303889\n",
      "iteration:  23 loss: 1.12655151\n",
      "iteration:  24 loss: 0.50769079\n",
      "iteration:  25 loss: 2.29122972\n",
      "iteration:  26 loss: 1.55222964\n",
      "iteration:  27 loss: 2.41568804\n",
      "iteration:  28 loss: 0.36845818\n",
      "iteration:  29 loss: 1.29241872\n",
      "iteration:  30 loss: 0.81807631\n",
      "iteration:  31 loss: 0.53482479\n",
      "iteration:  32 loss: 0.50280851\n",
      "iteration:  33 loss: 0.55390257\n",
      "iteration:  34 loss: 2.65631247\n",
      "iteration:  35 loss: 1.17549992\n",
      "iteration:  36 loss: 0.20316879\n",
      "iteration:  37 loss: 0.34634018\n",
      "iteration:  38 loss: 1.14015019\n",
      "iteration:  39 loss: 3.42927670\n",
      "iteration:  40 loss: 2.02137065\n",
      "iteration:  41 loss: 1.16875207\n",
      "iteration:  42 loss: 1.13785231\n",
      "iteration:  43 loss: 2.99094892\n",
      "iteration:  44 loss: 0.64986557\n",
      "iteration:  45 loss: 0.43604741\n",
      "iteration:  46 loss: 1.80277264\n",
      "iteration:  47 loss: 0.93163520\n",
      "iteration:  48 loss: 0.77802694\n",
      "iteration:  49 loss: 0.67461360\n",
      "iteration:  50 loss: 2.60746598\n",
      "iteration:  51 loss: 0.40051991\n",
      "iteration:  52 loss: 1.40337479\n",
      "iteration:  53 loss: 0.31058165\n",
      "iteration:  54 loss: 0.76571929\n",
      "iteration:  55 loss: 3.75999475\n",
      "iteration:  56 loss: 1.02506304\n",
      "iteration:  57 loss: 2.82242489\n",
      "iteration:  58 loss: 3.55418015\n",
      "iteration:  59 loss: 1.07957757\n",
      "iteration:  60 loss: 0.36230490\n",
      "iteration:  61 loss: 0.29360223\n",
      "iteration:  62 loss: 0.37412366\n",
      "iteration:  63 loss: 0.68289387\n",
      "iteration:  64 loss: 0.25884703\n",
      "iteration:  65 loss: 1.78625214\n",
      "iteration:  66 loss: 0.57835895\n",
      "iteration:  67 loss: 0.22895592\n",
      "iteration:  68 loss: 0.29392686\n",
      "iteration:  69 loss: 0.47347710\n",
      "iteration:  70 loss: 0.47496831\n",
      "iteration:  71 loss: 0.54668099\n",
      "iteration:  72 loss: 1.12448537\n",
      "iteration:  73 loss: 2.03765917\n",
      "iteration:  74 loss: 0.73510015\n",
      "iteration:  75 loss: 0.58100057\n",
      "iteration:  76 loss: 0.59909558\n",
      "iteration:  77 loss: 0.93740094\n",
      "iteration:  78 loss: 1.61245394\n",
      "iteration:  79 loss: 0.63683105\n",
      "iteration:  80 loss: 1.84976900\n",
      "iteration:  81 loss: 0.27837566\n",
      "iteration:  82 loss: 0.92070872\n",
      "iteration:  83 loss: 0.97179663\n",
      "iteration:  84 loss: 3.14234734\n",
      "iteration:  85 loss: 0.24569675\n",
      "iteration:  86 loss: 0.65269351\n",
      "iteration:  87 loss: 3.17156458\n",
      "iteration:  88 loss: 0.34960836\n",
      "iteration:  89 loss: 3.09083772\n",
      "iteration:  90 loss: 2.30517054\n",
      "iteration:  91 loss: 0.53512096\n",
      "iteration:  92 loss: 0.97222477\n",
      "iteration:  93 loss: 2.36330700\n",
      "iteration:  94 loss: 1.29639399\n",
      "iteration:  95 loss: 0.81095409\n",
      "iteration:  96 loss: 4.48421860\n",
      "iteration:  97 loss: 1.00482523\n",
      "iteration:  98 loss: 0.55130398\n",
      "iteration:  99 loss: 0.41562599\n",
      "iteration: 100 loss: 1.33795977\n",
      "iteration: 101 loss: 0.37979472\n",
      "iteration: 102 loss: 0.99541783\n",
      "iteration: 103 loss: 2.94720268\n",
      "iteration: 104 loss: 0.46482810\n",
      "iteration: 105 loss: 0.79983962\n",
      "iteration: 106 loss: 0.31962901\n",
      "iteration: 107 loss: 0.81908965\n",
      "iteration: 108 loss: 1.25973237\n",
      "iteration: 109 loss: 0.43649605\n",
      "iteration: 110 loss: 2.90718579\n",
      "iteration: 111 loss: 3.08928251\n",
      "iteration: 112 loss: 2.96943617\n",
      "iteration: 113 loss: 1.75978661\n",
      "iteration: 114 loss: 2.53273797\n",
      "iteration: 115 loss: 3.40819716\n",
      "iteration: 116 loss: 1.75433052\n",
      "iteration: 117 loss: 0.22854088\n",
      "iteration: 118 loss: 0.53827447\n",
      "iteration: 119 loss: 1.31914270\n",
      "iteration: 120 loss: 0.27737030\n",
      "iteration: 121 loss: 0.67337877\n",
      "iteration: 122 loss: 0.44219896\n",
      "iteration: 123 loss: 2.42227387\n",
      "iteration: 124 loss: 2.25505519\n",
      "iteration: 125 loss: 2.63430882\n",
      "iteration: 126 loss: 3.03108144\n",
      "iteration: 127 loss: 0.93705970\n",
      "iteration: 128 loss: 2.01430845\n",
      "iteration: 129 loss: 0.40663001\n",
      "iteration: 130 loss: 2.73588014\n",
      "iteration: 131 loss: 2.03533769\n",
      "iteration: 132 loss: 2.74067020\n",
      "iteration: 133 loss: 0.65586054\n",
      "iteration: 134 loss: 0.66734821\n",
      "iteration: 135 loss: 1.32222903\n",
      "iteration: 136 loss: 0.94826347\n",
      "iteration: 137 loss: 1.05312634\n",
      "iteration: 138 loss: 0.97004908\n",
      "iteration: 139 loss: 0.97270757\n",
      "iteration: 140 loss: 2.05996633\n",
      "iteration: 141 loss: 0.85942483\n",
      "iteration: 142 loss: 0.82052398\n",
      "iteration: 143 loss: 1.55768466\n",
      "iteration: 144 loss: 3.48718548\n",
      "iteration: 145 loss: 2.26074052\n",
      "iteration: 146 loss: 0.67489445\n",
      "iteration: 147 loss: 0.99119663\n",
      "iteration: 148 loss: 1.02846897\n",
      "iteration: 149 loss: 1.93720853\n",
      "iteration: 150 loss: 1.92445552\n",
      "iteration: 151 loss: 1.33966720\n",
      "iteration: 152 loss: 3.34359789\n",
      "iteration: 153 loss: 3.51024294\n",
      "iteration: 154 loss: 1.47976172\n",
      "iteration: 155 loss: 2.86689091\n",
      "iteration: 156 loss: 0.34598541\n",
      "iteration: 157 loss: 1.00096118\n",
      "iteration: 158 loss: 0.80852795\n",
      "iteration: 159 loss: 0.37455434\n",
      "iteration: 160 loss: 0.84262043\n",
      "iteration: 161 loss: 1.30185783\n",
      "iteration: 162 loss: 0.34519726\n",
      "iteration: 163 loss: 3.06191444\n",
      "iteration: 164 loss: 3.39950895\n",
      "iteration: 165 loss: 1.63851333\n",
      "iteration: 166 loss: 0.56549370\n",
      "iteration: 167 loss: 0.22085927\n",
      "iteration: 168 loss: 0.74944884\n",
      "iteration: 169 loss: 1.31637657\n",
      "iteration: 170 loss: 3.35120058\n",
      "iteration: 171 loss: 0.72261399\n",
      "iteration: 172 loss: 0.68635589\n",
      "iteration: 173 loss: 0.27239981\n",
      "iteration: 174 loss: 3.67784929\n",
      "iteration: 175 loss: 2.51604581\n",
      "iteration: 176 loss: 2.17047143\n",
      "iteration: 177 loss: 0.71487510\n",
      "iteration: 178 loss: 0.60373008\n",
      "iteration: 179 loss: 0.16678590\n",
      "iteration: 180 loss: 0.28170514\n",
      "iteration: 181 loss: 1.02535951\n",
      "iteration: 182 loss: 0.33900994\n",
      "iteration: 183 loss: 1.30931163\n",
      "iteration: 184 loss: 0.82033920\n",
      "iteration: 185 loss: 1.97167242\n",
      "iteration: 186 loss: 1.53847003\n",
      "iteration: 187 loss: 0.88394910\n",
      "iteration: 188 loss: 3.33314085\n",
      "iteration: 189 loss: 2.11422634\n",
      "iteration: 190 loss: 2.05676723\n",
      "iteration: 191 loss: 0.95156890\n",
      "iteration: 192 loss: 1.13053715\n",
      "iteration: 193 loss: 1.55415010\n",
      "iteration: 194 loss: 0.54779631\n",
      "iteration: 195 loss: 3.14464664\n",
      "iteration: 196 loss: 3.19111872\n",
      "iteration: 197 loss: 0.79892129\n",
      "iteration: 198 loss: 0.94542199\n",
      "iteration: 199 loss: 0.78494775\n",
      "epoch:  40 mean loss training: 1.41021681\n",
      "epoch:  40 mean loss validation: 1.48425448\n",
      "iteration:   0 loss: 3.09046388\n",
      "iteration:   1 loss: 1.88353288\n",
      "iteration:   2 loss: 1.46842217\n",
      "iteration:   3 loss: 0.87515718\n",
      "iteration:   4 loss: 3.55125427\n",
      "iteration:   5 loss: 2.31406116\n",
      "iteration:   6 loss: 1.92820418\n",
      "iteration:   7 loss: 1.21425641\n",
      "iteration:   8 loss: 2.98724246\n",
      "iteration:   9 loss: 3.79178476\n",
      "iteration:  10 loss: 1.04934132\n",
      "iteration:  11 loss: 0.90493625\n",
      "iteration:  12 loss: 1.71755815\n",
      "iteration:  13 loss: 0.98611736\n",
      "iteration:  14 loss: 0.99427742\n",
      "iteration:  15 loss: 0.80519861\n",
      "iteration:  16 loss: 1.09278560\n",
      "iteration:  17 loss: 1.09462225\n",
      "iteration:  18 loss: 1.72972119\n",
      "iteration:  19 loss: 1.00581038\n",
      "iteration:  20 loss: 0.88739616\n",
      "iteration:  21 loss: 2.89752626\n",
      "iteration:  22 loss: 0.53265077\n",
      "iteration:  23 loss: 1.10657084\n",
      "iteration:  24 loss: 0.52586663\n",
      "iteration:  25 loss: 2.34023094\n",
      "iteration:  26 loss: 1.54152548\n",
      "iteration:  27 loss: 2.46819568\n",
      "iteration:  28 loss: 0.35998029\n",
      "iteration:  29 loss: 1.36561000\n",
      "iteration:  30 loss: 1.13862860\n",
      "iteration:  31 loss: 0.69792384\n",
      "iteration:  32 loss: 0.49005339\n",
      "iteration:  33 loss: 0.47006622\n",
      "iteration:  34 loss: 2.63530898\n",
      "iteration:  35 loss: 1.18386006\n",
      "iteration:  36 loss: 0.20683050\n",
      "iteration:  37 loss: 0.33961138\n",
      "iteration:  38 loss: 1.17239559\n",
      "iteration:  39 loss: 3.43110991\n",
      "iteration:  40 loss: 2.06168270\n",
      "iteration:  41 loss: 1.21570718\n",
      "iteration:  42 loss: 1.38509202\n",
      "iteration:  43 loss: 3.10620570\n",
      "iteration:  44 loss: 0.78839594\n",
      "iteration:  45 loss: 0.49956903\n",
      "iteration:  46 loss: 1.11120939\n",
      "iteration:  47 loss: 1.35708606\n",
      "iteration:  48 loss: 0.75826126\n",
      "iteration:  49 loss: 0.64856642\n",
      "iteration:  50 loss: 2.42322135\n",
      "iteration:  51 loss: 0.42998329\n",
      "iteration:  52 loss: 1.30697513\n",
      "iteration:  53 loss: 0.78334266\n",
      "iteration:  54 loss: 0.86790770\n",
      "iteration:  55 loss: 2.70948005\n",
      "iteration:  56 loss: 0.93262792\n",
      "iteration:  57 loss: 1.28551650\n",
      "iteration:  58 loss: 3.82964778\n",
      "iteration:  59 loss: 1.77706039\n",
      "iteration:  60 loss: 1.04568136\n",
      "iteration:  61 loss: 0.58527410\n",
      "iteration:  62 loss: 1.69590414\n",
      "iteration:  63 loss: 1.23842919\n",
      "iteration:  64 loss: 0.51157492\n",
      "iteration:  65 loss: 1.13701618\n",
      "iteration:  66 loss: 0.86883080\n",
      "iteration:  67 loss: 0.57687116\n",
      "iteration:  68 loss: 0.53693503\n",
      "iteration:  69 loss: 1.14487231\n",
      "iteration:  70 loss: 0.62363696\n",
      "iteration:  71 loss: 0.50415313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  72 loss: 1.17252624\n",
      "iteration:  73 loss: 3.00586653\n",
      "iteration:  74 loss: 0.90517396\n",
      "iteration:  75 loss: 0.78720462\n",
      "iteration:  76 loss: 0.67280418\n",
      "iteration:  77 loss: 0.76421350\n",
      "iteration:  78 loss: 1.61244822\n",
      "iteration:  79 loss: 0.63000429\n",
      "iteration:  80 loss: 1.87399733\n",
      "iteration:  81 loss: 0.34802324\n",
      "iteration:  82 loss: 0.99349135\n",
      "iteration:  83 loss: 1.40046203\n",
      "iteration:  84 loss: 2.58742285\n",
      "iteration:  85 loss: 0.22637753\n",
      "iteration:  86 loss: 0.53572565\n",
      "iteration:  87 loss: 3.23442221\n",
      "iteration:  88 loss: 0.46712315\n",
      "iteration:  89 loss: 3.18410087\n",
      "iteration:  90 loss: 3.01537895\n",
      "iteration:  91 loss: 0.41311002\n",
      "iteration:  92 loss: 0.61179584\n",
      "iteration:  93 loss: 2.31894803\n",
      "iteration:  94 loss: 1.86850786\n",
      "iteration:  95 loss: 0.75021815\n",
      "iteration:  96 loss: 3.90386081\n",
      "iteration:  97 loss: 1.16447926\n",
      "iteration:  98 loss: 0.50085706\n",
      "iteration:  99 loss: 0.37955320\n",
      "iteration: 100 loss: 1.46958661\n",
      "iteration: 101 loss: 0.45705259\n",
      "iteration: 102 loss: 0.67418963\n",
      "iteration: 103 loss: 2.47945523\n",
      "iteration: 104 loss: 0.75692397\n",
      "iteration: 105 loss: 0.81490022\n",
      "iteration: 106 loss: 0.39152527\n",
      "iteration: 107 loss: 1.73955011\n",
      "iteration: 108 loss: 1.18169093\n",
      "iteration: 109 loss: 0.50959134\n",
      "iteration: 110 loss: 3.42406297\n",
      "iteration: 111 loss: 3.10427785\n",
      "iteration: 112 loss: 3.51163030\n",
      "iteration: 113 loss: 2.74348927\n",
      "iteration: 114 loss: 1.69762540\n",
      "iteration: 115 loss: 3.96999907\n",
      "iteration: 116 loss: 1.16217816\n",
      "iteration: 117 loss: 0.37653723\n",
      "iteration: 118 loss: 0.53243166\n",
      "iteration: 119 loss: 1.03190196\n",
      "iteration: 120 loss: 0.31523201\n",
      "iteration: 121 loss: 0.68140554\n",
      "iteration: 122 loss: 0.33580616\n",
      "iteration: 123 loss: 2.50504971\n",
      "iteration: 124 loss: 1.77232385\n",
      "iteration: 125 loss: 2.44717622\n",
      "iteration: 126 loss: 3.22414327\n",
      "iteration: 127 loss: 1.00692499\n",
      "iteration: 128 loss: 1.59454000\n",
      "iteration: 129 loss: 0.43154630\n",
      "iteration: 130 loss: 2.13639808\n",
      "iteration: 131 loss: 1.70255089\n",
      "iteration: 132 loss: 2.15246224\n",
      "iteration: 133 loss: 1.58279467\n",
      "iteration: 134 loss: 1.43945169\n",
      "iteration: 135 loss: 1.43994260\n",
      "iteration: 136 loss: 1.03505683\n",
      "iteration: 137 loss: 1.70501065\n",
      "iteration: 138 loss: 0.96968359\n",
      "iteration: 139 loss: 0.73996192\n",
      "iteration: 140 loss: 1.73841190\n",
      "iteration: 141 loss: 1.16617072\n",
      "iteration: 142 loss: 1.08888280\n",
      "iteration: 143 loss: 1.68211687\n",
      "iteration: 144 loss: 3.62078142\n",
      "iteration: 145 loss: 2.14275479\n",
      "iteration: 146 loss: 1.98621809\n",
      "iteration: 147 loss: 1.17824316\n",
      "iteration: 148 loss: 1.54319310\n",
      "iteration: 149 loss: 1.88033056\n",
      "iteration: 150 loss: 1.76719606\n",
      "iteration: 151 loss: 1.41092312\n",
      "iteration: 152 loss: 3.42812705\n",
      "iteration: 153 loss: 3.02080560\n",
      "iteration: 154 loss: 1.57848978\n",
      "iteration: 155 loss: 2.91107130\n",
      "iteration: 156 loss: 0.59236473\n",
      "iteration: 157 loss: 0.85260713\n",
      "iteration: 158 loss: 1.68224585\n",
      "iteration: 159 loss: 0.43912271\n",
      "iteration: 160 loss: 0.88499480\n",
      "iteration: 161 loss: 1.41925299\n",
      "iteration: 162 loss: 0.56238002\n",
      "iteration: 163 loss: 1.68003285\n",
      "iteration: 164 loss: 3.45344138\n",
      "iteration: 165 loss: 2.13504553\n",
      "iteration: 166 loss: 0.60716283\n",
      "iteration: 167 loss: 1.36859941\n",
      "iteration: 168 loss: 1.16902673\n",
      "iteration: 169 loss: 1.75795376\n",
      "iteration: 170 loss: 3.43434691\n",
      "iteration: 171 loss: 1.87523162\n",
      "iteration: 172 loss: 0.86334115\n",
      "iteration: 173 loss: 0.79001278\n",
      "iteration: 174 loss: 2.66425633\n",
      "iteration: 175 loss: 2.48326969\n",
      "iteration: 176 loss: 2.31883168\n",
      "iteration: 177 loss: 0.52800447\n",
      "iteration: 178 loss: 1.33027196\n",
      "iteration: 179 loss: 0.33122900\n",
      "iteration: 180 loss: 1.97053194\n",
      "iteration: 181 loss: 1.21548021\n",
      "iteration: 182 loss: 0.50214624\n",
      "iteration: 183 loss: 1.47286701\n",
      "iteration: 184 loss: 1.30401301\n",
      "iteration: 185 loss: 1.71749485\n",
      "iteration: 186 loss: 1.81999135\n",
      "iteration: 187 loss: 0.59765095\n",
      "iteration: 188 loss: 3.07236600\n",
      "iteration: 189 loss: 2.10425425\n",
      "iteration: 190 loss: 1.77425003\n",
      "iteration: 191 loss: 1.07908976\n",
      "iteration: 192 loss: 1.05976593\n",
      "iteration: 193 loss: 1.51734638\n",
      "iteration: 194 loss: 0.69985926\n",
      "iteration: 195 loss: 3.05059528\n",
      "iteration: 196 loss: 3.33433771\n",
      "iteration: 197 loss: 0.81998414\n",
      "iteration: 198 loss: 1.05390453\n",
      "iteration: 199 loss: 0.81179923\n",
      "epoch:  41 mean loss training: 1.49470687\n",
      "epoch:  41 mean loss validation: 1.41867602\n",
      "iteration:   0 loss: 2.63373685\n",
      "iteration:   1 loss: 1.34250033\n",
      "iteration:   2 loss: 1.27467811\n",
      "iteration:   3 loss: 0.71947592\n",
      "iteration:   4 loss: 3.28143859\n",
      "iteration:   5 loss: 2.75213671\n",
      "iteration:   6 loss: 1.56612992\n",
      "iteration:   7 loss: 1.39653301\n",
      "iteration:   8 loss: 3.00381684\n",
      "iteration:   9 loss: 3.60401011\n",
      "iteration:  10 loss: 0.80973440\n",
      "iteration:  11 loss: 0.53423518\n",
      "iteration:  12 loss: 1.82319486\n",
      "iteration:  13 loss: 0.81492245\n",
      "iteration:  14 loss: 0.99548817\n",
      "iteration:  15 loss: 0.66432649\n",
      "iteration:  16 loss: 1.08678424\n",
      "iteration:  17 loss: 0.86088157\n",
      "iteration:  18 loss: 1.61343777\n",
      "iteration:  19 loss: 1.51614285\n",
      "iteration:  20 loss: 0.69269907\n",
      "iteration:  21 loss: 1.21240938\n",
      "iteration:  22 loss: 0.29828408\n",
      "iteration:  23 loss: 1.15817487\n",
      "iteration:  24 loss: 0.68171316\n",
      "iteration:  25 loss: 2.59649014\n",
      "iteration:  26 loss: 1.36677098\n",
      "iteration:  27 loss: 2.69936633\n",
      "iteration:  28 loss: 0.56427324\n",
      "iteration:  29 loss: 1.17755544\n",
      "iteration:  30 loss: 0.62179756\n",
      "iteration:  31 loss: 0.41910818\n",
      "iteration:  32 loss: 0.39314356\n",
      "iteration:  33 loss: 0.45790732\n",
      "iteration:  34 loss: 1.26051319\n",
      "iteration:  35 loss: 1.29564548\n",
      "iteration:  36 loss: 0.19116767\n",
      "iteration:  37 loss: 0.45288622\n",
      "iteration:  38 loss: 1.55603039\n",
      "iteration:  39 loss: 3.29293346\n",
      "iteration:  40 loss: 2.00692058\n",
      "iteration:  41 loss: 2.21350408\n",
      "iteration:  42 loss: 1.53188336\n",
      "iteration:  43 loss: 2.23312187\n",
      "iteration:  44 loss: 0.80095088\n",
      "iteration:  45 loss: 0.29893547\n",
      "iteration:  46 loss: 1.60888326\n",
      "iteration:  47 loss: 1.26286912\n",
      "iteration:  48 loss: 1.24354076\n",
      "iteration:  49 loss: 1.03142273\n",
      "iteration:  50 loss: 1.04371202\n",
      "iteration:  51 loss: 0.64857936\n",
      "iteration:  52 loss: 1.77510953\n",
      "iteration:  53 loss: 0.87715763\n",
      "iteration:  54 loss: 0.79991555\n",
      "iteration:  55 loss: 2.12281084\n",
      "iteration:  56 loss: 1.14468014\n",
      "iteration:  57 loss: 1.43128681\n",
      "iteration:  58 loss: 2.36362743\n",
      "iteration:  59 loss: 1.85620058\n",
      "iteration:  60 loss: 0.61913484\n",
      "iteration:  61 loss: 0.59844798\n",
      "iteration:  62 loss: 0.94066846\n",
      "iteration:  63 loss: 1.06402624\n",
      "iteration:  64 loss: 0.78633708\n",
      "iteration:  65 loss: 1.48385775\n",
      "iteration:  66 loss: 1.46035123\n",
      "iteration:  67 loss: 1.14171016\n",
      "iteration:  68 loss: 1.44002581\n",
      "iteration:  69 loss: 1.42126250\n",
      "iteration:  70 loss: 0.75892627\n",
      "iteration:  71 loss: 0.82959539\n",
      "iteration:  72 loss: 1.40591884\n",
      "iteration:  73 loss: 2.72945952\n",
      "iteration:  74 loss: 0.92308879\n",
      "iteration:  75 loss: 0.54287559\n",
      "iteration:  76 loss: 0.88400757\n",
      "iteration:  77 loss: 0.99240845\n",
      "iteration:  78 loss: 1.59591448\n",
      "iteration:  79 loss: 0.57328635\n",
      "iteration:  80 loss: 1.88165617\n",
      "iteration:  81 loss: 0.21232200\n",
      "iteration:  82 loss: 0.57426482\n",
      "iteration:  83 loss: 1.31423926\n",
      "iteration:  84 loss: 3.29405522\n",
      "iteration:  85 loss: 0.21514060\n",
      "iteration:  86 loss: 0.53185695\n",
      "iteration:  87 loss: 3.16296077\n",
      "iteration:  88 loss: 0.25356585\n",
      "iteration:  89 loss: 3.16802073\n",
      "iteration:  90 loss: 2.08113241\n",
      "iteration:  91 loss: 0.61565608\n",
      "iteration:  92 loss: 0.69887739\n",
      "iteration:  93 loss: 2.73775506\n",
      "iteration:  94 loss: 2.00778556\n",
      "iteration:  95 loss: 0.47507355\n",
      "iteration:  96 loss: 3.73064351\n",
      "iteration:  97 loss: 1.10883319\n",
      "iteration:  98 loss: 0.47385946\n",
      "iteration:  99 loss: 0.29250982\n",
      "iteration: 100 loss: 0.50125194\n",
      "iteration: 101 loss: 0.30979195\n",
      "iteration: 102 loss: 0.97871584\n",
      "iteration: 103 loss: 3.23229051\n",
      "iteration: 104 loss: 0.31822315\n",
      "iteration: 105 loss: 0.83594167\n",
      "iteration: 106 loss: 0.36497813\n",
      "iteration: 107 loss: 0.96183610\n",
      "iteration: 108 loss: 1.66404402\n",
      "iteration: 109 loss: 0.56203932\n",
      "iteration: 110 loss: 2.82759905\n",
      "iteration: 111 loss: 3.11516333\n",
      "iteration: 112 loss: 2.97488689\n",
      "iteration: 113 loss: 0.80759966\n",
      "iteration: 114 loss: 1.33537376\n",
      "iteration: 115 loss: 4.05339384\n",
      "iteration: 116 loss: 1.90204704\n",
      "iteration: 117 loss: 0.38171816\n",
      "iteration: 118 loss: 0.51091868\n",
      "iteration: 119 loss: 1.61098135\n",
      "iteration: 120 loss: 0.33411998\n",
      "iteration: 121 loss: 0.62211108\n",
      "iteration: 122 loss: 0.47437155\n",
      "iteration: 123 loss: 2.62359238\n",
      "iteration: 124 loss: 1.56253040\n",
      "iteration: 125 loss: 1.70932460\n",
      "iteration: 126 loss: 3.11230493\n",
      "iteration: 127 loss: 0.73061174\n",
      "iteration: 128 loss: 1.27549636\n",
      "iteration: 129 loss: 0.37453812\n",
      "iteration: 130 loss: 2.82776022\n",
      "iteration: 131 loss: 1.70245790\n",
      "iteration: 132 loss: 1.45279598\n",
      "iteration: 133 loss: 0.58970034\n",
      "iteration: 134 loss: 0.64250672\n",
      "iteration: 135 loss: 1.60803723\n",
      "iteration: 136 loss: 1.23624706\n",
      "iteration: 137 loss: 0.99427140\n",
      "iteration: 138 loss: 1.28683567\n",
      "iteration: 139 loss: 1.07959080\n",
      "iteration: 140 loss: 1.43661809\n",
      "iteration: 141 loss: 1.50370812\n",
      "iteration: 142 loss: 1.19045675\n",
      "iteration: 143 loss: 1.62721193\n",
      "iteration: 144 loss: 3.56438446\n",
      "iteration: 145 loss: 1.48921776\n",
      "iteration: 146 loss: 0.45570335\n",
      "iteration: 147 loss: 1.13655746\n",
      "iteration: 148 loss: 1.13148212\n",
      "iteration: 149 loss: 1.55698299\n",
      "iteration: 150 loss: 2.05274820\n",
      "iteration: 151 loss: 1.33914399\n",
      "iteration: 152 loss: 3.55567074\n",
      "iteration: 153 loss: 3.47921634\n",
      "iteration: 154 loss: 1.65172696\n",
      "iteration: 155 loss: 2.89087796\n",
      "iteration: 156 loss: 0.28005156\n",
      "iteration: 157 loss: 0.88304067\n",
      "iteration: 158 loss: 0.75726891\n",
      "iteration: 159 loss: 0.33683038\n",
      "iteration: 160 loss: 0.79056901\n",
      "iteration: 161 loss: 1.62520432\n",
      "iteration: 162 loss: 0.31895858\n",
      "iteration: 163 loss: 3.28647900\n",
      "iteration: 164 loss: 3.41819096\n",
      "iteration: 165 loss: 2.23215437\n",
      "iteration: 166 loss: 0.54380572\n",
      "iteration: 167 loss: 0.20971000\n",
      "iteration: 168 loss: 0.59811270\n",
      "iteration: 169 loss: 0.79052514\n",
      "iteration: 170 loss: 3.28089905\n",
      "iteration: 171 loss: 1.06378794\n",
      "iteration: 172 loss: 0.59470719\n",
      "iteration: 173 loss: 0.24494077\n",
      "iteration: 174 loss: 3.56284308\n",
      "iteration: 175 loss: 2.34818816\n",
      "iteration: 176 loss: 2.57293534\n",
      "iteration: 177 loss: 0.85919052\n",
      "iteration: 178 loss: 0.40715715\n",
      "iteration: 179 loss: 0.16614339\n",
      "iteration: 180 loss: 0.27689615\n",
      "iteration: 181 loss: 0.92109889\n",
      "iteration: 182 loss: 0.27089593\n",
      "iteration: 183 loss: 1.55770385\n",
      "iteration: 184 loss: 0.61788255\n",
      "iteration: 185 loss: 2.09075141\n",
      "iteration: 186 loss: 1.78288269\n",
      "iteration: 187 loss: 0.77471387\n",
      "iteration: 188 loss: 3.62794638\n",
      "iteration: 189 loss: 2.11777687\n",
      "iteration: 190 loss: 2.28454208\n",
      "iteration: 191 loss: 0.91740632\n",
      "iteration: 192 loss: 1.05771852\n",
      "iteration: 193 loss: 1.59983528\n",
      "iteration: 194 loss: 0.52532125\n",
      "iteration: 195 loss: 3.18479872\n",
      "iteration: 196 loss: 3.20049572\n",
      "iteration: 197 loss: 0.77720791\n",
      "iteration: 198 loss: 0.74290693\n",
      "iteration: 199 loss: 0.73759955\n",
      "epoch:  42 mean loss training: 1.39893448\n",
      "epoch:  42 mean loss validation: 1.47857881\n",
      "iteration:   0 loss: 2.83518505\n",
      "iteration:   1 loss: 2.51885462\n",
      "iteration:   2 loss: 1.28073001\n",
      "iteration:   3 loss: 0.81019622\n",
      "iteration:   4 loss: 2.85555410\n",
      "iteration:   5 loss: 2.15810943\n",
      "iteration:   6 loss: 1.95886135\n",
      "iteration:   7 loss: 1.28132987\n",
      "iteration:   8 loss: 3.59060431\n",
      "iteration:   9 loss: 3.82181811\n",
      "iteration:  10 loss: 0.74228531\n",
      "iteration:  11 loss: 0.41072181\n",
      "iteration:  12 loss: 1.29885638\n",
      "iteration:  13 loss: 1.00453556\n",
      "iteration:  14 loss: 0.93405372\n",
      "iteration:  15 loss: 0.65223813\n",
      "iteration:  16 loss: 0.99944830\n",
      "iteration:  17 loss: 1.01507270\n",
      "iteration:  18 loss: 1.62546909\n",
      "iteration:  19 loss: 0.92963874\n",
      "iteration:  20 loss: 0.87852424\n",
      "iteration:  21 loss: 2.22843766\n",
      "iteration:  22 loss: 0.35597304\n",
      "iteration:  23 loss: 1.07061386\n",
      "iteration:  24 loss: 0.21351770\n",
      "iteration:  25 loss: 2.21285391\n",
      "iteration:  26 loss: 1.47673321\n",
      "iteration:  27 loss: 2.50346446\n",
      "iteration:  28 loss: 0.53998542\n",
      "iteration:  29 loss: 1.03707659\n",
      "iteration:  30 loss: 0.54223919\n",
      "iteration:  31 loss: 0.37072393\n",
      "iteration:  32 loss: 0.44740954\n",
      "iteration:  33 loss: 0.50085932\n",
      "iteration:  34 loss: 2.53159094\n",
      "iteration:  35 loss: 1.23305905\n",
      "iteration:  36 loss: 0.12847821\n",
      "iteration:  37 loss: 0.29536989\n",
      "iteration:  38 loss: 1.19267833\n",
      "iteration:  39 loss: 3.36387229\n",
      "iteration:  40 loss: 2.10038567\n",
      "iteration:  41 loss: 1.78055346\n",
      "iteration:  42 loss: 1.24326146\n",
      "iteration:  43 loss: 3.17699790\n",
      "iteration:  44 loss: 0.85640478\n",
      "iteration:  45 loss: 0.31527296\n",
      "iteration:  46 loss: 1.85832763\n",
      "iteration:  47 loss: 0.92900240\n",
      "iteration:  48 loss: 0.76951587\n",
      "iteration:  49 loss: 0.71240413\n",
      "iteration:  50 loss: 2.53090596\n",
      "iteration:  51 loss: 0.38361493\n",
      "iteration:  52 loss: 1.25637400\n",
      "iteration:  53 loss: 0.41174114\n",
      "iteration:  54 loss: 0.68379599\n",
      "iteration:  55 loss: 3.78202295\n",
      "iteration:  56 loss: 0.96059179\n",
      "iteration:  57 loss: 2.87300801\n",
      "iteration:  58 loss: 3.57896733\n",
      "iteration:  59 loss: 1.19667041\n",
      "iteration:  60 loss: 0.34176332\n",
      "iteration:  61 loss: 0.31509787\n",
      "iteration:  62 loss: 0.42789266\n",
      "iteration:  63 loss: 0.61939210\n",
      "iteration:  64 loss: 0.29091194\n",
      "iteration:  65 loss: 1.72913837\n",
      "iteration:  66 loss: 0.59455413\n",
      "iteration:  67 loss: 0.25564712\n",
      "iteration:  68 loss: 0.20994130\n",
      "iteration:  69 loss: 0.50179136\n",
      "iteration:  70 loss: 0.38104859\n",
      "iteration:  71 loss: 0.58855706\n",
      "iteration:  72 loss: 1.09232414\n",
      "iteration:  73 loss: 1.86193895\n",
      "iteration:  74 loss: 0.97488838\n",
      "iteration:  75 loss: 0.71030390\n",
      "iteration:  76 loss: 0.56863230\n",
      "iteration:  77 loss: 0.65725058\n",
      "iteration:  78 loss: 1.59356260\n",
      "iteration:  79 loss: 0.62160623\n",
      "iteration:  80 loss: 2.02177453\n",
      "iteration:  81 loss: 0.21865448\n",
      "iteration:  82 loss: 0.82459259\n",
      "iteration:  83 loss: 0.84175378\n",
      "iteration:  84 loss: 3.24770975\n",
      "iteration:  85 loss: 0.22143227\n",
      "iteration:  86 loss: 0.65351188\n",
      "iteration:  87 loss: 3.28068781\n",
      "iteration:  88 loss: 0.29149204\n",
      "iteration:  89 loss: 2.77673864\n",
      "iteration:  90 loss: 2.43700528\n",
      "iteration:  91 loss: 0.43071598\n",
      "iteration:  92 loss: 0.80149221\n",
      "iteration:  93 loss: 2.04772782\n",
      "iteration:  94 loss: 0.93454325\n",
      "iteration:  95 loss: 0.80226070\n",
      "iteration:  96 loss: 4.45511103\n",
      "iteration:  97 loss: 0.77950519\n",
      "iteration:  98 loss: 0.52405179\n",
      "iteration:  99 loss: 0.35556379\n",
      "iteration: 100 loss: 1.32521832\n",
      "iteration: 101 loss: 0.49807820\n",
      "iteration: 102 loss: 0.88805783\n",
      "iteration: 103 loss: 2.64947772\n",
      "iteration: 104 loss: 0.44476643\n",
      "iteration: 105 loss: 0.76312739\n",
      "iteration: 106 loss: 0.40607181\n",
      "iteration: 107 loss: 1.20832586\n",
      "iteration: 108 loss: 1.19647074\n",
      "iteration: 109 loss: 0.33619803\n",
      "iteration: 110 loss: 3.08815742\n",
      "iteration: 111 loss: 3.13450146\n",
      "iteration: 112 loss: 3.20091581\n",
      "iteration: 113 loss: 2.01669621\n",
      "iteration: 114 loss: 2.61011410\n",
      "iteration: 115 loss: 4.16920948\n",
      "iteration: 116 loss: 2.12484312\n",
      "iteration: 117 loss: 0.17825305\n",
      "iteration: 118 loss: 0.52377021\n",
      "iteration: 119 loss: 0.78816968\n",
      "iteration: 120 loss: 0.57974178\n",
      "iteration: 121 loss: 0.65324122\n",
      "iteration: 122 loss: 0.30779493\n",
      "iteration: 123 loss: 2.35790610\n",
      "iteration: 124 loss: 1.64820731\n",
      "iteration: 125 loss: 3.29801297\n",
      "iteration: 126 loss: 3.18282938\n",
      "iteration: 127 loss: 0.76483333\n",
      "iteration: 128 loss: 1.65772390\n",
      "iteration: 129 loss: 0.35922411\n",
      "iteration: 130 loss: 2.78854108\n",
      "iteration: 131 loss: 1.88574851\n",
      "iteration: 132 loss: 1.57792616\n",
      "iteration: 133 loss: 0.61657035\n",
      "iteration: 134 loss: 0.60903120\n",
      "iteration: 135 loss: 1.64421403\n",
      "iteration: 136 loss: 1.20624220\n",
      "iteration: 137 loss: 1.39886737\n",
      "iteration: 138 loss: 1.49199533\n",
      "iteration: 139 loss: 1.05323732\n",
      "iteration: 140 loss: 1.70874119\n",
      "iteration: 141 loss: 1.15647709\n",
      "iteration: 142 loss: 1.04838073\n",
      "iteration: 143 loss: 1.54498363\n",
      "iteration: 144 loss: 3.55176926\n",
      "iteration: 145 loss: 1.85874498\n",
      "iteration: 146 loss: 0.66822165\n",
      "iteration: 147 loss: 1.07069612\n",
      "iteration: 148 loss: 1.21203375\n",
      "iteration: 149 loss: 1.87867379\n",
      "iteration: 150 loss: 1.97903299\n",
      "iteration: 151 loss: 1.39756179\n",
      "iteration: 152 loss: 3.40008068\n",
      "iteration: 153 loss: 3.36618876\n",
      "iteration: 154 loss: 1.54393697\n",
      "iteration: 155 loss: 2.90629244\n",
      "iteration: 156 loss: 0.37769401\n",
      "iteration: 157 loss: 0.90795547\n",
      "iteration: 158 loss: 0.68714768\n",
      "iteration: 159 loss: 0.36916012\n",
      "iteration: 160 loss: 0.65399158\n",
      "iteration: 161 loss: 1.51369274\n",
      "iteration: 162 loss: 0.27893516\n",
      "iteration: 163 loss: 3.15603042\n",
      "iteration: 164 loss: 3.49919128\n",
      "iteration: 165 loss: 2.38740373\n",
      "iteration: 166 loss: 0.51087540\n",
      "iteration: 167 loss: 0.88602096\n",
      "iteration: 168 loss: 0.66998291\n",
      "iteration: 169 loss: 1.46603036\n",
      "iteration: 170 loss: 3.58231163\n",
      "iteration: 171 loss: 0.78196603\n",
      "iteration: 172 loss: 0.69674587\n",
      "iteration: 173 loss: 0.28227773\n",
      "iteration: 174 loss: 4.03237104\n",
      "iteration: 175 loss: 2.40922356\n",
      "iteration: 176 loss: 2.33642960\n",
      "iteration: 177 loss: 0.74039429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 178 loss: 0.47637099\n",
      "iteration: 179 loss: 0.24036673\n",
      "iteration: 180 loss: 0.33772835\n",
      "iteration: 181 loss: 1.20124066\n",
      "iteration: 182 loss: 0.43250960\n",
      "iteration: 183 loss: 2.03876710\n",
      "iteration: 184 loss: 0.76904374\n",
      "iteration: 185 loss: 1.98591936\n",
      "iteration: 186 loss: 1.12537789\n",
      "iteration: 187 loss: 1.01979589\n",
      "iteration: 188 loss: 3.17058945\n",
      "iteration: 189 loss: 2.10132098\n",
      "iteration: 190 loss: 2.16577554\n",
      "iteration: 191 loss: 1.11867547\n",
      "iteration: 192 loss: 0.83254468\n",
      "iteration: 193 loss: 1.50267899\n",
      "iteration: 194 loss: 0.62910217\n",
      "iteration: 195 loss: 3.08191276\n",
      "iteration: 196 loss: 3.18162823\n",
      "iteration: 197 loss: 1.14349473\n",
      "iteration: 198 loss: 0.69982487\n",
      "iteration: 199 loss: 0.62959260\n",
      "epoch:  43 mean loss training: 1.41281819\n",
      "epoch:  43 mean loss validation: 1.49452138\n",
      "iteration:   0 loss: 2.86106038\n",
      "iteration:   1 loss: 1.29852676\n",
      "iteration:   2 loss: 1.21943772\n",
      "iteration:   3 loss: 0.86697477\n",
      "iteration:   4 loss: 2.69988275\n",
      "iteration:   5 loss: 2.80580878\n",
      "iteration:   6 loss: 2.29023623\n",
      "iteration:   7 loss: 1.21629906\n",
      "iteration:   8 loss: 2.88848019\n",
      "iteration:   9 loss: 3.95182586\n",
      "iteration:  10 loss: 1.37169278\n",
      "iteration:  11 loss: 0.70113039\n",
      "iteration:  12 loss: 1.39723861\n",
      "iteration:  13 loss: 1.42635608\n",
      "iteration:  14 loss: 1.03213918\n",
      "iteration:  15 loss: 0.84306693\n",
      "iteration:  16 loss: 1.55747437\n",
      "iteration:  17 loss: 1.04146302\n",
      "iteration:  18 loss: 1.57936001\n",
      "iteration:  19 loss: 1.00798869\n",
      "iteration:  20 loss: 1.17393100\n",
      "iteration:  21 loss: 1.74927938\n",
      "iteration:  22 loss: 0.59516007\n",
      "iteration:  23 loss: 1.29134405\n",
      "iteration:  24 loss: 0.49413276\n",
      "iteration:  25 loss: 1.87483060\n",
      "iteration:  26 loss: 1.47736406\n",
      "iteration:  27 loss: 2.44289374\n",
      "iteration:  28 loss: 0.56352341\n",
      "iteration:  29 loss: 0.82412910\n",
      "iteration:  30 loss: 0.55080432\n",
      "iteration:  31 loss: 0.27416521\n",
      "iteration:  32 loss: 0.62363404\n",
      "iteration:  33 loss: 0.80848557\n",
      "iteration:  34 loss: 2.42569828\n",
      "iteration:  35 loss: 1.19569814\n",
      "iteration:  36 loss: 0.12004575\n",
      "iteration:  37 loss: 0.29960513\n",
      "iteration:  38 loss: 1.49971259\n",
      "iteration:  39 loss: 3.33026719\n",
      "iteration:  40 loss: 2.57825661\n",
      "iteration:  41 loss: 1.32883680\n",
      "iteration:  42 loss: 1.45338726\n",
      "iteration:  43 loss: 2.85178828\n",
      "iteration:  44 loss: 0.73704064\n",
      "iteration:  45 loss: 0.21401949\n",
      "iteration:  46 loss: 2.18352270\n",
      "iteration:  47 loss: 1.33103955\n",
      "iteration:  48 loss: 1.29087055\n",
      "iteration:  49 loss: 1.10067427\n",
      "iteration:  50 loss: 1.49294651\n",
      "iteration:  51 loss: 0.62530190\n",
      "iteration:  52 loss: 1.58108199\n",
      "iteration:  53 loss: 0.55752778\n",
      "iteration:  54 loss: 0.63795710\n",
      "iteration:  55 loss: 2.49140787\n",
      "iteration:  56 loss: 1.05899191\n",
      "iteration:  57 loss: 1.37991345\n",
      "iteration:  58 loss: 2.30148458\n",
      "iteration:  59 loss: 1.47042108\n",
      "iteration:  60 loss: 0.41053632\n",
      "iteration:  61 loss: 0.44620097\n",
      "iteration:  62 loss: 0.67022586\n",
      "iteration:  63 loss: 1.00738752\n",
      "iteration:  64 loss: 0.73931694\n",
      "iteration:  65 loss: 3.45952368\n",
      "iteration:  66 loss: 1.48734105\n",
      "iteration:  67 loss: 1.15972602\n",
      "iteration:  68 loss: 1.50177205\n",
      "iteration:  69 loss: 1.24425244\n",
      "iteration:  70 loss: 0.58486372\n",
      "iteration:  71 loss: 0.82771665\n",
      "iteration:  72 loss: 1.19532514\n",
      "iteration:  73 loss: 2.18542624\n",
      "iteration:  74 loss: 1.25900304\n",
      "iteration:  75 loss: 0.29658696\n",
      "iteration:  76 loss: 0.87756938\n",
      "iteration:  77 loss: 0.74019164\n",
      "iteration:  78 loss: 1.41860366\n",
      "iteration:  79 loss: 0.59081233\n",
      "iteration:  80 loss: 2.04114962\n",
      "iteration:  81 loss: 0.18409979\n",
      "iteration:  82 loss: 0.83348989\n",
      "iteration:  83 loss: 1.24280381\n",
      "iteration:  84 loss: 3.44196272\n",
      "iteration:  85 loss: 0.23310046\n",
      "iteration:  86 loss: 0.49894699\n",
      "iteration:  87 loss: 3.35415387\n",
      "iteration:  88 loss: 0.19066715\n",
      "iteration:  89 loss: 2.61137128\n",
      "iteration:  90 loss: 1.53607762\n",
      "iteration:  91 loss: 0.45775634\n",
      "iteration:  92 loss: 0.73962700\n",
      "iteration:  93 loss: 1.67272627\n",
      "iteration:  94 loss: 1.58775866\n",
      "iteration:  95 loss: 0.72382528\n",
      "iteration:  96 loss: 4.27076960\n",
      "iteration:  97 loss: 0.73052084\n",
      "iteration:  98 loss: 0.56228930\n",
      "iteration:  99 loss: 0.28274518\n",
      "iteration: 100 loss: 0.55025601\n",
      "iteration: 101 loss: 0.30017582\n",
      "iteration: 102 loss: 0.92289805\n",
      "iteration: 103 loss: 2.62771225\n",
      "iteration: 104 loss: 0.34717625\n",
      "iteration: 105 loss: 0.77782536\n",
      "iteration: 106 loss: 0.33756176\n",
      "iteration: 107 loss: 1.29569805\n",
      "iteration: 108 loss: 1.41392016\n",
      "iteration: 109 loss: 0.49952075\n",
      "iteration: 110 loss: 3.11683202\n",
      "iteration: 111 loss: 3.13099146\n",
      "iteration: 112 loss: 3.25669980\n",
      "iteration: 113 loss: 1.99824321\n",
      "iteration: 114 loss: 2.59129524\n",
      "iteration: 115 loss: 4.07402897\n",
      "iteration: 116 loss: 2.23233676\n",
      "iteration: 117 loss: 0.18349786\n",
      "iteration: 118 loss: 0.52478266\n",
      "iteration: 119 loss: 0.81129068\n",
      "iteration: 120 loss: 0.31695801\n",
      "iteration: 121 loss: 0.68922073\n",
      "iteration: 122 loss: 0.31354389\n",
      "iteration: 123 loss: 2.45968342\n",
      "iteration: 124 loss: 1.27325428\n",
      "iteration: 125 loss: 2.61398792\n",
      "iteration: 126 loss: 3.02360725\n",
      "iteration: 127 loss: 0.65846705\n",
      "iteration: 128 loss: 1.77586985\n",
      "iteration: 129 loss: 0.36610809\n",
      "iteration: 130 loss: 2.84787273\n",
      "iteration: 131 loss: 1.88143492\n",
      "iteration: 132 loss: 1.38155639\n",
      "iteration: 133 loss: 0.59926140\n",
      "iteration: 134 loss: 0.60083699\n",
      "iteration: 135 loss: 1.74825251\n",
      "iteration: 136 loss: 1.22556281\n",
      "iteration: 137 loss: 1.48178315\n",
      "iteration: 138 loss: 1.68030262\n",
      "iteration: 139 loss: 1.11757886\n",
      "iteration: 140 loss: 1.66357529\n",
      "iteration: 141 loss: 1.41908026\n",
      "iteration: 142 loss: 1.09622085\n",
      "iteration: 143 loss: 0.80626988\n",
      "iteration: 144 loss: 3.57724380\n",
      "iteration: 145 loss: 1.84519935\n",
      "iteration: 146 loss: 0.66241223\n",
      "iteration: 147 loss: 1.02468014\n",
      "iteration: 148 loss: 1.30357230\n",
      "iteration: 149 loss: 1.65892291\n",
      "iteration: 150 loss: 1.92172050\n",
      "iteration: 151 loss: 1.38468421\n",
      "iteration: 152 loss: 3.35456729\n",
      "iteration: 153 loss: 3.37261558\n",
      "iteration: 154 loss: 1.62534153\n",
      "iteration: 155 loss: 2.90782952\n",
      "iteration: 156 loss: 0.41401809\n",
      "iteration: 157 loss: 0.90896493\n",
      "iteration: 158 loss: 0.84203357\n",
      "iteration: 159 loss: 0.45795959\n",
      "iteration: 160 loss: 0.65268409\n",
      "iteration: 161 loss: 1.59142256\n",
      "iteration: 162 loss: 0.30932349\n",
      "iteration: 163 loss: 3.03555608\n",
      "iteration: 164 loss: 3.47365832\n",
      "iteration: 165 loss: 1.40015423\n",
      "iteration: 166 loss: 0.55056614\n",
      "iteration: 167 loss: 0.31817210\n",
      "iteration: 168 loss: 0.71432018\n",
      "iteration: 169 loss: 1.77596736\n",
      "iteration: 170 loss: 3.28163886\n",
      "iteration: 171 loss: 0.69527996\n",
      "iteration: 172 loss: 0.54146034\n",
      "iteration: 173 loss: 0.24203146\n",
      "iteration: 174 loss: 3.56779909\n",
      "iteration: 175 loss: 2.65355778\n",
      "iteration: 176 loss: 1.78972495\n",
      "iteration: 177 loss: 0.69079596\n",
      "iteration: 178 loss: 0.35062325\n",
      "iteration: 179 loss: 0.14882717\n",
      "iteration: 180 loss: 0.33572507\n",
      "iteration: 181 loss: 0.89905280\n",
      "iteration: 182 loss: 0.30465081\n",
      "iteration: 183 loss: 1.79013848\n",
      "iteration: 184 loss: 0.95463014\n",
      "iteration: 185 loss: 2.36540103\n",
      "iteration: 186 loss: 1.77344131\n",
      "iteration: 187 loss: 0.83832026\n",
      "iteration: 188 loss: 3.88777637\n",
      "iteration: 189 loss: 2.06526661\n",
      "iteration: 190 loss: 2.94387698\n",
      "iteration: 191 loss: 1.13400888\n",
      "iteration: 192 loss: 0.50962788\n",
      "iteration: 193 loss: 1.93958318\n",
      "iteration: 194 loss: 0.57508409\n",
      "iteration: 195 loss: 3.00581384\n",
      "iteration: 196 loss: 3.06586599\n",
      "iteration: 197 loss: 1.18993878\n",
      "iteration: 198 loss: 0.56682169\n",
      "iteration: 199 loss: 0.47490820\n",
      "epoch:  44 mean loss training: 1.42667067\n",
      "epoch:  44 mean loss validation: 1.43541908\n",
      "iteration:   0 loss: 2.67782974\n",
      "iteration:   1 loss: 1.29136741\n",
      "iteration:   2 loss: 1.04595816\n",
      "iteration:   3 loss: 0.78190321\n",
      "iteration:   4 loss: 2.59534097\n",
      "iteration:   5 loss: 2.88969922\n",
      "iteration:   6 loss: 2.35285139\n",
      "iteration:   7 loss: 1.47582328\n",
      "iteration:   8 loss: 2.77784038\n",
      "iteration:   9 loss: 3.70119500\n",
      "iteration:  10 loss: 1.05128515\n",
      "iteration:  11 loss: 0.69462585\n",
      "iteration:  12 loss: 1.74529696\n",
      "iteration:  13 loss: 1.54352736\n",
      "iteration:  14 loss: 1.05769038\n",
      "iteration:  15 loss: 0.81196022\n",
      "iteration:  16 loss: 1.18480170\n",
      "iteration:  17 loss: 1.25149214\n",
      "iteration:  18 loss: 1.76063657\n",
      "iteration:  19 loss: 0.99909198\n",
      "iteration:  20 loss: 1.15574229\n",
      "iteration:  21 loss: 2.16416454\n",
      "iteration:  22 loss: 0.81537271\n",
      "iteration:  23 loss: 1.38372159\n",
      "iteration:  24 loss: 0.54732692\n",
      "iteration:  25 loss: 2.05319524\n",
      "iteration:  26 loss: 1.86513710\n",
      "iteration:  27 loss: 2.43518567\n",
      "iteration:  28 loss: 0.56302226\n",
      "iteration:  29 loss: 0.92021483\n",
      "iteration:  30 loss: 0.56097704\n",
      "iteration:  31 loss: 0.23304147\n",
      "iteration:  32 loss: 0.58279401\n",
      "iteration:  33 loss: 0.73651785\n",
      "iteration:  34 loss: 2.54562044\n",
      "iteration:  35 loss: 1.16820824\n",
      "iteration:  36 loss: 0.14202885\n",
      "iteration:  37 loss: 0.30185953\n",
      "iteration:  38 loss: 1.18474197\n",
      "iteration:  39 loss: 3.34805751\n",
      "iteration:  40 loss: 2.25728846\n",
      "iteration:  41 loss: 1.00107396\n",
      "iteration:  42 loss: 1.17087710\n",
      "iteration:  43 loss: 3.27640390\n",
      "iteration:  44 loss: 0.68908358\n",
      "iteration:  45 loss: 0.29162729\n",
      "iteration:  46 loss: 2.08105159\n",
      "iteration:  47 loss: 0.91706634\n",
      "iteration:  48 loss: 0.75309837\n",
      "iteration:  49 loss: 0.65265441\n",
      "iteration:  50 loss: 2.95409536\n",
      "iteration:  51 loss: 0.35889599\n",
      "iteration:  52 loss: 1.26472926\n",
      "iteration:  53 loss: 0.24280012\n",
      "iteration:  54 loss: 0.63482225\n",
      "iteration:  55 loss: 3.60768580\n",
      "iteration:  56 loss: 0.88926786\n",
      "iteration:  57 loss: 2.88495588\n",
      "iteration:  58 loss: 3.68475986\n",
      "iteration:  59 loss: 0.93821019\n",
      "iteration:  60 loss: 0.19539188\n",
      "iteration:  61 loss: 0.22241552\n",
      "iteration:  62 loss: 0.26705086\n",
      "iteration:  63 loss: 0.55339032\n",
      "iteration:  64 loss: 0.29997689\n",
      "iteration:  65 loss: 2.69863272\n",
      "iteration:  66 loss: 0.52486438\n",
      "iteration:  67 loss: 0.16318463\n",
      "iteration:  68 loss: 0.32578337\n",
      "iteration:  69 loss: 0.27442393\n",
      "iteration:  70 loss: 0.26639861\n",
      "iteration:  71 loss: 0.49107262\n",
      "iteration:  72 loss: 1.10399163\n",
      "iteration:  73 loss: 2.32740045\n",
      "iteration:  74 loss: 0.54407865\n",
      "iteration:  75 loss: 0.27306709\n",
      "iteration:  76 loss: 0.54780215\n",
      "iteration:  77 loss: 0.77447021\n",
      "iteration:  78 loss: 1.47881603\n",
      "iteration:  79 loss: 0.58457041\n",
      "iteration:  80 loss: 2.04871392\n",
      "iteration:  81 loss: 0.19638588\n",
      "iteration:  82 loss: 0.67505753\n",
      "iteration:  83 loss: 1.06817746\n",
      "iteration:  84 loss: 3.18823624\n",
      "iteration:  85 loss: 0.20116888\n",
      "iteration:  86 loss: 0.51004624\n",
      "iteration:  87 loss: 3.22617221\n",
      "iteration:  88 loss: 0.25430194\n",
      "iteration:  89 loss: 2.51582861\n",
      "iteration:  90 loss: 2.72167873\n",
      "iteration:  91 loss: 0.41785070\n",
      "iteration:  92 loss: 0.73832464\n",
      "iteration:  93 loss: 1.89348936\n",
      "iteration:  94 loss: 1.42920613\n",
      "iteration:  95 loss: 0.73470700\n",
      "iteration:  96 loss: 4.22976112\n",
      "iteration:  97 loss: 0.94215077\n",
      "iteration:  98 loss: 0.39793107\n",
      "iteration:  99 loss: 0.24271655\n",
      "iteration: 100 loss: 1.37574327\n",
      "iteration: 101 loss: 0.44451329\n",
      "iteration: 102 loss: 0.88135475\n",
      "iteration: 103 loss: 2.68349123\n",
      "iteration: 104 loss: 0.32758057\n",
      "iteration: 105 loss: 0.93970680\n",
      "iteration: 106 loss: 0.40822795\n",
      "iteration: 107 loss: 1.28363669\n",
      "iteration: 108 loss: 1.18427563\n",
      "iteration: 109 loss: 0.35593802\n",
      "iteration: 110 loss: 3.09992671\n",
      "iteration: 111 loss: 3.11024499\n",
      "iteration: 112 loss: 3.20013380\n",
      "iteration: 113 loss: 2.02918863\n",
      "iteration: 114 loss: 2.56031418\n",
      "iteration: 115 loss: 3.93146658\n",
      "iteration: 116 loss: 2.18570852\n",
      "iteration: 117 loss: 0.15593536\n",
      "iteration: 118 loss: 0.43860424\n",
      "iteration: 119 loss: 0.75527745\n",
      "iteration: 120 loss: 0.56369090\n",
      "iteration: 121 loss: 0.65419960\n",
      "iteration: 122 loss: 0.27278924\n",
      "iteration: 123 loss: 2.34740424\n",
      "iteration: 124 loss: 1.69196713\n",
      "iteration: 125 loss: 3.15499830\n",
      "iteration: 126 loss: 3.14785099\n",
      "iteration: 127 loss: 0.70056105\n",
      "iteration: 128 loss: 1.82750678\n",
      "iteration: 129 loss: 0.24767840\n",
      "iteration: 130 loss: 2.74633026\n",
      "iteration: 131 loss: 1.84637415\n",
      "iteration: 132 loss: 1.32263410\n",
      "iteration: 133 loss: 0.58114743\n",
      "iteration: 134 loss: 0.55477589\n",
      "iteration: 135 loss: 1.11830103\n",
      "iteration: 136 loss: 1.22896194\n",
      "iteration: 137 loss: 1.37879908\n",
      "iteration: 138 loss: 1.67484713\n",
      "iteration: 139 loss: 1.16447198\n",
      "iteration: 140 loss: 1.67165637\n",
      "iteration: 141 loss: 1.35159063\n",
      "iteration: 142 loss: 1.16756690\n",
      "iteration: 143 loss: 0.88584459\n",
      "iteration: 144 loss: 3.89295363\n",
      "iteration: 145 loss: 1.79411650\n",
      "iteration: 146 loss: 0.71628809\n",
      "iteration: 147 loss: 1.11811984\n",
      "iteration: 148 loss: 1.24856436\n",
      "iteration: 149 loss: 1.71133053\n",
      "iteration: 150 loss: 2.08746266\n",
      "iteration: 151 loss: 1.26452267\n",
      "iteration: 152 loss: 3.42967010\n",
      "iteration: 153 loss: 3.23711729\n",
      "iteration: 154 loss: 1.46798944\n",
      "iteration: 155 loss: 2.76003432\n",
      "iteration: 156 loss: 0.36474919\n",
      "iteration: 157 loss: 0.92544204\n",
      "iteration: 158 loss: 0.95153934\n",
      "iteration: 159 loss: 0.45538136\n",
      "iteration: 160 loss: 0.83879077\n",
      "iteration: 161 loss: 1.49543595\n",
      "iteration: 162 loss: 0.33769277\n",
      "iteration: 163 loss: 3.10185266\n",
      "iteration: 164 loss: 3.20347810\n",
      "iteration: 165 loss: 1.54538417\n",
      "iteration: 166 loss: 0.56885326\n",
      "iteration: 167 loss: 0.20025878\n",
      "iteration: 168 loss: 0.81942511\n",
      "iteration: 169 loss: 1.71150601\n",
      "iteration: 170 loss: 3.34735465\n",
      "iteration: 171 loss: 0.82184470\n",
      "iteration: 172 loss: 0.72918296\n",
      "iteration: 173 loss: 0.28703180\n",
      "iteration: 174 loss: 3.58727694\n",
      "iteration: 175 loss: 1.50274575\n",
      "iteration: 176 loss: 1.82102585\n",
      "iteration: 177 loss: 0.83178258\n",
      "iteration: 178 loss: 0.52101302\n",
      "iteration: 179 loss: 0.16524212\n",
      "iteration: 180 loss: 0.94502276\n",
      "iteration: 181 loss: 0.95436263\n",
      "iteration: 182 loss: 0.48610088\n",
      "iteration: 183 loss: 0.83902830\n",
      "iteration: 184 loss: 0.79338020\n",
      "iteration: 185 loss: 2.20071197\n",
      "iteration: 186 loss: 1.39138150\n",
      "iteration: 187 loss: 1.04153168\n",
      "iteration: 188 loss: 3.85617304\n",
      "iteration: 189 loss: 2.06202793\n",
      "iteration: 190 loss: 2.97317195\n",
      "iteration: 191 loss: 1.04088998\n",
      "iteration: 192 loss: 0.85819840\n",
      "iteration: 193 loss: 1.68966436\n",
      "iteration: 194 loss: 0.69945532\n",
      "iteration: 195 loss: 3.21407247\n",
      "iteration: 196 loss: 3.21525002\n",
      "iteration: 197 loss: 0.77614570\n",
      "iteration: 198 loss: 0.64286667\n",
      "iteration: 199 loss: 0.83288616\n",
      "epoch:  45 mean loss training: 1.40262127\n",
      "epoch:  45 mean loss validation: 1.51213610\n",
      "iteration:   0 loss: 2.95930529\n",
      "iteration:   1 loss: 2.58332753\n",
      "iteration:   2 loss: 1.41380608\n",
      "iteration:   3 loss: 0.90258360\n",
      "iteration:   4 loss: 2.87719393\n",
      "iteration:   5 loss: 2.06589150\n",
      "iteration:   6 loss: 2.20019197\n",
      "iteration:   7 loss: 1.39327025\n",
      "iteration:   8 loss: 3.43740320\n",
      "iteration:   9 loss: 3.56201077\n",
      "iteration:  10 loss: 0.93254942\n",
      "iteration:  11 loss: 0.46222812\n",
      "iteration:  12 loss: 1.18763947\n",
      "iteration:  13 loss: 1.04869676\n",
      "iteration:  14 loss: 0.91453236\n",
      "iteration:  15 loss: 0.66072834\n",
      "iteration:  16 loss: 1.19289589\n",
      "iteration:  17 loss: 1.03007960\n",
      "iteration:  18 loss: 1.75487471\n",
      "iteration:  19 loss: 0.79540408\n",
      "iteration:  20 loss: 0.96668679\n",
      "iteration:  21 loss: 1.33326292\n",
      "iteration:  22 loss: 0.50418228\n",
      "iteration:  23 loss: 1.14888394\n",
      "iteration:  24 loss: 0.23248748\n",
      "iteration:  25 loss: 1.45505166\n",
      "iteration:  26 loss: 1.36626410\n",
      "iteration:  27 loss: 2.11950588\n",
      "iteration:  28 loss: 0.48279268\n",
      "iteration:  29 loss: 0.88543648\n",
      "iteration:  30 loss: 0.51587492\n",
      "iteration:  31 loss: 0.33152291\n",
      "iteration:  32 loss: 0.67334366\n",
      "iteration:  33 loss: 0.61422455\n",
      "iteration:  34 loss: 2.43846035\n",
      "iteration:  35 loss: 1.21820557\n",
      "iteration:  36 loss: 0.12090517\n",
      "iteration:  37 loss: 0.30137619\n",
      "iteration:  38 loss: 1.16250503\n",
      "iteration:  39 loss: 3.35723567\n",
      "iteration:  40 loss: 2.10805917\n",
      "iteration:  41 loss: 1.23256445\n",
      "iteration:  42 loss: 1.26359439\n",
      "iteration:  43 loss: 3.50532150\n",
      "iteration:  44 loss: 1.03495276\n",
      "iteration:  45 loss: 0.29553315\n",
      "iteration:  46 loss: 1.97789180\n",
      "iteration:  47 loss: 1.03533065\n",
      "iteration:  48 loss: 0.78720814\n",
      "iteration:  49 loss: 0.66632515\n",
      "iteration:  50 loss: 2.68134427\n",
      "iteration:  51 loss: 0.40628499\n",
      "iteration:  52 loss: 1.35650516\n",
      "iteration:  53 loss: 0.45489645\n",
      "iteration:  54 loss: 0.63628799\n",
      "iteration:  55 loss: 3.66328859\n",
      "iteration:  56 loss: 1.07935870\n",
      "iteration:  57 loss: 2.86311364\n",
      "iteration:  58 loss: 3.69838309\n",
      "iteration:  59 loss: 1.19843996\n",
      "iteration:  60 loss: 0.25412309\n",
      "iteration:  61 loss: 0.29307088\n",
      "iteration:  62 loss: 0.30678797\n",
      "iteration:  63 loss: 0.63921016\n",
      "iteration:  64 loss: 0.25260660\n",
      "iteration:  65 loss: 2.38386941\n",
      "iteration:  66 loss: 0.52500165\n",
      "iteration:  67 loss: 0.25466979\n",
      "iteration:  68 loss: 0.33729774\n",
      "iteration:  69 loss: 0.47242871\n",
      "iteration:  70 loss: 0.42477599\n",
      "iteration:  71 loss: 0.76293129\n",
      "iteration:  72 loss: 1.00745964\n",
      "iteration:  73 loss: 1.90474331\n",
      "iteration:  74 loss: 0.97988743\n",
      "iteration:  75 loss: 0.60600418\n",
      "iteration:  76 loss: 0.59058577\n",
      "iteration:  77 loss: 0.74223012\n",
      "iteration:  78 loss: 1.53919268\n",
      "iteration:  79 loss: 0.64003295\n",
      "iteration:  80 loss: 2.10622859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  81 loss: 0.22892228\n",
      "iteration:  82 loss: 0.91893917\n",
      "iteration:  83 loss: 0.81032497\n",
      "iteration:  84 loss: 3.18883872\n",
      "iteration:  85 loss: 0.24843103\n",
      "iteration:  86 loss: 0.67488176\n",
      "iteration:  87 loss: 3.23201942\n",
      "iteration:  88 loss: 0.28236678\n",
      "iteration:  89 loss: 2.87359381\n",
      "iteration:  90 loss: 2.39945364\n",
      "iteration:  91 loss: 0.42462867\n",
      "iteration:  92 loss: 0.90647119\n",
      "iteration:  93 loss: 1.95150697\n",
      "iteration:  94 loss: 0.82131302\n",
      "iteration:  95 loss: 0.81057268\n",
      "iteration:  96 loss: 4.50840950\n",
      "iteration:  97 loss: 0.84111321\n",
      "iteration:  98 loss: 0.46226582\n",
      "iteration:  99 loss: 0.35826778\n",
      "iteration: 100 loss: 1.67593503\n",
      "iteration: 101 loss: 0.55265445\n",
      "iteration: 102 loss: 0.89010620\n",
      "iteration: 103 loss: 2.78766751\n",
      "iteration: 104 loss: 0.41907015\n",
      "iteration: 105 loss: 0.62375224\n",
      "iteration: 106 loss: 0.32345143\n",
      "iteration: 107 loss: 1.09735858\n",
      "iteration: 108 loss: 1.06157684\n",
      "iteration: 109 loss: 0.26124558\n",
      "iteration: 110 loss: 3.07659817\n",
      "iteration: 111 loss: 3.09327626\n",
      "iteration: 112 loss: 3.09332919\n",
      "iteration: 113 loss: 1.92476606\n",
      "iteration: 114 loss: 2.55714989\n",
      "iteration: 115 loss: 3.64034247\n",
      "iteration: 116 loss: 2.09177995\n",
      "iteration: 117 loss: 0.15963863\n",
      "iteration: 118 loss: 0.42744088\n",
      "iteration: 119 loss: 0.73419803\n",
      "iteration: 120 loss: 0.35459122\n",
      "iteration: 121 loss: 0.64752489\n",
      "iteration: 122 loss: 0.23547943\n",
      "iteration: 123 loss: 2.34534669\n",
      "iteration: 124 loss: 1.92657602\n",
      "iteration: 125 loss: 2.98915911\n",
      "iteration: 126 loss: 3.32682538\n",
      "iteration: 127 loss: 0.82238585\n",
      "iteration: 128 loss: 2.36641121\n",
      "iteration: 129 loss: 0.24190122\n",
      "iteration: 130 loss: 2.62842226\n",
      "iteration: 131 loss: 2.22270727\n",
      "iteration: 132 loss: 2.49561739\n",
      "iteration: 133 loss: 0.58068341\n",
      "iteration: 134 loss: 0.60505664\n",
      "iteration: 135 loss: 1.15380263\n",
      "iteration: 136 loss: 0.90333503\n",
      "iteration: 137 loss: 1.20942235\n",
      "iteration: 138 loss: 0.87743235\n",
      "iteration: 139 loss: 0.96698314\n",
      "iteration: 140 loss: 1.85416055\n",
      "iteration: 141 loss: 0.85069972\n",
      "iteration: 142 loss: 0.97774404\n",
      "iteration: 143 loss: 0.70505631\n",
      "iteration: 144 loss: 3.79412937\n",
      "iteration: 145 loss: 1.92444992\n",
      "iteration: 146 loss: 0.47610343\n",
      "iteration: 147 loss: 1.05513752\n",
      "iteration: 148 loss: 1.06244349\n",
      "iteration: 149 loss: 1.83420396\n",
      "iteration: 150 loss: 2.06235719\n",
      "iteration: 151 loss: 1.23689985\n",
      "iteration: 152 loss: 3.36717820\n",
      "iteration: 153 loss: 3.35562825\n",
      "iteration: 154 loss: 1.41161990\n",
      "iteration: 155 loss: 2.64276624\n",
      "iteration: 156 loss: 0.27097914\n",
      "iteration: 157 loss: 0.91076261\n",
      "iteration: 158 loss: 0.78157294\n",
      "iteration: 159 loss: 0.36172286\n",
      "iteration: 160 loss: 0.89123952\n",
      "iteration: 161 loss: 1.46720684\n",
      "iteration: 162 loss: 0.34045556\n",
      "iteration: 163 loss: 2.64596510\n",
      "iteration: 164 loss: 3.18550229\n",
      "iteration: 165 loss: 2.26471686\n",
      "iteration: 166 loss: 0.52323848\n",
      "iteration: 167 loss: 0.19134741\n",
      "iteration: 168 loss: 0.66743374\n",
      "iteration: 169 loss: 0.88434541\n",
      "iteration: 170 loss: 3.18873596\n",
      "iteration: 171 loss: 1.12419391\n",
      "iteration: 172 loss: 0.50962460\n",
      "iteration: 173 loss: 0.28813151\n",
      "iteration: 174 loss: 3.57123208\n",
      "iteration: 175 loss: 2.73223925\n",
      "iteration: 176 loss: 2.52414894\n",
      "iteration: 177 loss: 0.71479619\n",
      "iteration: 178 loss: 0.36528495\n",
      "iteration: 179 loss: 0.16236381\n",
      "iteration: 180 loss: 0.40970102\n",
      "iteration: 181 loss: 1.03127491\n",
      "iteration: 182 loss: 0.28881374\n",
      "iteration: 183 loss: 2.24855494\n",
      "iteration: 184 loss: 0.65809709\n",
      "iteration: 185 loss: 1.78985322\n",
      "iteration: 186 loss: 1.88194716\n",
      "iteration: 187 loss: 0.72301936\n",
      "iteration: 188 loss: 3.41936374\n",
      "iteration: 189 loss: 2.03338122\n",
      "iteration: 190 loss: 2.47626710\n",
      "iteration: 191 loss: 0.96278656\n",
      "iteration: 192 loss: 0.83456194\n",
      "iteration: 193 loss: 1.85057855\n",
      "iteration: 194 loss: 0.69605762\n",
      "iteration: 195 loss: 3.01393056\n",
      "iteration: 196 loss: 3.08848262\n",
      "iteration: 197 loss: 0.99757534\n",
      "iteration: 198 loss: 0.59470266\n",
      "iteration: 199 loss: 0.56916064\n",
      "epoch:  46 mean loss training: 1.38824916\n",
      "epoch:  46 mean loss validation: 1.41080225\n",
      "iteration:   0 loss: 2.74162674\n",
      "iteration:   1 loss: 1.31871092\n",
      "iteration:   2 loss: 1.11305475\n",
      "iteration:   3 loss: 0.72361231\n",
      "iteration:   4 loss: 2.77580380\n",
      "iteration:   5 loss: 2.75011516\n",
      "iteration:   6 loss: 2.12192655\n",
      "iteration:   7 loss: 1.48316872\n",
      "iteration:   8 loss: 2.80704355\n",
      "iteration:   9 loss: 3.63711429\n",
      "iteration:  10 loss: 1.12307894\n",
      "iteration:  11 loss: 0.63505960\n",
      "iteration:  12 loss: 1.61081445\n",
      "iteration:  13 loss: 1.47679543\n",
      "iteration:  14 loss: 1.06316316\n",
      "iteration:  15 loss: 0.76451164\n",
      "iteration:  16 loss: 1.45119143\n",
      "iteration:  17 loss: 1.23066783\n",
      "iteration:  18 loss: 1.64231181\n",
      "iteration:  19 loss: 0.92790443\n",
      "iteration:  20 loss: 1.09080386\n",
      "iteration:  21 loss: 1.51481652\n",
      "iteration:  22 loss: 0.57811862\n",
      "iteration:  23 loss: 1.21537232\n",
      "iteration:  24 loss: 0.44488788\n",
      "iteration:  25 loss: 2.56055045\n",
      "iteration:  26 loss: 1.72536135\n",
      "iteration:  27 loss: 2.84957838\n",
      "iteration:  28 loss: 0.44983891\n",
      "iteration:  29 loss: 1.03709209\n",
      "iteration:  30 loss: 0.54143482\n",
      "iteration:  31 loss: 0.28723523\n",
      "iteration:  32 loss: 0.32080984\n",
      "iteration:  33 loss: 0.53261822\n",
      "iteration:  34 loss: 2.42843008\n",
      "iteration:  35 loss: 1.19033730\n",
      "iteration:  36 loss: 0.25293142\n",
      "iteration:  37 loss: 0.29091060\n",
      "iteration:  38 loss: 1.16687572\n",
      "iteration:  39 loss: 3.33452368\n",
      "iteration:  40 loss: 2.40347099\n",
      "iteration:  41 loss: 0.98979938\n",
      "iteration:  42 loss: 1.16866302\n",
      "iteration:  43 loss: 3.28404045\n",
      "iteration:  44 loss: 0.41441870\n",
      "iteration:  45 loss: 0.27888387\n",
      "iteration:  46 loss: 2.15907907\n",
      "iteration:  47 loss: 0.81475139\n",
      "iteration:  48 loss: 0.74013901\n",
      "iteration:  49 loss: 0.62529105\n",
      "iteration:  50 loss: 2.78708053\n",
      "iteration:  51 loss: 0.32065731\n",
      "iteration:  52 loss: 1.20713246\n",
      "iteration:  53 loss: 0.24553744\n",
      "iteration:  54 loss: 0.61266768\n",
      "iteration:  55 loss: 3.58776832\n",
      "iteration:  56 loss: 0.80283237\n",
      "iteration:  57 loss: 2.84485269\n",
      "iteration:  58 loss: 3.65514565\n",
      "iteration:  59 loss: 0.94631839\n",
      "iteration:  60 loss: 0.19073816\n",
      "iteration:  61 loss: 0.19542116\n",
      "iteration:  62 loss: 0.27502871\n",
      "iteration:  63 loss: 0.46342376\n",
      "iteration:  64 loss: 0.24651819\n",
      "iteration:  65 loss: 2.69479132\n",
      "iteration:  66 loss: 0.51463270\n",
      "iteration:  67 loss: 0.15850775\n",
      "iteration:  68 loss: 0.25834796\n",
      "iteration:  69 loss: 0.27416471\n",
      "iteration:  70 loss: 0.18531933\n",
      "iteration:  71 loss: 0.56700087\n",
      "iteration:  72 loss: 1.10477245\n",
      "iteration:  73 loss: 2.25194454\n",
      "iteration:  74 loss: 0.57629633\n",
      "iteration:  75 loss: 0.26067403\n",
      "iteration:  76 loss: 0.54413372\n",
      "iteration:  77 loss: 0.62346047\n",
      "iteration:  78 loss: 1.44387388\n",
      "iteration:  79 loss: 0.58169192\n",
      "iteration:  80 loss: 2.05628657\n",
      "iteration:  81 loss: 0.16687708\n",
      "iteration:  82 loss: 0.58434707\n",
      "iteration:  83 loss: 1.29399240\n",
      "iteration:  84 loss: 3.17299628\n",
      "iteration:  85 loss: 0.19767027\n",
      "iteration:  86 loss: 0.50622654\n",
      "iteration:  87 loss: 3.22010636\n",
      "iteration:  88 loss: 0.24339516\n",
      "iteration:  89 loss: 2.57717323\n",
      "iteration:  90 loss: 2.69440007\n",
      "iteration:  91 loss: 0.29656509\n",
      "iteration:  92 loss: 0.51994163\n",
      "iteration:  93 loss: 1.90376043\n",
      "iteration:  94 loss: 1.16521645\n",
      "iteration:  95 loss: 0.72392726\n",
      "iteration:  96 loss: 4.20402384\n",
      "iteration:  97 loss: 0.94262874\n",
      "iteration:  98 loss: 0.35965458\n",
      "iteration:  99 loss: 0.20941737\n",
      "iteration: 100 loss: 1.17765093\n",
      "iteration: 101 loss: 0.48293915\n",
      "iteration: 102 loss: 0.84538966\n",
      "iteration: 103 loss: 2.76171565\n",
      "iteration: 104 loss: 0.26853764\n",
      "iteration: 105 loss: 0.61909550\n",
      "iteration: 106 loss: 0.24188532\n",
      "iteration: 107 loss: 1.25027049\n",
      "iteration: 108 loss: 1.00032020\n",
      "iteration: 109 loss: 0.22633038\n",
      "iteration: 110 loss: 3.07681847\n",
      "iteration: 111 loss: 3.09275579\n",
      "iteration: 112 loss: 3.10857844\n",
      "iteration: 113 loss: 1.87882841\n",
      "iteration: 114 loss: 2.50884604\n",
      "iteration: 115 loss: 3.57577872\n",
      "iteration: 116 loss: 2.22870541\n",
      "iteration: 117 loss: 0.16969503\n",
      "iteration: 118 loss: 0.56297922\n",
      "iteration: 119 loss: 0.63453966\n",
      "iteration: 120 loss: 0.31568679\n",
      "iteration: 121 loss: 0.64138901\n",
      "iteration: 122 loss: 0.26348436\n",
      "iteration: 123 loss: 2.46888638\n",
      "iteration: 124 loss: 1.38898301\n",
      "iteration: 125 loss: 2.37083721\n",
      "iteration: 126 loss: 3.23858118\n",
      "iteration: 127 loss: 0.62262362\n",
      "iteration: 128 loss: 1.44769049\n",
      "iteration: 129 loss: 0.36633119\n",
      "iteration: 130 loss: 2.81945157\n",
      "iteration: 131 loss: 1.97610414\n",
      "iteration: 132 loss: 1.22632313\n",
      "iteration: 133 loss: 0.56234050\n",
      "iteration: 134 loss: 0.58332837\n",
      "iteration: 135 loss: 2.10562634\n",
      "iteration: 136 loss: 1.20970368\n",
      "iteration: 137 loss: 1.35421968\n",
      "iteration: 138 loss: 1.49871325\n",
      "iteration: 139 loss: 1.15280569\n",
      "iteration: 140 loss: 1.53425574\n",
      "iteration: 141 loss: 1.30696332\n",
      "iteration: 142 loss: 0.98305559\n",
      "iteration: 143 loss: 0.74533427\n",
      "iteration: 144 loss: 3.53887486\n",
      "iteration: 145 loss: 1.87280476\n",
      "iteration: 146 loss: 0.60985488\n",
      "iteration: 147 loss: 1.03678632\n",
      "iteration: 148 loss: 1.10719156\n",
      "iteration: 149 loss: 1.43612432\n",
      "iteration: 150 loss: 1.91274726\n",
      "iteration: 151 loss: 1.41386414\n",
      "iteration: 152 loss: 3.32959080\n",
      "iteration: 153 loss: 3.36924052\n",
      "iteration: 154 loss: 1.58302116\n",
      "iteration: 155 loss: 2.89690590\n",
      "iteration: 156 loss: 0.24905413\n",
      "iteration: 157 loss: 0.86096525\n",
      "iteration: 158 loss: 0.78155637\n",
      "iteration: 159 loss: 0.34683597\n",
      "iteration: 160 loss: 0.72677958\n",
      "iteration: 161 loss: 1.19815671\n",
      "iteration: 162 loss: 0.32711178\n",
      "iteration: 163 loss: 3.17682052\n",
      "iteration: 164 loss: 3.46628857\n",
      "iteration: 165 loss: 1.60484600\n",
      "iteration: 166 loss: 0.57761091\n",
      "iteration: 167 loss: 0.19168775\n",
      "iteration: 168 loss: 0.80692935\n",
      "iteration: 169 loss: 1.31581199\n",
      "iteration: 170 loss: 3.36778212\n",
      "iteration: 171 loss: 0.55742890\n",
      "iteration: 172 loss: 0.57385904\n",
      "iteration: 173 loss: 0.31952921\n",
      "iteration: 174 loss: 3.86518979\n",
      "iteration: 175 loss: 2.35913229\n",
      "iteration: 176 loss: 1.90381813\n",
      "iteration: 177 loss: 0.55740166\n",
      "iteration: 178 loss: 0.41369653\n",
      "iteration: 179 loss: 0.17632368\n",
      "iteration: 180 loss: 0.32469973\n",
      "iteration: 181 loss: 0.91718966\n",
      "iteration: 182 loss: 0.38952440\n",
      "iteration: 183 loss: 2.62454772\n",
      "iteration: 184 loss: 0.82772279\n",
      "iteration: 185 loss: 2.39138031\n",
      "iteration: 186 loss: 1.24069953\n",
      "iteration: 187 loss: 0.85657102\n",
      "iteration: 188 loss: 3.48997903\n",
      "iteration: 189 loss: 2.13063669\n",
      "iteration: 190 loss: 2.26001072\n",
      "iteration: 191 loss: 0.92946142\n",
      "iteration: 192 loss: 0.78776336\n",
      "iteration: 193 loss: 1.72289288\n",
      "iteration: 194 loss: 0.66854769\n",
      "iteration: 195 loss: 3.05205560\n",
      "iteration: 196 loss: 3.20591354\n",
      "iteration: 197 loss: 0.93672901\n",
      "iteration: 198 loss: 0.71420628\n",
      "iteration: 199 loss: 0.56362736\n",
      "epoch:  47 mean loss training: 1.36654127\n",
      "epoch:  47 mean loss validation: 1.48446655\n",
      "iteration:   0 loss: 3.00397587\n",
      "iteration:   1 loss: 1.36818719\n",
      "iteration:   2 loss: 1.26906633\n",
      "iteration:   3 loss: 0.85199296\n",
      "iteration:   4 loss: 2.52473235\n",
      "iteration:   5 loss: 2.59952211\n",
      "iteration:   6 loss: 2.12775183\n",
      "iteration:   7 loss: 1.21911514\n",
      "iteration:   8 loss: 2.85746694\n",
      "iteration:   9 loss: 3.70308018\n",
      "iteration:  10 loss: 1.25928140\n",
      "iteration:  11 loss: 0.92291391\n",
      "iteration:  12 loss: 1.49777865\n",
      "iteration:  13 loss: 1.77296209\n",
      "iteration:  14 loss: 1.06206799\n",
      "iteration:  15 loss: 0.88886136\n",
      "iteration:  16 loss: 1.28753352\n",
      "iteration:  17 loss: 1.21384692\n",
      "iteration:  18 loss: 1.75724161\n",
      "iteration:  19 loss: 0.71729773\n",
      "iteration:  20 loss: 1.25758243\n",
      "iteration:  21 loss: 1.13595366\n",
      "iteration:  22 loss: 0.70249462\n",
      "iteration:  23 loss: 1.18517601\n",
      "iteration:  24 loss: 0.45618254\n",
      "iteration:  25 loss: 2.15463686\n",
      "iteration:  26 loss: 1.61033738\n",
      "iteration:  27 loss: 2.08384824\n",
      "iteration:  28 loss: 0.39698875\n",
      "iteration:  29 loss: 0.94144177\n",
      "iteration:  30 loss: 0.51696974\n",
      "iteration:  31 loss: 0.26254222\n",
      "iteration:  32 loss: 0.48365760\n",
      "iteration:  33 loss: 0.65052009\n",
      "iteration:  34 loss: 2.41889548\n",
      "iteration:  35 loss: 1.13298070\n",
      "iteration:  36 loss: 0.13085453\n",
      "iteration:  37 loss: 0.29037252\n",
      "iteration:  38 loss: 1.17220592\n",
      "iteration:  39 loss: 3.34870648\n",
      "iteration:  40 loss: 2.36713982\n",
      "iteration:  41 loss: 1.21076667\n",
      "iteration:  42 loss: 1.13097537\n",
      "iteration:  43 loss: 3.46139789\n",
      "iteration:  44 loss: 0.43493941\n",
      "iteration:  45 loss: 0.29980072\n",
      "iteration:  46 loss: 2.02091265\n",
      "iteration:  47 loss: 0.85050446\n",
      "iteration:  48 loss: 0.73769474\n",
      "iteration:  49 loss: 0.59358579\n",
      "iteration:  50 loss: 2.75965691\n",
      "iteration:  51 loss: 0.34286696\n",
      "iteration:  52 loss: 1.07889485\n",
      "iteration:  53 loss: 0.49888101\n",
      "iteration:  54 loss: 0.67070055\n",
      "iteration:  55 loss: 3.71168184\n",
      "iteration:  56 loss: 0.90965128\n",
      "iteration:  57 loss: 2.85959983\n",
      "iteration:  58 loss: 3.63782382\n",
      "iteration:  59 loss: 1.24675083\n",
      "iteration:  60 loss: 0.20034167\n",
      "iteration:  61 loss: 0.20589504\n",
      "iteration:  62 loss: 0.31809050\n",
      "iteration:  63 loss: 0.51922166\n",
      "iteration:  64 loss: 0.15031144\n",
      "iteration:  65 loss: 2.75011086\n",
      "iteration:  66 loss: 0.51653463\n",
      "iteration:  67 loss: 0.17584963\n",
      "iteration:  68 loss: 0.31922257\n",
      "iteration:  69 loss: 0.29736534\n",
      "iteration:  70 loss: 0.28880978\n",
      "iteration:  71 loss: 0.54348069\n",
      "iteration:  72 loss: 0.99277025\n",
      "iteration:  73 loss: 2.25061131\n",
      "iteration:  74 loss: 0.64745456\n",
      "iteration:  75 loss: 0.40033805\n",
      "iteration:  76 loss: 0.56414193\n",
      "iteration:  77 loss: 0.76515090\n",
      "iteration:  78 loss: 1.45671713\n",
      "iteration:  79 loss: 0.60097784\n",
      "iteration:  80 loss: 1.93628728\n",
      "iteration:  81 loss: 0.21604423\n",
      "iteration:  82 loss: 0.99971753\n",
      "iteration:  83 loss: 0.31744778\n",
      "iteration:  84 loss: 3.06858683\n",
      "iteration:  85 loss: 0.18428984\n",
      "iteration:  86 loss: 0.55902123\n",
      "iteration:  87 loss: 3.22124004\n",
      "iteration:  88 loss: 0.32904124\n",
      "iteration:  89 loss: 3.37574530\n",
      "iteration:  90 loss: 2.58987880\n",
      "iteration:  91 loss: 0.32437676\n",
      "iteration:  92 loss: 0.55166751\n",
      "iteration:  93 loss: 1.97481823\n",
      "iteration:  94 loss: 1.63280976\n",
      "iteration:  95 loss: 0.78767163\n",
      "iteration:  96 loss: 4.40653706\n",
      "iteration:  97 loss: 1.08337224\n",
      "iteration:  98 loss: 0.48891592\n",
      "iteration:  99 loss: 0.35798949\n",
      "iteration: 100 loss: 1.19951260\n",
      "iteration: 101 loss: 0.42570555\n",
      "iteration: 102 loss: 0.93126869\n",
      "iteration: 103 loss: 2.89655733\n",
      "iteration: 104 loss: 0.41254187\n",
      "iteration: 105 loss: 0.68413168\n",
      "iteration: 106 loss: 0.38863638\n",
      "iteration: 107 loss: 0.97813880\n",
      "iteration: 108 loss: 0.92950630\n",
      "iteration: 109 loss: 0.21647102\n",
      "iteration: 110 loss: 3.04014778\n",
      "iteration: 111 loss: 3.09344935\n",
      "iteration: 112 loss: 3.01894164\n",
      "iteration: 113 loss: 1.98853171\n",
      "iteration: 114 loss: 2.59360123\n",
      "iteration: 115 loss: 3.84537530\n",
      "iteration: 116 loss: 1.89665508\n",
      "iteration: 117 loss: 0.16342241\n",
      "iteration: 118 loss: 0.46945962\n",
      "iteration: 119 loss: 1.34267735\n",
      "iteration: 120 loss: 0.30475959\n",
      "iteration: 121 loss: 0.61006320\n",
      "iteration: 122 loss: 0.30252856\n",
      "iteration: 123 loss: 2.31889200\n",
      "iteration: 124 loss: 1.58160424\n",
      "iteration: 125 loss: 2.39153504\n",
      "iteration: 126 loss: 2.91346288\n",
      "iteration: 127 loss: 0.48934436\n",
      "iteration: 128 loss: 1.56996477\n",
      "iteration: 129 loss: 0.25164926\n",
      "iteration: 130 loss: 2.83608365\n",
      "iteration: 131 loss: 1.77803755\n",
      "iteration: 132 loss: 1.21741247\n",
      "iteration: 133 loss: 0.55248260\n",
      "iteration: 134 loss: 0.56959689\n",
      "iteration: 135 loss: 1.48413765\n",
      "iteration: 136 loss: 1.08011401\n",
      "iteration: 137 loss: 1.39868832\n",
      "iteration: 138 loss: 1.59560609\n",
      "iteration: 139 loss: 1.31539261\n",
      "iteration: 140 loss: 1.66516244\n",
      "iteration: 141 loss: 1.37405157\n",
      "iteration: 142 loss: 1.23929906\n",
      "iteration: 143 loss: 0.80177373\n",
      "iteration: 144 loss: 3.86796212\n",
      "iteration: 145 loss: 2.24602103\n",
      "iteration: 146 loss: 0.68550032\n",
      "iteration: 147 loss: 0.99008155\n",
      "iteration: 148 loss: 1.23611748\n",
      "iteration: 149 loss: 1.35194945\n",
      "iteration: 150 loss: 2.01943588\n",
      "iteration: 151 loss: 1.60938311\n",
      "iteration: 152 loss: 3.18078232\n",
      "iteration: 153 loss: 3.45450902\n",
      "iteration: 154 loss: 1.15391910\n",
      "iteration: 155 loss: 2.57665539\n",
      "iteration: 156 loss: 0.28037643\n",
      "iteration: 157 loss: 0.88783956\n",
      "iteration: 158 loss: 1.22011256\n",
      "iteration: 159 loss: 0.31547272\n",
      "iteration: 160 loss: 0.71182036\n",
      "iteration: 161 loss: 1.40424001\n",
      "iteration: 162 loss: 0.33781487\n",
      "iteration: 163 loss: 2.99581552\n",
      "iteration: 164 loss: 2.99928761\n",
      "iteration: 165 loss: 1.06552017\n",
      "iteration: 166 loss: 0.36438695\n",
      "iteration: 167 loss: 0.20579699\n",
      "iteration: 168 loss: 0.67504740\n",
      "iteration: 169 loss: 1.51784754\n",
      "iteration: 170 loss: 3.54262638\n",
      "iteration: 171 loss: 0.83274502\n",
      "iteration: 172 loss: 0.71818048\n",
      "iteration: 173 loss: 0.25696090\n",
      "iteration: 174 loss: 3.66499710\n",
      "iteration: 175 loss: 2.44426751\n",
      "iteration: 176 loss: 2.42513609\n",
      "iteration: 177 loss: 0.33013949\n",
      "iteration: 178 loss: 0.54828089\n",
      "iteration: 179 loss: 0.15020047\n",
      "iteration: 180 loss: 0.23280387\n",
      "iteration: 181 loss: 0.88975310\n",
      "iteration: 182 loss: 0.50561416\n",
      "iteration: 183 loss: 1.48378980\n",
      "iteration: 184 loss: 0.76578814\n",
      "iteration: 185 loss: 2.12786841\n",
      "iteration: 186 loss: 0.87201560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 187 loss: 1.06194413\n",
      "iteration: 188 loss: 3.42652106\n",
      "iteration: 189 loss: 2.14910412\n",
      "iteration: 190 loss: 1.64773059\n",
      "iteration: 191 loss: 0.85423160\n",
      "iteration: 192 loss: 1.09827864\n",
      "iteration: 193 loss: 0.97500652\n",
      "iteration: 194 loss: 0.79940665\n",
      "iteration: 195 loss: 2.98483324\n",
      "iteration: 196 loss: 3.39220572\n",
      "iteration: 197 loss: 0.66826564\n",
      "iteration: 198 loss: 1.16535234\n",
      "iteration: 199 loss: 0.97482121\n",
      "epoch:  48 mean loss training: 1.36964524\n",
      "epoch:  48 mean loss validation: 1.47637820\n",
      "iteration:   0 loss: 2.88948655\n",
      "iteration:   1 loss: 2.52587175\n",
      "iteration:   2 loss: 1.20358694\n",
      "iteration:   3 loss: 0.68414831\n",
      "iteration:   4 loss: 3.80942059\n",
      "iteration:   5 loss: 2.80130267\n",
      "iteration:   6 loss: 1.88717508\n",
      "iteration:   7 loss: 1.33699679\n",
      "iteration:   8 loss: 3.85721588\n",
      "iteration:   9 loss: 3.49132228\n",
      "iteration:  10 loss: 0.50970507\n",
      "iteration:  11 loss: 0.94269890\n",
      "iteration:  12 loss: 1.61591768\n",
      "iteration:  13 loss: 0.95013118\n",
      "iteration:  14 loss: 0.95575398\n",
      "iteration:  15 loss: 0.57522875\n",
      "iteration:  16 loss: 0.80199629\n",
      "iteration:  17 loss: 0.94130200\n",
      "iteration:  18 loss: 1.67406201\n",
      "iteration:  19 loss: 0.91742784\n",
      "iteration:  20 loss: 0.63052458\n",
      "iteration:  21 loss: 1.12365067\n",
      "iteration:  22 loss: 0.59730440\n",
      "iteration:  23 loss: 1.14019835\n",
      "iteration:  24 loss: 0.17114130\n",
      "iteration:  25 loss: 1.39466643\n",
      "iteration:  26 loss: 1.42609024\n",
      "iteration:  27 loss: 2.15304756\n",
      "iteration:  28 loss: 0.36744589\n",
      "iteration:  29 loss: 0.85053217\n",
      "iteration:  30 loss: 0.48665965\n",
      "iteration:  31 loss: 0.32450765\n",
      "iteration:  32 loss: 0.76075304\n",
      "iteration:  33 loss: 0.55601239\n",
      "iteration:  34 loss: 2.39319468\n",
      "iteration:  35 loss: 1.14188385\n",
      "iteration:  36 loss: 0.12572923\n",
      "iteration:  37 loss: 0.28777808\n",
      "iteration:  38 loss: 1.17263365\n",
      "iteration:  39 loss: 3.29498363\n",
      "iteration:  40 loss: 2.32349753\n",
      "iteration:  41 loss: 1.04781103\n",
      "iteration:  42 loss: 1.29265070\n",
      "iteration:  43 loss: 3.11259794\n",
      "iteration:  44 loss: 0.65687621\n",
      "iteration:  45 loss: 0.21085538\n",
      "iteration:  46 loss: 2.06738210\n",
      "iteration:  47 loss: 1.03503799\n",
      "iteration:  48 loss: 0.77904993\n",
      "iteration:  49 loss: 0.61883259\n",
      "iteration:  50 loss: 2.57823420\n",
      "iteration:  51 loss: 0.40011171\n",
      "iteration:  52 loss: 1.14364660\n",
      "iteration:  53 loss: 0.35957402\n",
      "iteration:  54 loss: 0.67210603\n",
      "iteration:  55 loss: 3.69817281\n",
      "iteration:  56 loss: 0.98652619\n",
      "iteration:  57 loss: 2.89942169\n",
      "iteration:  58 loss: 3.46723890\n",
      "iteration:  59 loss: 1.28471303\n",
      "iteration:  60 loss: 0.28397578\n",
      "iteration:  61 loss: 0.26319617\n",
      "iteration:  62 loss: 0.31475815\n",
      "iteration:  63 loss: 0.78780776\n",
      "iteration:  64 loss: 0.16324408\n",
      "iteration:  65 loss: 1.84492016\n",
      "iteration:  66 loss: 0.54537004\n",
      "iteration:  67 loss: 0.21486899\n",
      "iteration:  68 loss: 0.18920222\n",
      "iteration:  69 loss: 0.52617222\n",
      "iteration:  70 loss: 0.72284496\n",
      "iteration:  71 loss: 0.54026860\n",
      "iteration:  72 loss: 0.91274780\n",
      "iteration:  73 loss: 1.73420298\n",
      "iteration:  74 loss: 0.70490271\n",
      "iteration:  75 loss: 0.60243791\n",
      "iteration:  76 loss: 0.60993153\n",
      "iteration:  77 loss: 0.97801197\n",
      "iteration:  78 loss: 1.46643376\n",
      "iteration:  79 loss: 0.63241601\n",
      "iteration:  80 loss: 1.76905310\n",
      "iteration:  81 loss: 0.33437720\n",
      "iteration:  82 loss: 0.89588660\n",
      "iteration:  83 loss: 0.69375581\n",
      "iteration:  84 loss: 3.06192684\n",
      "iteration:  85 loss: 0.19505958\n",
      "iteration:  86 loss: 0.67032605\n",
      "iteration:  87 loss: 3.25566792\n",
      "iteration:  88 loss: 0.39171696\n",
      "iteration:  89 loss: 3.04989910\n",
      "iteration:  90 loss: 2.41544008\n",
      "iteration:  91 loss: 0.35102507\n",
      "iteration:  92 loss: 0.64453804\n",
      "iteration:  93 loss: 1.88335979\n",
      "iteration:  94 loss: 1.33071196\n",
      "iteration:  95 loss: 0.86194980\n",
      "iteration:  96 loss: 4.43467236\n",
      "iteration:  97 loss: 0.98830819\n",
      "iteration:  98 loss: 0.47597605\n",
      "iteration:  99 loss: 0.42177975\n",
      "iteration: 100 loss: 1.04611063\n",
      "iteration: 101 loss: 0.40367192\n",
      "iteration: 102 loss: 0.93703395\n",
      "iteration: 103 loss: 3.02259231\n",
      "iteration: 104 loss: 0.44453609\n",
      "iteration: 105 loss: 0.71419573\n",
      "iteration: 106 loss: 0.36889449\n",
      "iteration: 107 loss: 0.79485178\n",
      "iteration: 108 loss: 0.89820307\n",
      "iteration: 109 loss: 0.21634194\n",
      "iteration: 110 loss: 3.09431005\n",
      "iteration: 111 loss: 3.09093428\n",
      "iteration: 112 loss: 3.01246452\n",
      "iteration: 113 loss: 2.00576043\n",
      "iteration: 114 loss: 2.59068608\n",
      "iteration: 115 loss: 3.34574270\n",
      "iteration: 116 loss: 1.90159404\n",
      "iteration: 117 loss: 0.18347746\n",
      "iteration: 118 loss: 0.57571787\n",
      "iteration: 119 loss: 1.11813998\n",
      "iteration: 120 loss: 0.30122358\n",
      "iteration: 121 loss: 0.60863131\n",
      "iteration: 122 loss: 0.33814397\n",
      "iteration: 123 loss: 2.37346578\n",
      "iteration: 124 loss: 1.56597745\n",
      "iteration: 125 loss: 1.88147521\n",
      "iteration: 126 loss: 3.24722147\n",
      "iteration: 127 loss: 0.54325706\n",
      "iteration: 128 loss: 1.20333219\n",
      "iteration: 129 loss: 0.35565275\n",
      "iteration: 130 loss: 2.72308064\n",
      "iteration: 131 loss: 2.10392642\n",
      "iteration: 132 loss: 1.91907334\n",
      "iteration: 133 loss: 0.56840646\n",
      "iteration: 134 loss: 0.56119990\n",
      "iteration: 135 loss: 2.05937696\n",
      "iteration: 136 loss: 0.96226233\n",
      "iteration: 137 loss: 1.09741604\n",
      "iteration: 138 loss: 1.09131300\n",
      "iteration: 139 loss: 1.01107991\n",
      "iteration: 140 loss: 2.01211023\n",
      "iteration: 141 loss: 0.93035275\n",
      "iteration: 142 loss: 0.93236899\n",
      "iteration: 143 loss: 1.50034463\n",
      "iteration: 144 loss: 3.78552079\n",
      "iteration: 145 loss: 1.92022228\n",
      "iteration: 146 loss: 0.51754218\n",
      "iteration: 147 loss: 1.00119793\n",
      "iteration: 148 loss: 1.24021578\n",
      "iteration: 149 loss: 1.88251126\n",
      "iteration: 150 loss: 2.23577571\n",
      "iteration: 151 loss: 1.54337931\n",
      "iteration: 152 loss: 3.55274725\n",
      "iteration: 153 loss: 3.37144876\n",
      "iteration: 154 loss: 1.14542317\n",
      "iteration: 155 loss: 2.56951690\n",
      "iteration: 156 loss: 0.49988914\n",
      "iteration: 157 loss: 0.89080024\n",
      "iteration: 158 loss: 0.93801385\n",
      "iteration: 159 loss: 0.33303058\n",
      "iteration: 160 loss: 0.63094586\n",
      "iteration: 161 loss: 1.55990434\n",
      "iteration: 162 loss: 0.33570236\n",
      "iteration: 163 loss: 3.25820756\n",
      "iteration: 164 loss: 2.74832177\n",
      "iteration: 165 loss: 2.65615606\n",
      "iteration: 166 loss: 0.37140578\n",
      "iteration: 167 loss: 0.20140836\n",
      "iteration: 168 loss: 0.73387146\n",
      "iteration: 169 loss: 1.05027413\n",
      "iteration: 170 loss: 3.36422086\n",
      "iteration: 171 loss: 0.77638090\n",
      "iteration: 172 loss: 0.58059531\n",
      "iteration: 173 loss: 0.29829371\n",
      "iteration: 174 loss: 3.56106997\n",
      "iteration: 175 loss: 2.12505984\n",
      "iteration: 176 loss: 2.44607711\n",
      "iteration: 177 loss: 0.37385854\n",
      "iteration: 178 loss: 0.41075748\n",
      "iteration: 179 loss: 0.17436004\n",
      "iteration: 180 loss: 0.77610284\n",
      "iteration: 181 loss: 1.19831216\n",
      "iteration: 182 loss: 0.28198776\n",
      "iteration: 183 loss: 0.78088176\n",
      "iteration: 184 loss: 0.65005869\n",
      "iteration: 185 loss: 2.37649202\n",
      "iteration: 186 loss: 1.23349369\n",
      "iteration: 187 loss: 0.82814711\n",
      "iteration: 188 loss: 3.37596607\n",
      "iteration: 189 loss: 2.06367898\n",
      "iteration: 190 loss: 2.33151340\n",
      "iteration: 191 loss: 0.91995662\n",
      "iteration: 192 loss: 0.83670121\n",
      "iteration: 193 loss: 1.79494631\n",
      "iteration: 194 loss: 0.67807770\n",
      "iteration: 195 loss: 2.91554928\n",
      "iteration: 196 loss: 3.03464365\n",
      "iteration: 197 loss: 1.20529473\n",
      "iteration: 198 loss: 0.45830205\n",
      "iteration: 199 loss: 0.45158827\n",
      "epoch:  49 mean loss training: 1.36249363\n",
      "epoch:  49 mean loss validation: 1.38302255\n",
      "iteration:   0 loss: 3.21432424\n",
      "iteration:   1 loss: 1.25812125\n",
      "iteration:   2 loss: 0.95929837\n",
      "iteration:   3 loss: 0.71373838\n",
      "iteration:   4 loss: 2.32133770\n",
      "iteration:   5 loss: 2.53500724\n",
      "iteration:   6 loss: 2.11351371\n",
      "iteration:   7 loss: 1.43282676\n",
      "iteration:   8 loss: 2.78437877\n",
      "iteration:   9 loss: 3.57957745\n",
      "iteration:  10 loss: 0.95336342\n",
      "iteration:  11 loss: 0.51997876\n",
      "iteration:  12 loss: 1.74151146\n",
      "iteration:  13 loss: 1.48630977\n",
      "iteration:  14 loss: 1.06164789\n",
      "iteration:  15 loss: 0.70040619\n",
      "iteration:  16 loss: 1.28298342\n",
      "iteration:  17 loss: 1.22492063\n",
      "iteration:  18 loss: 1.65803671\n",
      "iteration:  19 loss: 0.75663209\n",
      "iteration:  20 loss: 0.88316816\n",
      "iteration:  21 loss: 1.12793612\n",
      "iteration:  22 loss: 0.46900609\n",
      "iteration:  23 loss: 1.05990970\n",
      "iteration:  24 loss: 0.29847389\n",
      "iteration:  25 loss: 2.50671315\n",
      "iteration:  26 loss: 1.51637530\n",
      "iteration:  27 loss: 2.78747439\n",
      "iteration:  28 loss: 0.28216818\n",
      "iteration:  29 loss: 0.82696080\n",
      "iteration:  30 loss: 0.51640046\n",
      "iteration:  31 loss: 0.28577808\n",
      "iteration:  32 loss: 0.26414350\n",
      "iteration:  33 loss: 0.43558055\n",
      "iteration:  34 loss: 2.36709261\n",
      "iteration:  35 loss: 1.04304171\n",
      "iteration:  36 loss: 0.19242103\n",
      "iteration:  37 loss: 0.27386695\n",
      "iteration:  38 loss: 1.16444016\n",
      "iteration:  39 loss: 3.29876423\n",
      "iteration:  40 loss: 2.12438369\n",
      "iteration:  41 loss: 1.18279827\n",
      "iteration:  42 loss: 1.28938425\n",
      "iteration:  43 loss: 3.21716332\n",
      "iteration:  44 loss: 0.84558833\n",
      "iteration:  45 loss: 0.26541439\n",
      "iteration:  46 loss: 2.05008411\n",
      "iteration:  47 loss: 0.94777584\n",
      "iteration:  48 loss: 0.72535855\n",
      "iteration:  49 loss: 0.50758523\n",
      "iteration:  50 loss: 2.68495083\n",
      "iteration:  51 loss: 0.30228925\n",
      "iteration:  52 loss: 1.11593008\n",
      "iteration:  53 loss: 0.24762474\n",
      "iteration:  54 loss: 0.80725145\n",
      "iteration:  55 loss: 3.42950130\n",
      "iteration:  56 loss: 0.82935262\n",
      "iteration:  57 loss: 2.60701346\n",
      "iteration:  58 loss: 3.75322032\n",
      "iteration:  59 loss: 1.52370894\n",
      "iteration:  60 loss: 0.32019964\n",
      "iteration:  61 loss: 0.34368059\n",
      "iteration:  62 loss: 0.70932811\n",
      "iteration:  63 loss: 0.54133987\n",
      "iteration:  64 loss: 0.16456765\n",
      "iteration:  65 loss: 0.67111546\n",
      "iteration:  66 loss: 0.51172161\n",
      "iteration:  67 loss: 0.18084379\n",
      "iteration:  68 loss: 0.16871780\n",
      "iteration:  69 loss: 0.50868517\n",
      "iteration:  70 loss: 0.42210835\n",
      "iteration:  71 loss: 0.75266224\n",
      "iteration:  72 loss: 1.17357540\n",
      "iteration:  73 loss: 2.38283682\n",
      "iteration:  74 loss: 0.76787931\n",
      "iteration:  75 loss: 0.66019726\n",
      "iteration:  76 loss: 0.55641443\n",
      "iteration:  77 loss: 0.69730943\n",
      "iteration:  78 loss: 1.65976167\n",
      "iteration:  79 loss: 0.58831596\n",
      "iteration:  80 loss: 2.06334734\n",
      "iteration:  81 loss: 0.21916051\n",
      "iteration:  82 loss: 0.84824115\n",
      "iteration:  83 loss: 0.49433872\n",
      "iteration:  84 loss: 3.06493592\n",
      "iteration:  85 loss: 0.18148512\n",
      "iteration:  86 loss: 0.65392339\n",
      "iteration:  87 loss: 3.34250164\n",
      "iteration:  88 loss: 0.31100637\n",
      "iteration:  89 loss: 2.69462848\n",
      "iteration:  90 loss: 2.53065133\n",
      "iteration:  91 loss: 0.25720000\n",
      "iteration:  92 loss: 0.60481477\n",
      "iteration:  93 loss: 2.22642207\n",
      "iteration:  94 loss: 1.64274430\n",
      "iteration:  95 loss: 0.73891854\n",
      "iteration:  96 loss: 2.80274129\n",
      "iteration:  97 loss: 0.73735780\n",
      "iteration:  98 loss: 0.50326651\n",
      "iteration:  99 loss: 0.39079168\n",
      "iteration: 100 loss: 0.91895145\n",
      "iteration: 101 loss: 0.38848379\n",
      "iteration: 102 loss: 1.06275773\n",
      "iteration: 103 loss: 2.90826058\n",
      "iteration: 104 loss: 0.41861644\n",
      "iteration: 105 loss: 0.73961043\n",
      "iteration: 106 loss: 0.44359586\n",
      "iteration: 107 loss: 0.92230529\n",
      "iteration: 108 loss: 0.87866956\n",
      "iteration: 109 loss: 0.19140747\n",
      "iteration: 110 loss: 2.84902692\n",
      "iteration: 111 loss: 3.08505082\n",
      "iteration: 112 loss: 2.93051028\n",
      "iteration: 113 loss: 0.82859981\n",
      "iteration: 114 loss: 1.75420451\n",
      "iteration: 115 loss: 3.61290979\n",
      "iteration: 116 loss: 1.99252045\n",
      "iteration: 117 loss: 0.26650581\n",
      "iteration: 118 loss: 0.54999179\n",
      "iteration: 119 loss: 1.32932830\n",
      "iteration: 120 loss: 0.29178357\n",
      "iteration: 121 loss: 0.60289550\n",
      "iteration: 122 loss: 0.29878971\n",
      "iteration: 123 loss: 2.57023025\n",
      "iteration: 124 loss: 1.57218337\n",
      "iteration: 125 loss: 1.89924955\n",
      "iteration: 126 loss: 3.16849542\n",
      "iteration: 127 loss: 0.59034187\n",
      "iteration: 128 loss: 1.30758750\n",
      "iteration: 129 loss: 0.26758766\n",
      "iteration: 130 loss: 2.63558006\n",
      "iteration: 131 loss: 1.82729995\n",
      "iteration: 132 loss: 1.36337173\n",
      "iteration: 133 loss: 0.50784099\n",
      "iteration: 134 loss: 0.54927188\n",
      "iteration: 135 loss: 1.24786067\n",
      "iteration: 136 loss: 1.15429890\n",
      "iteration: 137 loss: 1.10880816\n",
      "iteration: 138 loss: 1.25276184\n",
      "iteration: 139 loss: 1.23584902\n",
      "iteration: 140 loss: 1.26219296\n",
      "iteration: 141 loss: 1.61343861\n",
      "iteration: 142 loss: 1.19469094\n",
      "iteration: 143 loss: 1.80945766\n",
      "iteration: 144 loss: 3.69695449\n",
      "iteration: 145 loss: 1.37944341\n",
      "iteration: 146 loss: 0.40959194\n",
      "iteration: 147 loss: 1.06480622\n",
      "iteration: 148 loss: 0.99593759\n",
      "iteration: 149 loss: 1.52099609\n",
      "iteration: 150 loss: 2.18157768\n",
      "iteration: 151 loss: 1.61121118\n",
      "iteration: 152 loss: 3.31595707\n",
      "iteration: 153 loss: 3.55834532\n",
      "iteration: 154 loss: 1.23970294\n",
      "iteration: 155 loss: 2.52253485\n",
      "iteration: 156 loss: 0.20934261\n",
      "iteration: 157 loss: 0.81844360\n",
      "iteration: 158 loss: 0.86253464\n",
      "iteration: 159 loss: 0.23194747\n",
      "iteration: 160 loss: 0.88701355\n",
      "iteration: 161 loss: 1.36620653\n",
      "iteration: 162 loss: 0.26145479\n",
      "iteration: 163 loss: 3.33590174\n",
      "iteration: 164 loss: 3.26284909\n",
      "iteration: 165 loss: 2.74295568\n",
      "iteration: 166 loss: 0.29190442\n",
      "iteration: 167 loss: 0.19955121\n",
      "iteration: 168 loss: 0.51568037\n",
      "iteration: 169 loss: 0.58833539\n",
      "iteration: 170 loss: 3.12472701\n",
      "iteration: 171 loss: 1.00026083\n",
      "iteration: 172 loss: 0.53233165\n",
      "iteration: 173 loss: 0.29758123\n",
      "iteration: 174 loss: 3.46223164\n",
      "iteration: 175 loss: 2.53262424\n",
      "iteration: 176 loss: 2.64047790\n",
      "iteration: 177 loss: 0.38564098\n",
      "iteration: 178 loss: 0.34618452\n",
      "iteration: 179 loss: 0.16631065\n",
      "iteration: 180 loss: 0.21877424\n",
      "iteration: 181 loss: 0.91330904\n",
      "iteration: 182 loss: 0.24887107\n",
      "iteration: 183 loss: 1.53470051\n",
      "iteration: 184 loss: 0.46420574\n",
      "iteration: 185 loss: 2.23118949\n",
      "iteration: 186 loss: 1.78409028\n",
      "iteration: 187 loss: 0.75590122\n",
      "iteration: 188 loss: 3.33328891\n",
      "iteration: 189 loss: 2.09668970\n",
      "iteration: 190 loss: 1.77638638\n",
      "iteration: 191 loss: 0.71491212\n",
      "iteration: 192 loss: 1.23492801\n",
      "iteration: 193 loss: 1.19388890\n",
      "iteration: 194 loss: 0.71102107\n",
      "iteration: 195 loss: 2.96487617\n",
      "iteration: 196 loss: 3.21798015\n",
      "iteration: 197 loss: 0.72558999\n",
      "iteration: 198 loss: 0.87464631\n",
      "iteration: 199 loss: 0.91783565\n",
      "epoch:  50 mean loss training: 1.32659769\n",
      "epoch:  50 mean loss validation: 1.42808604\n",
      "iteration:   0 loss: 2.81301332\n",
      "iteration:   1 loss: 2.25266194\n",
      "iteration:   2 loss: 1.12283599\n",
      "iteration:   3 loss: 0.65833533\n",
      "iteration:   4 loss: 3.51488781\n",
      "iteration:   5 loss: 2.30125952\n",
      "iteration:   6 loss: 1.64095736\n",
      "iteration:   7 loss: 1.40562141\n",
      "iteration:   8 loss: 3.02909255\n",
      "iteration:   9 loss: 3.45502210\n",
      "iteration:  10 loss: 0.58914149\n",
      "iteration:  11 loss: 0.37030181\n",
      "iteration:  12 loss: 1.37741160\n",
      "iteration:  13 loss: 0.81904685\n",
      "iteration:  14 loss: 1.02776206\n",
      "iteration:  15 loss: 0.57033426\n",
      "iteration:  16 loss: 0.89162141\n",
      "iteration:  17 loss: 1.01063621\n",
      "iteration:  18 loss: 1.67110360\n",
      "iteration:  19 loss: 0.97672832\n",
      "iteration:  20 loss: 0.53560352\n",
      "iteration:  21 loss: 1.03653634\n",
      "iteration:  22 loss: 0.26079702\n",
      "iteration:  23 loss: 0.98244160\n",
      "iteration:  24 loss: 0.21961667\n",
      "iteration:  25 loss: 2.88936806\n",
      "iteration:  26 loss: 1.28356290\n",
      "iteration:  27 loss: 2.95070410\n",
      "iteration:  28 loss: 0.28004557\n",
      "iteration:  29 loss: 0.92016697\n",
      "iteration:  30 loss: 0.58752435\n",
      "iteration:  31 loss: 0.35381946\n",
      "iteration:  32 loss: 0.28976175\n",
      "iteration:  33 loss: 0.50179088\n",
      "iteration:  34 loss: 2.08102179\n",
      "iteration:  35 loss: 1.07508028\n",
      "iteration:  36 loss: 0.12845026\n",
      "iteration:  37 loss: 0.47120357\n",
      "iteration:  38 loss: 1.09548604\n",
      "iteration:  39 loss: 3.32730317\n",
      "iteration:  40 loss: 1.91851246\n",
      "iteration:  41 loss: 1.17473352\n",
      "iteration:  42 loss: 1.10544622\n",
      "iteration:  43 loss: 2.39079547\n",
      "iteration:  44 loss: 0.43893388\n",
      "iteration:  45 loss: 0.26245993\n",
      "iteration:  46 loss: 2.29755402\n",
      "iteration:  47 loss: 1.59177577\n",
      "iteration:  48 loss: 0.96629816\n",
      "iteration:  49 loss: 1.47282255\n",
      "iteration:  50 loss: 2.80695081\n",
      "iteration:  51 loss: 0.32225463\n",
      "iteration:  52 loss: 1.44972801\n",
      "iteration:  53 loss: 0.44205511\n",
      "iteration:  54 loss: 0.96723264\n",
      "iteration:  55 loss: 3.31109214\n",
      "iteration:  56 loss: 1.02909589\n",
      "iteration:  57 loss: 1.51663518\n",
      "iteration:  58 loss: 2.97203183\n",
      "iteration:  59 loss: 1.58054781\n",
      "iteration:  60 loss: 0.61127794\n",
      "iteration:  61 loss: 0.54888684\n",
      "iteration:  62 loss: 1.58477437\n",
      "iteration:  63 loss: 0.74198538\n",
      "iteration:  64 loss: 0.69889224\n",
      "iteration:  65 loss: 1.58313930\n",
      "iteration:  66 loss: 0.99374497\n",
      "iteration:  67 loss: 0.62620133\n",
      "iteration:  68 loss: 1.02949250\n",
      "iteration:  69 loss: 0.62365526\n",
      "iteration:  70 loss: 0.91871995\n",
      "iteration:  71 loss: 1.05331528\n",
      "iteration:  72 loss: 0.85323465\n",
      "iteration:  73 loss: 2.82043362\n",
      "iteration:  74 loss: 0.97362161\n",
      "iteration:  75 loss: 0.88603991\n",
      "iteration:  76 loss: 0.66646850\n",
      "iteration:  77 loss: 0.57003909\n",
      "iteration:  78 loss: 1.55961180\n",
      "iteration:  79 loss: 0.57944340\n",
      "iteration:  80 loss: 1.97020090\n",
      "iteration:  81 loss: 0.26302820\n",
      "iteration:  82 loss: 1.04184163\n",
      "iteration:  83 loss: 0.37857327\n",
      "iteration:  84 loss: 3.25452662\n",
      "iteration:  85 loss: 0.18772510\n",
      "iteration:  86 loss: 0.63154823\n",
      "iteration:  87 loss: 3.13176394\n",
      "iteration:  88 loss: 0.33699328\n",
      "iteration:  89 loss: 2.44146585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  90 loss: 2.02714229\n",
      "iteration:  91 loss: 0.47322556\n",
      "iteration:  92 loss: 0.63625032\n",
      "iteration:  93 loss: 2.01338577\n",
      "iteration:  94 loss: 1.03585112\n",
      "iteration:  95 loss: 0.78035212\n",
      "iteration:  96 loss: 4.25973177\n",
      "iteration:  97 loss: 0.61580586\n",
      "iteration:  98 loss: 0.62217885\n",
      "iteration:  99 loss: 0.27324662\n",
      "iteration: 100 loss: 0.39090765\n",
      "iteration: 101 loss: 0.23089708\n",
      "iteration: 102 loss: 0.96056449\n",
      "iteration: 103 loss: 2.70570588\n",
      "iteration: 104 loss: 0.60920948\n",
      "iteration: 105 loss: 0.82135296\n",
      "iteration: 106 loss: 0.28597015\n",
      "iteration: 107 loss: 0.85661328\n",
      "iteration: 108 loss: 1.96872151\n",
      "iteration: 109 loss: 0.54705966\n",
      "iteration: 110 loss: 2.73424315\n",
      "iteration: 111 loss: 3.08851624\n",
      "iteration: 112 loss: 3.08748746\n",
      "iteration: 113 loss: 1.93363690\n",
      "iteration: 114 loss: 2.52549291\n",
      "iteration: 115 loss: 3.63969159\n",
      "iteration: 116 loss: 2.29815245\n",
      "iteration: 117 loss: 0.15409788\n",
      "iteration: 118 loss: 0.30713427\n",
      "iteration: 119 loss: 0.69740742\n",
      "iteration: 120 loss: 0.44426981\n",
      "iteration: 121 loss: 0.69440007\n",
      "iteration: 122 loss: 0.21196681\n",
      "iteration: 123 loss: 2.34263396\n",
      "iteration: 124 loss: 1.57574892\n",
      "iteration: 125 loss: 2.97855234\n",
      "iteration: 126 loss: 3.14534473\n",
      "iteration: 127 loss: 0.59252053\n",
      "iteration: 128 loss: 1.42777789\n",
      "iteration: 129 loss: 0.36480317\n",
      "iteration: 130 loss: 2.76496363\n",
      "iteration: 131 loss: 2.20303845\n",
      "iteration: 132 loss: 1.45158219\n",
      "iteration: 133 loss: 0.72445899\n",
      "iteration: 134 loss: 0.56632060\n",
      "iteration: 135 loss: 2.14495611\n",
      "iteration: 136 loss: 1.01641262\n",
      "iteration: 137 loss: 1.51601708\n",
      "iteration: 138 loss: 1.33215070\n",
      "iteration: 139 loss: 1.11818588\n",
      "iteration: 140 loss: 1.64311934\n",
      "iteration: 141 loss: 1.26324558\n",
      "iteration: 142 loss: 0.99719977\n",
      "iteration: 143 loss: 0.75223613\n",
      "iteration: 144 loss: 3.54538059\n",
      "iteration: 145 loss: 1.82182610\n",
      "iteration: 146 loss: 0.50266302\n",
      "iteration: 147 loss: 0.97712028\n",
      "iteration: 148 loss: 1.10574257\n",
      "iteration: 149 loss: 1.23864341\n",
      "iteration: 150 loss: 1.88152373\n",
      "iteration: 151 loss: 2.23725390\n",
      "iteration: 152 loss: 3.25153828\n",
      "iteration: 153 loss: 3.45708966\n",
      "iteration: 154 loss: 1.34365654\n",
      "iteration: 155 loss: 2.76709223\n",
      "iteration: 156 loss: 0.19804105\n",
      "iteration: 157 loss: 0.90915042\n",
      "iteration: 158 loss: 0.97950226\n",
      "iteration: 159 loss: 0.34063414\n",
      "iteration: 160 loss: 1.07558525\n",
      "iteration: 161 loss: 1.37463188\n",
      "iteration: 162 loss: 0.31017834\n",
      "iteration: 163 loss: 3.16828346\n",
      "iteration: 164 loss: 3.01663423\n",
      "iteration: 165 loss: 1.13220716\n",
      "iteration: 166 loss: 0.32865697\n",
      "iteration: 167 loss: 0.16850609\n",
      "iteration: 168 loss: 0.73523068\n",
      "iteration: 169 loss: 1.19101787\n",
      "iteration: 170 loss: 3.17699575\n",
      "iteration: 171 loss: 0.68717384\n",
      "iteration: 172 loss: 0.59206778\n",
      "iteration: 173 loss: 0.20961212\n",
      "iteration: 174 loss: 3.48893762\n",
      "iteration: 175 loss: 2.03854942\n",
      "iteration: 176 loss: 1.78396630\n",
      "iteration: 177 loss: 0.37938577\n",
      "iteration: 178 loss: 0.32891175\n",
      "iteration: 179 loss: 0.12933521\n",
      "iteration: 180 loss: 0.22481458\n",
      "iteration: 181 loss: 0.88287091\n",
      "iteration: 182 loss: 0.29317355\n",
      "iteration: 183 loss: 2.53107309\n",
      "iteration: 184 loss: 0.63764197\n",
      "iteration: 185 loss: 2.33203363\n",
      "iteration: 186 loss: 1.43200374\n",
      "iteration: 187 loss: 0.68293762\n",
      "iteration: 188 loss: 3.83800721\n",
      "iteration: 189 loss: 2.11126280\n",
      "iteration: 190 loss: 2.27296615\n",
      "iteration: 191 loss: 0.73440278\n",
      "iteration: 192 loss: 0.99916458\n",
      "iteration: 193 loss: 1.43363285\n",
      "iteration: 194 loss: 0.59604472\n",
      "iteration: 195 loss: 2.97587895\n",
      "iteration: 196 loss: 3.14756179\n",
      "iteration: 197 loss: 0.68285370\n",
      "iteration: 198 loss: 0.67462099\n",
      "iteration: 199 loss: 0.71227866\n",
      "epoch:  51 mean loss training: 1.36632776\n",
      "epoch:  51 mean loss validation: 1.42203426\n",
      "iteration:   0 loss: 3.29870701\n",
      "iteration:   1 loss: 2.46392179\n",
      "iteration:   2 loss: 1.14998758\n",
      "iteration:   3 loss: 0.66248310\n",
      "iteration:   4 loss: 3.33934188\n",
      "iteration:   5 loss: 2.28947663\n",
      "iteration:   6 loss: 1.52729702\n",
      "iteration:   7 loss: 1.50313246\n",
      "iteration:   8 loss: 3.86136460\n",
      "iteration:   9 loss: 3.39823580\n",
      "iteration:  10 loss: 0.40640393\n",
      "iteration:  11 loss: 0.69162595\n",
      "iteration:  12 loss: 1.50735998\n",
      "iteration:  13 loss: 0.78951269\n",
      "iteration:  14 loss: 0.94902027\n",
      "iteration:  15 loss: 0.55077636\n",
      "iteration:  16 loss: 0.76566660\n",
      "iteration:  17 loss: 0.98391080\n",
      "iteration:  18 loss: 1.81752706\n",
      "iteration:  19 loss: 0.82140946\n",
      "iteration:  20 loss: 0.65395451\n",
      "iteration:  21 loss: 0.90695429\n",
      "iteration:  22 loss: 0.29470733\n",
      "iteration:  23 loss: 1.07406795\n",
      "iteration:  24 loss: 0.21074949\n",
      "iteration:  25 loss: 2.58156085\n",
      "iteration:  26 loss: 1.34214759\n",
      "iteration:  27 loss: 2.54925632\n",
      "iteration:  28 loss: 0.32931700\n",
      "iteration:  29 loss: 0.94212282\n",
      "iteration:  30 loss: 0.46367034\n",
      "iteration:  31 loss: 0.31612545\n",
      "iteration:  32 loss: 0.35871926\n",
      "iteration:  33 loss: 0.50430655\n",
      "iteration:  34 loss: 2.43761158\n",
      "iteration:  35 loss: 1.18140852\n",
      "iteration:  36 loss: 0.13821135\n",
      "iteration:  37 loss: 0.68720639\n",
      "iteration:  38 loss: 1.48695767\n",
      "iteration:  39 loss: 2.76602697\n",
      "iteration:  40 loss: 1.92769825\n",
      "iteration:  41 loss: 1.13879251\n",
      "iteration:  42 loss: 1.16333640\n",
      "iteration:  43 loss: 2.15239072\n",
      "iteration:  44 loss: 0.77452755\n",
      "iteration:  45 loss: 0.21973149\n",
      "iteration:  46 loss: 1.90122020\n",
      "iteration:  47 loss: 0.94990975\n",
      "iteration:  48 loss: 0.92574865\n",
      "iteration:  49 loss: 1.04884970\n",
      "iteration:  50 loss: 2.72680116\n",
      "iteration:  51 loss: 0.46554250\n",
      "iteration:  52 loss: 1.25113750\n",
      "iteration:  53 loss: 0.52369243\n",
      "iteration:  54 loss: 0.66810673\n",
      "iteration:  55 loss: 3.75692773\n",
      "iteration:  56 loss: 0.97307432\n",
      "iteration:  57 loss: 2.90730476\n",
      "iteration:  58 loss: 3.16720462\n",
      "iteration:  59 loss: 1.32060182\n",
      "iteration:  60 loss: 0.24923965\n",
      "iteration:  61 loss: 0.27391303\n",
      "iteration:  62 loss: 0.31939027\n",
      "iteration:  63 loss: 0.64480901\n",
      "iteration:  64 loss: 0.12582916\n",
      "iteration:  65 loss: 1.59688902\n",
      "iteration:  66 loss: 0.52032709\n",
      "iteration:  67 loss: 0.19547680\n",
      "iteration:  68 loss: 0.16328841\n",
      "iteration:  69 loss: 0.34328103\n",
      "iteration:  70 loss: 0.30040473\n",
      "iteration:  71 loss: 0.44761989\n",
      "iteration:  72 loss: 1.12862623\n",
      "iteration:  73 loss: 1.55927527\n",
      "iteration:  74 loss: 0.50197369\n",
      "iteration:  75 loss: 0.50826651\n",
      "iteration:  76 loss: 0.59769106\n",
      "iteration:  77 loss: 0.70878822\n",
      "iteration:  78 loss: 1.51202083\n",
      "iteration:  79 loss: 0.57158279\n",
      "iteration:  80 loss: 1.79672575\n",
      "iteration:  81 loss: 0.15944597\n",
      "iteration:  82 loss: 0.68089235\n",
      "iteration:  83 loss: 0.72382766\n",
      "iteration:  84 loss: 3.22601962\n",
      "iteration:  85 loss: 0.17222548\n",
      "iteration:  86 loss: 0.29326144\n",
      "iteration:  87 loss: 3.30835152\n",
      "iteration:  88 loss: 0.24499553\n",
      "iteration:  89 loss: 3.17773199\n",
      "iteration:  90 loss: 2.36429930\n",
      "iteration:  91 loss: 0.28067639\n",
      "iteration:  92 loss: 0.29919070\n",
      "iteration:  93 loss: 2.49820304\n",
      "iteration:  94 loss: 1.51913655\n",
      "iteration:  95 loss: 0.46429944\n",
      "iteration:  96 loss: 4.14434099\n",
      "iteration:  97 loss: 1.26468539\n",
      "iteration:  98 loss: 0.44073692\n",
      "iteration:  99 loss: 0.22514443\n",
      "iteration: 100 loss: 0.70532548\n",
      "iteration: 101 loss: 0.38833410\n",
      "iteration: 102 loss: 2.03155041\n",
      "iteration: 103 loss: 2.70861220\n",
      "iteration: 104 loss: 0.31634313\n",
      "iteration: 105 loss: 0.58342808\n",
      "iteration: 106 loss: 0.25995639\n",
      "iteration: 107 loss: 1.38357508\n",
      "iteration: 108 loss: 0.60427499\n",
      "iteration: 109 loss: 0.23529275\n",
      "iteration: 110 loss: 3.48357964\n",
      "iteration: 111 loss: 2.84023929\n",
      "iteration: 112 loss: 3.23761964\n",
      "iteration: 113 loss: 2.79051065\n",
      "iteration: 114 loss: 2.34444070\n",
      "iteration: 115 loss: 2.77066827\n",
      "iteration: 116 loss: 1.18352234\n",
      "iteration: 117 loss: 0.24604486\n",
      "iteration: 118 loss: 0.65272206\n",
      "iteration: 119 loss: 1.13252449\n",
      "iteration: 120 loss: 0.43625671\n",
      "iteration: 121 loss: 0.74583751\n",
      "iteration: 122 loss: 0.30207047\n",
      "iteration: 123 loss: 2.40943265\n",
      "iteration: 124 loss: 2.15375495\n",
      "iteration: 125 loss: 2.52755523\n",
      "iteration: 126 loss: 3.49982476\n",
      "iteration: 127 loss: 0.67402613\n",
      "iteration: 128 loss: 1.70823026\n",
      "iteration: 129 loss: 0.36023891\n",
      "iteration: 130 loss: 2.47886038\n",
      "iteration: 131 loss: 2.36956930\n",
      "iteration: 132 loss: 2.44549465\n",
      "iteration: 133 loss: 0.56579894\n",
      "iteration: 134 loss: 0.60520852\n",
      "iteration: 135 loss: 2.11930871\n",
      "iteration: 136 loss: 0.85796630\n",
      "iteration: 137 loss: 1.29744947\n",
      "iteration: 138 loss: 0.95295471\n",
      "iteration: 139 loss: 0.93959576\n",
      "iteration: 140 loss: 1.77531695\n",
      "iteration: 141 loss: 0.94179469\n",
      "iteration: 142 loss: 0.82132840\n",
      "iteration: 143 loss: 1.53859818\n",
      "iteration: 144 loss: 3.55672884\n",
      "iteration: 145 loss: 1.63854790\n",
      "iteration: 146 loss: 0.52967983\n",
      "iteration: 147 loss: 0.98210877\n",
      "iteration: 148 loss: 1.06857538\n",
      "iteration: 149 loss: 1.48544109\n",
      "iteration: 150 loss: 1.91411912\n",
      "iteration: 151 loss: 2.74196148\n",
      "iteration: 152 loss: 3.46458483\n",
      "iteration: 153 loss: 3.23849440\n",
      "iteration: 154 loss: 1.40277576\n",
      "iteration: 155 loss: 2.61927700\n",
      "iteration: 156 loss: 0.30160293\n",
      "iteration: 157 loss: 0.93177438\n",
      "iteration: 158 loss: 0.99075848\n",
      "iteration: 159 loss: 0.42313254\n",
      "iteration: 160 loss: 1.17046225\n",
      "iteration: 161 loss: 1.15115964\n",
      "iteration: 162 loss: 0.32914084\n",
      "iteration: 163 loss: 3.45085359\n",
      "iteration: 164 loss: 3.39933300\n",
      "iteration: 165 loss: 2.49765253\n",
      "iteration: 166 loss: 0.32668701\n",
      "iteration: 167 loss: 0.16176817\n",
      "iteration: 168 loss: 0.83768833\n",
      "iteration: 169 loss: 0.78035879\n",
      "iteration: 170 loss: 3.11857271\n",
      "iteration: 171 loss: 0.64315903\n",
      "iteration: 172 loss: 0.61751455\n",
      "iteration: 173 loss: 0.16751544\n",
      "iteration: 174 loss: 3.58877087\n",
      "iteration: 175 loss: 1.74234784\n",
      "iteration: 176 loss: 2.32989311\n",
      "iteration: 177 loss: 0.43461928\n",
      "iteration: 178 loss: 0.34218436\n",
      "iteration: 179 loss: 0.13232830\n",
      "iteration: 180 loss: 0.21753229\n",
      "iteration: 181 loss: 1.00281131\n",
      "iteration: 182 loss: 0.20385620\n",
      "iteration: 183 loss: 2.01792717\n",
      "iteration: 184 loss: 0.65331829\n",
      "iteration: 185 loss: 2.49727201\n",
      "iteration: 186 loss: 1.42072713\n",
      "iteration: 187 loss: 0.67030340\n",
      "iteration: 188 loss: 3.60650206\n",
      "iteration: 189 loss: 2.08578396\n",
      "iteration: 190 loss: 2.84206796\n",
      "iteration: 191 loss: 0.76090425\n",
      "iteration: 192 loss: 0.72955704\n",
      "iteration: 193 loss: 2.15187097\n",
      "iteration: 194 loss: 0.60244220\n",
      "iteration: 195 loss: 3.29771876\n",
      "iteration: 196 loss: 3.08602357\n",
      "iteration: 197 loss: 0.98188537\n",
      "iteration: 198 loss: 0.49967775\n",
      "iteration: 199 loss: 0.52527225\n",
      "epoch:  52 mean loss training: 1.36153901\n",
      "epoch:  52 mean loss validation: 1.36118197\n",
      "iteration:   0 loss: 2.51560092\n",
      "iteration:   1 loss: 1.89217484\n",
      "iteration:   2 loss: 1.00635767\n",
      "iteration:   3 loss: 0.68831730\n",
      "iteration:   4 loss: 3.13005185\n",
      "iteration:   5 loss: 2.92554617\n",
      "iteration:   6 loss: 1.68884933\n",
      "iteration:   7 loss: 1.25782943\n",
      "iteration:   8 loss: 2.82028365\n",
      "iteration:   9 loss: 3.61760592\n",
      "iteration:  10 loss: 0.92812353\n",
      "iteration:  11 loss: 0.51319492\n",
      "iteration:  12 loss: 1.34500551\n",
      "iteration:  13 loss: 1.37571812\n",
      "iteration:  14 loss: 1.08727252\n",
      "iteration:  15 loss: 0.65889424\n",
      "iteration:  16 loss: 1.35972703\n",
      "iteration:  17 loss: 1.31178629\n",
      "iteration:  18 loss: 1.65141153\n",
      "iteration:  19 loss: 0.95044345\n",
      "iteration:  20 loss: 0.86386681\n",
      "iteration:  21 loss: 0.75836104\n",
      "iteration:  22 loss: 0.38640347\n",
      "iteration:  23 loss: 1.03965545\n",
      "iteration:  24 loss: 0.28429481\n",
      "iteration:  25 loss: 2.99933219\n",
      "iteration:  26 loss: 1.36811030\n",
      "iteration:  27 loss: 3.03331590\n",
      "iteration:  28 loss: 0.26730633\n",
      "iteration:  29 loss: 0.82694060\n",
      "iteration:  30 loss: 0.46942443\n",
      "iteration:  31 loss: 0.27695823\n",
      "iteration:  32 loss: 0.23874824\n",
      "iteration:  33 loss: 0.41843620\n",
      "iteration:  34 loss: 2.36220288\n",
      "iteration:  35 loss: 1.06993794\n",
      "iteration:  36 loss: 0.12065759\n",
      "iteration:  37 loss: 0.27779698\n",
      "iteration:  38 loss: 1.16650403\n",
      "iteration:  39 loss: 3.26197290\n",
      "iteration:  40 loss: 2.17956853\n",
      "iteration:  41 loss: 0.74476910\n",
      "iteration:  42 loss: 1.10040152\n",
      "iteration:  43 loss: 3.09005213\n",
      "iteration:  44 loss: 0.24957569\n",
      "iteration:  45 loss: 0.19699037\n",
      "iteration:  46 loss: 2.02785826\n",
      "iteration:  47 loss: 0.78987199\n",
      "iteration:  48 loss: 0.72876912\n",
      "iteration:  49 loss: 0.51781780\n",
      "iteration:  50 loss: 2.51680684\n",
      "iteration:  51 loss: 0.29203138\n",
      "iteration:  52 loss: 1.06667662\n",
      "iteration:  53 loss: 0.15889053\n",
      "iteration:  54 loss: 0.59564883\n",
      "iteration:  55 loss: 4.79649830\n",
      "iteration:  56 loss: 0.67731816\n",
      "iteration:  57 loss: 2.60415387\n",
      "iteration:  58 loss: 3.79083753\n",
      "iteration:  59 loss: 1.08733666\n",
      "iteration:  60 loss: 0.17892273\n",
      "iteration:  61 loss: 0.16342537\n",
      "iteration:  62 loss: 0.30075309\n",
      "iteration:  63 loss: 0.30983225\n",
      "iteration:  64 loss: 0.33089721\n",
      "iteration:  65 loss: 2.15244436\n",
      "iteration:  66 loss: 0.42330357\n",
      "iteration:  67 loss: 0.17862104\n",
      "iteration:  68 loss: 0.14392005\n",
      "iteration:  69 loss: 0.27117482\n",
      "iteration:  70 loss: 0.24975860\n",
      "iteration:  71 loss: 0.52256906\n",
      "iteration:  72 loss: 0.93867731\n",
      "iteration:  73 loss: 3.48288465\n",
      "iteration:  74 loss: 0.69482112\n",
      "iteration:  75 loss: 0.90079665\n",
      "iteration:  76 loss: 0.79353923\n",
      "iteration:  77 loss: 0.67592603\n",
      "iteration:  78 loss: 1.72272027\n",
      "iteration:  79 loss: 1.58807349\n",
      "iteration:  80 loss: 1.55388594\n",
      "iteration:  81 loss: 0.25025749\n",
      "iteration:  82 loss: 0.76913452\n",
      "iteration:  83 loss: 0.60972142\n",
      "iteration:  84 loss: 3.45126987\n",
      "iteration:  85 loss: 0.19176094\n",
      "iteration:  86 loss: 0.64922345\n",
      "iteration:  87 loss: 3.35057187\n",
      "iteration:  88 loss: 0.23436843\n",
      "iteration:  89 loss: 2.43644643\n",
      "iteration:  90 loss: 1.43114972\n",
      "iteration:  91 loss: 0.37177995\n",
      "iteration:  92 loss: 0.28950310\n",
      "iteration:  93 loss: 2.37365198\n",
      "iteration:  94 loss: 1.65165651\n",
      "iteration:  95 loss: 0.24001157\n",
      "iteration:  96 loss: 3.78708982\n",
      "iteration:  97 loss: 0.86547834\n",
      "iteration:  98 loss: 0.57820511\n",
      "iteration:  99 loss: 0.25466725\n",
      "iteration: 100 loss: 0.40869763\n",
      "iteration: 101 loss: 0.19674140\n",
      "iteration: 102 loss: 1.67610443\n",
      "iteration: 103 loss: 2.66259575\n",
      "iteration: 104 loss: 0.31277996\n",
      "iteration: 105 loss: 0.81651020\n",
      "iteration: 106 loss: 0.33670509\n",
      "iteration: 107 loss: 1.21190226\n",
      "iteration: 108 loss: 1.81658494\n",
      "iteration: 109 loss: 0.49188855\n",
      "iteration: 110 loss: 2.67851901\n",
      "iteration: 111 loss: 3.07785940\n",
      "iteration: 112 loss: 3.11339808\n",
      "iteration: 113 loss: 0.49643052\n",
      "iteration: 114 loss: 0.77045077\n",
      "iteration: 115 loss: 2.35011053\n",
      "iteration: 116 loss: 2.07322311\n",
      "iteration: 117 loss: 0.41714752\n",
      "iteration: 118 loss: 0.40554121\n",
      "iteration: 119 loss: 0.64085889\n",
      "iteration: 120 loss: 0.24682987\n",
      "iteration: 121 loss: 0.64546406\n",
      "iteration: 122 loss: 0.25638768\n",
      "iteration: 123 loss: 2.50319076\n",
      "iteration: 124 loss: 1.63323963\n",
      "iteration: 125 loss: 2.36179876\n",
      "iteration: 126 loss: 3.32973862\n",
      "iteration: 127 loss: 0.50039595\n",
      "iteration: 128 loss: 1.81715238\n",
      "iteration: 129 loss: 0.21535262\n",
      "iteration: 130 loss: 2.64521050\n",
      "iteration: 131 loss: 2.07111216\n",
      "iteration: 132 loss: 1.56550550\n",
      "iteration: 133 loss: 0.46724814\n",
      "iteration: 134 loss: 0.45054853\n",
      "iteration: 135 loss: 1.85116434\n",
      "iteration: 136 loss: 0.98578048\n",
      "iteration: 137 loss: 0.81025302\n",
      "iteration: 138 loss: 1.11445439\n",
      "iteration: 139 loss: 1.10011089\n",
      "iteration: 140 loss: 1.51806974\n",
      "iteration: 141 loss: 1.45454848\n",
      "iteration: 142 loss: 0.85732830\n",
      "iteration: 143 loss: 0.77514857\n",
      "iteration: 144 loss: 3.75359058\n",
      "iteration: 145 loss: 2.16064119\n",
      "iteration: 146 loss: 0.42485476\n",
      "iteration: 147 loss: 1.06116450\n",
      "iteration: 148 loss: 1.09581327\n",
      "iteration: 149 loss: 1.27220154\n",
      "iteration: 150 loss: 2.36448836\n",
      "iteration: 151 loss: 1.58576381\n",
      "iteration: 152 loss: 3.56002235\n",
      "iteration: 153 loss: 3.41172791\n",
      "iteration: 154 loss: 1.58503985\n",
      "iteration: 155 loss: 2.70058846\n",
      "iteration: 156 loss: 0.23054071\n",
      "iteration: 157 loss: 0.87695438\n",
      "iteration: 158 loss: 1.01906252\n",
      "iteration: 159 loss: 0.28978243\n",
      "iteration: 160 loss: 0.95231324\n",
      "iteration: 161 loss: 1.21718717\n",
      "iteration: 162 loss: 0.28974360\n",
      "iteration: 163 loss: 2.82486343\n",
      "iteration: 164 loss: 3.34088945\n",
      "iteration: 165 loss: 2.36945987\n",
      "iteration: 166 loss: 0.20189020\n",
      "iteration: 167 loss: 0.22847719\n",
      "iteration: 168 loss: 0.90288830\n",
      "iteration: 169 loss: 0.40920326\n",
      "iteration: 170 loss: 3.18824792\n",
      "iteration: 171 loss: 1.03705645\n",
      "iteration: 172 loss: 0.70171404\n",
      "iteration: 173 loss: 0.28801191\n",
      "iteration: 174 loss: 3.51385117\n",
      "iteration: 175 loss: 2.35530877\n",
      "iteration: 176 loss: 2.23339057\n",
      "iteration: 177 loss: 0.36655515\n",
      "iteration: 178 loss: 0.42602527\n",
      "iteration: 179 loss: 0.25145558\n",
      "iteration: 180 loss: 0.30637106\n",
      "iteration: 181 loss: 1.03407145\n",
      "iteration: 182 loss: 0.44196251\n",
      "iteration: 183 loss: 1.33619344\n",
      "iteration: 184 loss: 0.53087932\n",
      "iteration: 185 loss: 2.39831424\n",
      "iteration: 186 loss: 1.57967591\n",
      "iteration: 187 loss: 0.81060177\n",
      "iteration: 188 loss: 2.85923338\n",
      "iteration: 189 loss: 2.10774755\n",
      "iteration: 190 loss: 1.78616095\n",
      "iteration: 191 loss: 0.59401792\n",
      "iteration: 192 loss: 1.15633702\n",
      "iteration: 193 loss: 1.14085472\n",
      "iteration: 194 loss: 0.63440305\n",
      "iteration: 195 loss: 3.10608602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 196 loss: 3.45247293\n",
      "iteration: 197 loss: 0.61650991\n",
      "iteration: 198 loss: 0.94302630\n",
      "iteration: 199 loss: 0.70981205\n",
      "epoch:  53 mean loss training: 1.31977391\n",
      "epoch:  53 mean loss validation: 1.33164370\n",
      "iteration:   0 loss: 2.26679516\n",
      "iteration:   1 loss: 2.37257671\n",
      "iteration:   2 loss: 0.82767957\n",
      "iteration:   3 loss: 0.37754735\n",
      "iteration:   4 loss: 3.95781612\n",
      "iteration:   5 loss: 2.63244677\n",
      "iteration:   6 loss: 1.93108165\n",
      "iteration:   7 loss: 1.47225797\n",
      "iteration:   8 loss: 3.24373960\n",
      "iteration:   9 loss: 3.37316275\n",
      "iteration:  10 loss: 0.41647962\n",
      "iteration:  11 loss: 1.13398743\n",
      "iteration:  12 loss: 1.82785964\n",
      "iteration:  13 loss: 0.92011946\n",
      "iteration:  14 loss: 0.94955087\n",
      "iteration:  15 loss: 0.53271997\n",
      "iteration:  16 loss: 0.79970181\n",
      "iteration:  17 loss: 1.04433858\n",
      "iteration:  18 loss: 1.85604393\n",
      "iteration:  19 loss: 1.03040922\n",
      "iteration:  20 loss: 0.89076394\n",
      "iteration:  21 loss: 1.62042451\n",
      "iteration:  22 loss: 0.35016489\n",
      "iteration:  23 loss: 1.03528500\n",
      "iteration:  24 loss: 0.21784425\n",
      "iteration:  25 loss: 2.51522136\n",
      "iteration:  26 loss: 1.57613337\n",
      "iteration:  27 loss: 2.65113091\n",
      "iteration:  28 loss: 0.44178814\n",
      "iteration:  29 loss: 0.96808624\n",
      "iteration:  30 loss: 0.54971439\n",
      "iteration:  31 loss: 0.37679592\n",
      "iteration:  32 loss: 0.40073240\n",
      "iteration:  33 loss: 0.42355928\n",
      "iteration:  34 loss: 2.27843237\n",
      "iteration:  35 loss: 1.22857213\n",
      "iteration:  36 loss: 0.12100920\n",
      "iteration:  37 loss: 0.39169985\n",
      "iteration:  38 loss: 1.18739057\n",
      "iteration:  39 loss: 3.30775809\n",
      "iteration:  40 loss: 1.89803529\n",
      "iteration:  41 loss: 1.25345004\n",
      "iteration:  42 loss: 1.20637476\n",
      "iteration:  43 loss: 2.88583946\n",
      "iteration:  44 loss: 0.57177562\n",
      "iteration:  45 loss: 0.21013546\n",
      "iteration:  46 loss: 1.97848332\n",
      "iteration:  47 loss: 0.85428500\n",
      "iteration:  48 loss: 0.76905012\n",
      "iteration:  49 loss: 0.61342025\n",
      "iteration:  50 loss: 2.41512513\n",
      "iteration:  51 loss: 0.33125287\n",
      "iteration:  52 loss: 1.14034569\n",
      "iteration:  53 loss: 0.47732806\n",
      "iteration:  54 loss: 0.57905537\n",
      "iteration:  55 loss: 4.13329792\n",
      "iteration:  56 loss: 1.04424310\n",
      "iteration:  57 loss: 2.86640453\n",
      "iteration:  58 loss: 3.04512978\n",
      "iteration:  59 loss: 1.32234609\n",
      "iteration:  60 loss: 0.24193707\n",
      "iteration:  61 loss: 0.21069281\n",
      "iteration:  62 loss: 0.29581195\n",
      "iteration:  63 loss: 0.69807136\n",
      "iteration:  64 loss: 0.32918620\n",
      "iteration:  65 loss: 1.72438657\n",
      "iteration:  66 loss: 0.44846436\n",
      "iteration:  67 loss: 0.21371010\n",
      "iteration:  68 loss: 0.32570210\n",
      "iteration:  69 loss: 0.36945146\n",
      "iteration:  70 loss: 0.51696557\n",
      "iteration:  71 loss: 0.51161152\n",
      "iteration:  72 loss: 0.95211363\n",
      "iteration:  73 loss: 2.88710237\n",
      "iteration:  74 loss: 0.68199456\n",
      "iteration:  75 loss: 0.51719683\n",
      "iteration:  76 loss: 0.75173205\n",
      "iteration:  77 loss: 0.86270922\n",
      "iteration:  78 loss: 1.44928718\n",
      "iteration:  79 loss: 0.57751065\n",
      "iteration:  80 loss: 1.59033787\n",
      "iteration:  81 loss: 0.20350213\n",
      "iteration:  82 loss: 0.99577194\n",
      "iteration:  83 loss: 0.35768452\n",
      "iteration:  84 loss: 2.92840862\n",
      "iteration:  85 loss: 0.19232972\n",
      "iteration:  86 loss: 0.57313234\n",
      "iteration:  87 loss: 3.22242308\n",
      "iteration:  88 loss: 0.33999974\n",
      "iteration:  89 loss: 2.91575050\n",
      "iteration:  90 loss: 1.63915241\n",
      "iteration:  91 loss: 0.37494481\n",
      "iteration:  92 loss: 0.53036702\n",
      "iteration:  93 loss: 2.17766047\n",
      "iteration:  94 loss: 1.28474283\n",
      "iteration:  95 loss: 0.78990090\n",
      "iteration:  96 loss: 4.26011801\n",
      "iteration:  97 loss: 0.89821738\n",
      "iteration:  98 loss: 0.53591877\n",
      "iteration:  99 loss: 0.26212153\n",
      "iteration: 100 loss: 0.36690816\n",
      "iteration: 101 loss: 0.24509367\n",
      "iteration: 102 loss: 0.95654941\n",
      "iteration: 103 loss: 2.91107130\n",
      "iteration: 104 loss: 0.35524940\n",
      "iteration: 105 loss: 0.75101352\n",
      "iteration: 106 loss: 0.21069273\n",
      "iteration: 107 loss: 0.73388195\n",
      "iteration: 108 loss: 1.95964670\n",
      "iteration: 109 loss: 0.54762167\n",
      "iteration: 110 loss: 2.82350016\n",
      "iteration: 111 loss: 3.08719563\n",
      "iteration: 112 loss: 2.83409119\n",
      "iteration: 113 loss: 1.39605987\n",
      "iteration: 114 loss: 2.36964536\n",
      "iteration: 115 loss: 2.68246770\n",
      "iteration: 116 loss: 1.95753324\n",
      "iteration: 117 loss: 0.19203265\n",
      "iteration: 118 loss: 0.31613073\n",
      "iteration: 119 loss: 1.05807531\n",
      "iteration: 120 loss: 0.25541908\n",
      "iteration: 121 loss: 0.91670632\n",
      "iteration: 122 loss: 0.48431277\n",
      "iteration: 123 loss: 2.27471495\n",
      "iteration: 124 loss: 2.08419800\n",
      "iteration: 125 loss: 2.26401830\n",
      "iteration: 126 loss: 3.23640919\n",
      "iteration: 127 loss: 0.53487396\n",
      "iteration: 128 loss: 1.32594728\n",
      "iteration: 129 loss: 0.37545609\n",
      "iteration: 130 loss: 2.71777177\n",
      "iteration: 131 loss: 2.18209481\n",
      "iteration: 132 loss: 2.19103980\n",
      "iteration: 133 loss: 0.57471251\n",
      "iteration: 134 loss: 0.57939094\n",
      "iteration: 135 loss: 2.14554596\n",
      "iteration: 136 loss: 1.07088482\n",
      "iteration: 137 loss: 1.23232234\n",
      "iteration: 138 loss: 1.26001084\n",
      "iteration: 139 loss: 1.00184405\n",
      "iteration: 140 loss: 1.77907491\n",
      "iteration: 141 loss: 0.95758653\n",
      "iteration: 142 loss: 0.72863317\n",
      "iteration: 143 loss: 1.68335509\n",
      "iteration: 144 loss: 3.56450105\n",
      "iteration: 145 loss: 2.20401692\n",
      "iteration: 146 loss: 0.43038762\n",
      "iteration: 147 loss: 0.97363621\n",
      "iteration: 148 loss: 1.04264915\n",
      "iteration: 149 loss: 1.52374375\n",
      "iteration: 150 loss: 1.85313427\n",
      "iteration: 151 loss: 2.06396961\n",
      "iteration: 152 loss: 3.20134425\n",
      "iteration: 153 loss: 3.31243277\n",
      "iteration: 154 loss: 1.60352719\n",
      "iteration: 155 loss: 2.71848679\n",
      "iteration: 156 loss: 0.22711354\n",
      "iteration: 157 loss: 0.88520914\n",
      "iteration: 158 loss: 1.01083350\n",
      "iteration: 159 loss: 0.37298775\n",
      "iteration: 160 loss: 0.79314297\n",
      "iteration: 161 loss: 1.17484581\n",
      "iteration: 162 loss: 0.30359805\n",
      "iteration: 163 loss: 3.26137543\n",
      "iteration: 164 loss: 3.06439257\n",
      "iteration: 165 loss: 2.36421442\n",
      "iteration: 166 loss: 0.47102532\n",
      "iteration: 167 loss: 0.23713598\n",
      "iteration: 168 loss: 0.79250097\n",
      "iteration: 169 loss: 0.85799116\n",
      "iteration: 170 loss: 2.79224706\n",
      "iteration: 171 loss: 0.97664541\n",
      "iteration: 172 loss: 0.76855600\n",
      "iteration: 173 loss: 0.16669834\n",
      "iteration: 174 loss: 3.57769084\n",
      "iteration: 175 loss: 1.89012587\n",
      "iteration: 176 loss: 2.33603477\n",
      "iteration: 177 loss: 0.41413313\n",
      "iteration: 178 loss: 0.46451485\n",
      "iteration: 179 loss: 0.15181069\n",
      "iteration: 180 loss: 0.21808217\n",
      "iteration: 181 loss: 0.96982068\n",
      "iteration: 182 loss: 0.37060061\n",
      "iteration: 183 loss: 1.76610613\n",
      "iteration: 184 loss: 0.75840896\n",
      "iteration: 185 loss: 2.05144835\n",
      "iteration: 186 loss: 0.90384960\n",
      "iteration: 187 loss: 0.76111168\n",
      "iteration: 188 loss: 3.77131343\n",
      "iteration: 189 loss: 2.08355546\n",
      "iteration: 190 loss: 2.69025803\n",
      "iteration: 191 loss: 0.79693592\n",
      "iteration: 192 loss: 0.69661331\n",
      "iteration: 193 loss: 1.90330625\n",
      "iteration: 194 loss: 0.58974546\n",
      "iteration: 195 loss: 2.97235847\n",
      "iteration: 196 loss: 3.01743603\n",
      "iteration: 197 loss: 1.07194960\n",
      "iteration: 198 loss: 0.27062044\n",
      "iteration: 199 loss: 0.59976703\n",
      "epoch:  54 mean loss training: 1.34726536\n",
      "epoch:  54 mean loss validation: 1.39184344\n",
      "iteration:   0 loss: 2.60165548\n",
      "iteration:   1 loss: 1.84414876\n",
      "iteration:   2 loss: 0.91204423\n",
      "iteration:   3 loss: 0.72251803\n",
      "iteration:   4 loss: 2.93114018\n",
      "iteration:   5 loss: 2.88459349\n",
      "iteration:   6 loss: 1.62615144\n",
      "iteration:   7 loss: 1.24342513\n",
      "iteration:   8 loss: 2.92256665\n",
      "iteration:   9 loss: 3.64199972\n",
      "iteration:  10 loss: 0.65080929\n",
      "iteration:  11 loss: 0.50027740\n",
      "iteration:  12 loss: 1.12798226\n",
      "iteration:  13 loss: 0.92260140\n",
      "iteration:  14 loss: 1.06296301\n",
      "iteration:  15 loss: 0.58556873\n",
      "iteration:  16 loss: 0.98946327\n",
      "iteration:  17 loss: 1.04139757\n",
      "iteration:  18 loss: 1.60159206\n",
      "iteration:  19 loss: 0.76011926\n",
      "iteration:  20 loss: 0.75469548\n",
      "iteration:  21 loss: 0.61277711\n",
      "iteration:  22 loss: 0.25346491\n",
      "iteration:  23 loss: 1.03413975\n",
      "iteration:  24 loss: 0.25030938\n",
      "iteration:  25 loss: 2.87359715\n",
      "iteration:  26 loss: 1.28615284\n",
      "iteration:  27 loss: 3.16918349\n",
      "iteration:  28 loss: 0.20943159\n",
      "iteration:  29 loss: 0.74612552\n",
      "iteration:  30 loss: 0.42603251\n",
      "iteration:  31 loss: 0.26274583\n",
      "iteration:  32 loss: 0.26996258\n",
      "iteration:  33 loss: 0.42864886\n",
      "iteration:  34 loss: 2.02459407\n",
      "iteration:  35 loss: 1.17832816\n",
      "iteration:  36 loss: 0.12130470\n",
      "iteration:  37 loss: 0.40827167\n",
      "iteration:  38 loss: 1.17784750\n",
      "iteration:  39 loss: 3.28397107\n",
      "iteration:  40 loss: 1.97654045\n",
      "iteration:  41 loss: 0.74911070\n",
      "iteration:  42 loss: 0.94221711\n",
      "iteration:  43 loss: 2.89505053\n",
      "iteration:  44 loss: 0.48493433\n",
      "iteration:  45 loss: 0.18726517\n",
      "iteration:  46 loss: 2.02924418\n",
      "iteration:  47 loss: 0.65855640\n",
      "iteration:  48 loss: 0.75482225\n",
      "iteration:  49 loss: 0.60111165\n",
      "iteration:  50 loss: 2.45584249\n",
      "iteration:  51 loss: 0.30574143\n",
      "iteration:  52 loss: 0.87131846\n",
      "iteration:  53 loss: 0.25093180\n",
      "iteration:  54 loss: 0.61676139\n",
      "iteration:  55 loss: 4.69414663\n",
      "iteration:  56 loss: 0.82995456\n",
      "iteration:  57 loss: 2.60839057\n",
      "iteration:  58 loss: 3.72182536\n",
      "iteration:  59 loss: 1.31299698\n",
      "iteration:  60 loss: 0.17355508\n",
      "iteration:  61 loss: 0.22379157\n",
      "iteration:  62 loss: 0.33748478\n",
      "iteration:  63 loss: 0.35526854\n",
      "iteration:  64 loss: 0.35788071\n",
      "iteration:  65 loss: 2.27093220\n",
      "iteration:  66 loss: 0.49650842\n",
      "iteration:  67 loss: 0.15644826\n",
      "iteration:  68 loss: 0.38863924\n",
      "iteration:  69 loss: 0.24350254\n",
      "iteration:  70 loss: 0.37525028\n",
      "iteration:  71 loss: 0.41960353\n",
      "iteration:  72 loss: 0.88223058\n",
      "iteration:  73 loss: 3.00135708\n",
      "iteration:  74 loss: 0.62663436\n",
      "iteration:  75 loss: 0.60701841\n",
      "iteration:  76 loss: 0.70983410\n",
      "iteration:  77 loss: 0.62971342\n",
      "iteration:  78 loss: 1.48236084\n",
      "iteration:  79 loss: 1.56716311\n",
      "iteration:  80 loss: 1.49678731\n",
      "iteration:  81 loss: 0.23195493\n",
      "iteration:  82 loss: 0.75648105\n",
      "iteration:  83 loss: 0.77946234\n",
      "iteration:  84 loss: 3.55028009\n",
      "iteration:  85 loss: 0.24632743\n",
      "iteration:  86 loss: 0.60177571\n",
      "iteration:  87 loss: 2.86910892\n",
      "iteration:  88 loss: 0.45501924\n",
      "iteration:  89 loss: 2.65052509\n",
      "iteration:  90 loss: 1.42979538\n",
      "iteration:  91 loss: 0.45801848\n",
      "iteration:  92 loss: 0.38386017\n",
      "iteration:  93 loss: 2.11533833\n",
      "iteration:  94 loss: 1.53251791\n",
      "iteration:  95 loss: 0.45915201\n",
      "iteration:  96 loss: 4.96804667\n",
      "iteration:  97 loss: 1.07201207\n",
      "iteration:  98 loss: 0.53930217\n",
      "iteration:  99 loss: 0.31437680\n",
      "iteration: 100 loss: 0.40075317\n",
      "iteration: 101 loss: 0.31824613\n",
      "iteration: 102 loss: 0.97285247\n",
      "iteration: 103 loss: 2.80219078\n",
      "iteration: 104 loss: 0.37861615\n",
      "iteration: 105 loss: 0.72110355\n",
      "iteration: 106 loss: 0.21871561\n",
      "iteration: 107 loss: 1.07845378\n",
      "iteration: 108 loss: 1.36428940\n",
      "iteration: 109 loss: 0.60892004\n",
      "iteration: 110 loss: 2.63584900\n",
      "iteration: 111 loss: 3.08960462\n",
      "iteration: 112 loss: 2.96589041\n",
      "iteration: 113 loss: 1.01604640\n",
      "iteration: 114 loss: 1.67068958\n",
      "iteration: 115 loss: 2.18778419\n",
      "iteration: 116 loss: 2.15526152\n",
      "iteration: 117 loss: 0.22384568\n",
      "iteration: 118 loss: 0.45989192\n",
      "iteration: 119 loss: 0.69341952\n",
      "iteration: 120 loss: 0.32010344\n",
      "iteration: 121 loss: 0.67115271\n",
      "iteration: 122 loss: 0.49456775\n",
      "iteration: 123 loss: 1.47997236\n",
      "iteration: 124 loss: 1.60261738\n",
      "iteration: 125 loss: 2.47985673\n",
      "iteration: 126 loss: 3.27787256\n",
      "iteration: 127 loss: 0.43898246\n",
      "iteration: 128 loss: 1.63413703\n",
      "iteration: 129 loss: 0.26999319\n",
      "iteration: 130 loss: 2.71412873\n",
      "iteration: 131 loss: 2.17219663\n",
      "iteration: 132 loss: 1.58771634\n",
      "iteration: 133 loss: 0.46621600\n",
      "iteration: 134 loss: 0.48604488\n",
      "iteration: 135 loss: 1.37280393\n",
      "iteration: 136 loss: 1.08883798\n",
      "iteration: 137 loss: 1.09873080\n",
      "iteration: 138 loss: 1.44593811\n",
      "iteration: 139 loss: 1.17313719\n",
      "iteration: 140 loss: 1.61170232\n",
      "iteration: 141 loss: 1.04360366\n",
      "iteration: 142 loss: 0.85109782\n",
      "iteration: 143 loss: 0.71751904\n",
      "iteration: 144 loss: 3.69475913\n",
      "iteration: 145 loss: 1.86487913\n",
      "iteration: 146 loss: 0.47670656\n",
      "iteration: 147 loss: 0.90174711\n",
      "iteration: 148 loss: 1.01905596\n",
      "iteration: 149 loss: 1.36055362\n",
      "iteration: 150 loss: 2.06656098\n",
      "iteration: 151 loss: 1.49143553\n",
      "iteration: 152 loss: 3.27025700\n",
      "iteration: 153 loss: 3.35656381\n",
      "iteration: 154 loss: 1.40516531\n",
      "iteration: 155 loss: 2.73812151\n",
      "iteration: 156 loss: 0.20529068\n",
      "iteration: 157 loss: 0.89792150\n",
      "iteration: 158 loss: 1.55812287\n",
      "iteration: 159 loss: 0.42642537\n",
      "iteration: 160 loss: 0.98382360\n",
      "iteration: 161 loss: 1.13894522\n",
      "iteration: 162 loss: 0.34934929\n",
      "iteration: 163 loss: 2.69544744\n",
      "iteration: 164 loss: 3.02552199\n",
      "iteration: 165 loss: 0.67505127\n",
      "iteration: 166 loss: 0.48002329\n",
      "iteration: 167 loss: 0.24746977\n",
      "iteration: 168 loss: 0.87845677\n",
      "iteration: 169 loss: 0.91739005\n",
      "iteration: 170 loss: 3.02790904\n",
      "iteration: 171 loss: 1.05233562\n",
      "iteration: 172 loss: 0.74477458\n",
      "iteration: 173 loss: 0.19826168\n",
      "iteration: 174 loss: 3.91366601\n",
      "iteration: 175 loss: 2.68770766\n",
      "iteration: 176 loss: 2.07883549\n",
      "iteration: 177 loss: 0.20789944\n",
      "iteration: 178 loss: 0.47258756\n",
      "iteration: 179 loss: 0.36488250\n",
      "iteration: 180 loss: 0.37432632\n",
      "iteration: 181 loss: 1.36728358\n",
      "iteration: 182 loss: 0.65005386\n",
      "iteration: 183 loss: 1.96207392\n",
      "iteration: 184 loss: 1.09274292\n",
      "iteration: 185 loss: 2.07966709\n",
      "iteration: 186 loss: 1.44577551\n",
      "iteration: 187 loss: 0.98057032\n",
      "iteration: 188 loss: 3.01270461\n",
      "iteration: 189 loss: 2.06707001\n",
      "iteration: 190 loss: 2.30679297\n",
      "iteration: 191 loss: 0.88404965\n",
      "iteration: 192 loss: 0.85147107\n",
      "iteration: 193 loss: 1.66164839\n",
      "iteration: 194 loss: 0.65817916\n",
      "iteration: 195 loss: 2.82836819\n",
      "iteration: 196 loss: 3.28797698\n",
      "iteration: 197 loss: 1.35223031\n",
      "iteration: 198 loss: 0.68354666\n",
      "iteration: 199 loss: 0.74189526\n",
      "epoch:  55 mean loss training: 1.31065166\n",
      "epoch:  55 mean loss validation: 1.42463124\n",
      "iteration:   0 loss: 3.10099459\n",
      "iteration:   1 loss: 1.08508277\n",
      "iteration:   2 loss: 1.06551683\n",
      "iteration:   3 loss: 0.79593635\n",
      "iteration:   4 loss: 2.36677217\n",
      "iteration:   5 loss: 1.64204824\n",
      "iteration:   6 loss: 1.91667080\n",
      "iteration:   7 loss: 1.13785470\n",
      "iteration:   8 loss: 2.65213561\n",
      "iteration:   9 loss: 3.52974367\n",
      "iteration:  10 loss: 1.46222663\n",
      "iteration:  11 loss: 0.75620741\n",
      "iteration:  12 loss: 1.33072460\n",
      "iteration:  13 loss: 1.88365400\n",
      "iteration:  14 loss: 1.23544085\n",
      "iteration:  15 loss: 0.75530809\n",
      "iteration:  16 loss: 1.61854947\n",
      "iteration:  17 loss: 1.48392785\n",
      "iteration:  18 loss: 1.72934127\n",
      "iteration:  19 loss: 0.97896057\n",
      "iteration:  20 loss: 0.96071297\n",
      "iteration:  21 loss: 1.04231894\n",
      "iteration:  22 loss: 0.51099783\n",
      "iteration:  23 loss: 1.07648110\n",
      "iteration:  24 loss: 0.42534873\n",
      "iteration:  25 loss: 2.58432484\n",
      "iteration:  26 loss: 1.50466657\n",
      "iteration:  27 loss: 2.76828647\n",
      "iteration:  28 loss: 0.33657250\n",
      "iteration:  29 loss: 0.79026777\n",
      "iteration:  30 loss: 0.45879477\n",
      "iteration:  31 loss: 0.27218655\n",
      "iteration:  32 loss: 0.32510805\n",
      "iteration:  33 loss: 0.42142186\n",
      "iteration:  34 loss: 1.82045817\n",
      "iteration:  35 loss: 1.14173234\n",
      "iteration:  36 loss: 0.11026622\n",
      "iteration:  37 loss: 0.40932554\n",
      "iteration:  38 loss: 1.17653728\n",
      "iteration:  39 loss: 3.29332733\n",
      "iteration:  40 loss: 2.18439198\n",
      "iteration:  41 loss: 0.82135332\n",
      "iteration:  42 loss: 1.16920805\n",
      "iteration:  43 loss: 2.92431784\n",
      "iteration:  44 loss: 0.34878424\n",
      "iteration:  45 loss: 0.17917521\n",
      "iteration:  46 loss: 2.06612134\n",
      "iteration:  47 loss: 0.66911334\n",
      "iteration:  48 loss: 0.79105604\n",
      "iteration:  49 loss: 0.55723667\n",
      "iteration:  50 loss: 2.53638744\n",
      "iteration:  51 loss: 0.30141869\n",
      "iteration:  52 loss: 1.15874350\n",
      "iteration:  53 loss: 0.29723695\n",
      "iteration:  54 loss: 0.62285310\n",
      "iteration:  55 loss: 4.44643879\n",
      "iteration:  56 loss: 0.84422117\n",
      "iteration:  57 loss: 2.73080993\n",
      "iteration:  58 loss: 3.52976274\n",
      "iteration:  59 loss: 1.46448159\n",
      "iteration:  60 loss: 0.16236652\n",
      "iteration:  61 loss: 0.14867786\n",
      "iteration:  62 loss: 0.45652851\n",
      "iteration:  63 loss: 0.41605321\n",
      "iteration:  64 loss: 0.34894150\n",
      "iteration:  65 loss: 1.90095806\n",
      "iteration:  66 loss: 0.45336968\n",
      "iteration:  67 loss: 0.20254470\n",
      "iteration:  68 loss: 0.15091150\n",
      "iteration:  69 loss: 0.28622615\n",
      "iteration:  70 loss: 0.23106016\n",
      "iteration:  71 loss: 0.72806311\n",
      "iteration:  72 loss: 0.87629819\n",
      "iteration:  73 loss: 2.79659390\n",
      "iteration:  74 loss: 0.85604048\n",
      "iteration:  75 loss: 0.68606877\n",
      "iteration:  76 loss: 0.80416417\n",
      "iteration:  77 loss: 0.80578148\n",
      "iteration:  78 loss: 1.57608223\n",
      "iteration:  79 loss: 0.63434112\n",
      "iteration:  80 loss: 1.79502451\n",
      "iteration:  81 loss: 0.21206586\n",
      "iteration:  82 loss: 0.99229008\n",
      "iteration:  83 loss: 0.38543147\n",
      "iteration:  84 loss: 2.98633027\n",
      "iteration:  85 loss: 0.18714093\n",
      "iteration:  86 loss: 0.61474860\n",
      "iteration:  87 loss: 3.31428933\n",
      "iteration:  88 loss: 0.28713873\n",
      "iteration:  89 loss: 3.06649518\n",
      "iteration:  90 loss: 1.19420028\n",
      "iteration:  91 loss: 0.38425344\n",
      "iteration:  92 loss: 0.30952069\n",
      "iteration:  93 loss: 2.27170682\n",
      "iteration:  94 loss: 1.56117582\n",
      "iteration:  95 loss: 0.76985705\n",
      "iteration:  96 loss: 4.43899679\n",
      "iteration:  97 loss: 0.87697262\n",
      "iteration:  98 loss: 0.58813721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  99 loss: 0.34075165\n",
      "iteration: 100 loss: 0.42302760\n",
      "iteration: 101 loss: 0.23010373\n",
      "iteration: 102 loss: 0.99705821\n",
      "iteration: 103 loss: 2.65923381\n",
      "iteration: 104 loss: 0.34565789\n",
      "iteration: 105 loss: 0.79759479\n",
      "iteration: 106 loss: 0.27075139\n",
      "iteration: 107 loss: 0.72113371\n",
      "iteration: 108 loss: 1.55894589\n",
      "iteration: 109 loss: 0.45220116\n",
      "iteration: 110 loss: 2.79878879\n",
      "iteration: 111 loss: 3.11168456\n",
      "iteration: 112 loss: 2.81509018\n",
      "iteration: 113 loss: 1.23726714\n",
      "iteration: 114 loss: 1.78573644\n",
      "iteration: 115 loss: 2.43362474\n",
      "iteration: 116 loss: 2.02637386\n",
      "iteration: 117 loss: 0.18122408\n",
      "iteration: 118 loss: 0.41118717\n",
      "iteration: 119 loss: 1.07524610\n",
      "iteration: 120 loss: 0.28997654\n",
      "iteration: 121 loss: 0.72624648\n",
      "iteration: 122 loss: 0.52949661\n",
      "iteration: 123 loss: 2.38882017\n",
      "iteration: 124 loss: 1.88766432\n",
      "iteration: 125 loss: 2.09848571\n",
      "iteration: 126 loss: 3.08860803\n",
      "iteration: 127 loss: 0.42814198\n",
      "iteration: 128 loss: 1.90588737\n",
      "iteration: 129 loss: 0.21356852\n",
      "iteration: 130 loss: 2.67191529\n",
      "iteration: 131 loss: 2.02288532\n",
      "iteration: 132 loss: 1.92296898\n",
      "iteration: 133 loss: 0.57045275\n",
      "iteration: 134 loss: 0.45307705\n",
      "iteration: 135 loss: 1.31402278\n",
      "iteration: 136 loss: 1.06728935\n",
      "iteration: 137 loss: 1.08462334\n",
      "iteration: 138 loss: 1.17496598\n",
      "iteration: 139 loss: 1.13420689\n",
      "iteration: 140 loss: 1.72791529\n",
      "iteration: 141 loss: 0.96575576\n",
      "iteration: 142 loss: 0.72534364\n",
      "iteration: 143 loss: 1.66792083\n",
      "iteration: 144 loss: 3.66606092\n",
      "iteration: 145 loss: 2.16392064\n",
      "iteration: 146 loss: 0.51336837\n",
      "iteration: 147 loss: 0.98965079\n",
      "iteration: 148 loss: 1.08863389\n",
      "iteration: 149 loss: 1.61036897\n",
      "iteration: 150 loss: 2.14674830\n",
      "iteration: 151 loss: 1.54634118\n",
      "iteration: 152 loss: 3.41389561\n",
      "iteration: 153 loss: 3.28057814\n",
      "iteration: 154 loss: 1.56826854\n",
      "iteration: 155 loss: 2.67889071\n",
      "iteration: 156 loss: 0.27937332\n",
      "iteration: 157 loss: 0.97481209\n",
      "iteration: 158 loss: 1.39819634\n",
      "iteration: 159 loss: 0.49440676\n",
      "iteration: 160 loss: 0.98846757\n",
      "iteration: 161 loss: 1.24289191\n",
      "iteration: 162 loss: 0.35832506\n",
      "iteration: 163 loss: 3.00000238\n",
      "iteration: 164 loss: 3.03693032\n",
      "iteration: 165 loss: 2.14235139\n",
      "iteration: 166 loss: 0.31633919\n",
      "iteration: 167 loss: 0.23780699\n",
      "iteration: 168 loss: 1.02307618\n",
      "iteration: 169 loss: 0.67368174\n",
      "iteration: 170 loss: 2.89548850\n",
      "iteration: 171 loss: 1.05919051\n",
      "iteration: 172 loss: 0.80125070\n",
      "iteration: 173 loss: 0.20303904\n",
      "iteration: 174 loss: 3.55619097\n",
      "iteration: 175 loss: 1.95652950\n",
      "iteration: 176 loss: 2.30860662\n",
      "iteration: 177 loss: 0.39796123\n",
      "iteration: 178 loss: 0.31118575\n",
      "iteration: 179 loss: 0.18443938\n",
      "iteration: 180 loss: 0.21166092\n",
      "iteration: 181 loss: 1.13334417\n",
      "iteration: 182 loss: 0.29880705\n",
      "iteration: 183 loss: 1.90867114\n",
      "iteration: 184 loss: 0.73665458\n",
      "iteration: 185 loss: 2.22397375\n",
      "iteration: 186 loss: 1.30054200\n",
      "iteration: 187 loss: 0.73632133\n",
      "iteration: 188 loss: 2.81918192\n",
      "iteration: 189 loss: 2.06418419\n",
      "iteration: 190 loss: 1.90409625\n",
      "iteration: 191 loss: 0.85601300\n",
      "iteration: 192 loss: 0.83708715\n",
      "iteration: 193 loss: 1.36545002\n",
      "iteration: 194 loss: 0.62044781\n",
      "iteration: 195 loss: 3.01124883\n",
      "iteration: 196 loss: 3.20783472\n",
      "iteration: 197 loss: 1.10024095\n",
      "iteration: 198 loss: 0.71735466\n",
      "iteration: 199 loss: 0.57215339\n",
      "epoch:  56 mean loss training: 1.32610822\n",
      "epoch:  56 mean loss validation: 1.41475952\n",
      "iteration:   0 loss: 3.20659065\n",
      "iteration:   1 loss: 1.00420189\n",
      "iteration:   2 loss: 0.96029484\n",
      "iteration:   3 loss: 0.81343228\n",
      "iteration:   4 loss: 2.11240363\n",
      "iteration:   5 loss: 1.79348743\n",
      "iteration:   6 loss: 2.12904453\n",
      "iteration:   7 loss: 1.14858055\n",
      "iteration:   8 loss: 2.77613664\n",
      "iteration:   9 loss: 3.51736403\n",
      "iteration:  10 loss: 1.02744782\n",
      "iteration:  11 loss: 0.53193837\n",
      "iteration:  12 loss: 1.42610919\n",
      "iteration:  13 loss: 1.16088676\n",
      "iteration:  14 loss: 1.07751417\n",
      "iteration:  15 loss: 0.71187389\n",
      "iteration:  16 loss: 1.10233796\n",
      "iteration:  17 loss: 1.00232244\n",
      "iteration:  18 loss: 1.65841436\n",
      "iteration:  19 loss: 0.84915388\n",
      "iteration:  20 loss: 0.77845490\n",
      "iteration:  21 loss: 0.89002222\n",
      "iteration:  22 loss: 0.30383348\n",
      "iteration:  23 loss: 1.15147161\n",
      "iteration:  24 loss: 0.23304009\n",
      "iteration:  25 loss: 2.64115095\n",
      "iteration:  26 loss: 1.29081225\n",
      "iteration:  27 loss: 2.96588254\n",
      "iteration:  28 loss: 0.18886456\n",
      "iteration:  29 loss: 0.87118548\n",
      "iteration:  30 loss: 0.46395710\n",
      "iteration:  31 loss: 0.22793426\n",
      "iteration:  32 loss: 0.38532582\n",
      "iteration:  33 loss: 0.46022892\n",
      "iteration:  34 loss: 2.37600589\n",
      "iteration:  35 loss: 1.14586914\n",
      "iteration:  36 loss: 0.15545334\n",
      "iteration:  37 loss: 0.24864186\n",
      "iteration:  38 loss: 1.18820345\n",
      "iteration:  39 loss: 3.29291105\n",
      "iteration:  40 loss: 2.02359724\n",
      "iteration:  41 loss: 0.86507303\n",
      "iteration:  42 loss: 1.19650114\n",
      "iteration:  43 loss: 3.10352516\n",
      "iteration:  44 loss: 0.47644493\n",
      "iteration:  45 loss: 0.17565823\n",
      "iteration:  46 loss: 2.11945629\n",
      "iteration:  47 loss: 0.77986288\n",
      "iteration:  48 loss: 0.75356632\n",
      "iteration:  49 loss: 0.57283032\n",
      "iteration:  50 loss: 2.63867092\n",
      "iteration:  51 loss: 0.36571231\n",
      "iteration:  52 loss: 1.17219162\n",
      "iteration:  53 loss: 0.21988671\n",
      "iteration:  54 loss: 0.46875075\n",
      "iteration:  55 loss: 4.38017464\n",
      "iteration:  56 loss: 0.75750721\n",
      "iteration:  57 loss: 2.87725472\n",
      "iteration:  58 loss: 3.35503912\n",
      "iteration:  59 loss: 1.20571005\n",
      "iteration:  60 loss: 0.26961300\n",
      "iteration:  61 loss: 0.16309804\n",
      "iteration:  62 loss: 0.24309915\n",
      "iteration:  63 loss: 0.67462456\n",
      "iteration:  64 loss: 0.33750892\n",
      "iteration:  65 loss: 2.47126889\n",
      "iteration:  66 loss: 0.43278813\n",
      "iteration:  67 loss: 0.21601963\n",
      "iteration:  68 loss: 0.15438470\n",
      "iteration:  69 loss: 0.35403782\n",
      "iteration:  70 loss: 0.32024485\n",
      "iteration:  71 loss: 0.51423818\n",
      "iteration:  72 loss: 0.76182669\n",
      "iteration:  73 loss: 2.98979974\n",
      "iteration:  74 loss: 0.81856322\n",
      "iteration:  75 loss: 0.55769676\n",
      "iteration:  76 loss: 0.78555900\n",
      "iteration:  77 loss: 0.66610694\n",
      "iteration:  78 loss: 1.56636417\n",
      "iteration:  79 loss: 0.58212179\n",
      "iteration:  80 loss: 1.99195611\n",
      "iteration:  81 loss: 0.23528354\n",
      "iteration:  82 loss: 0.83841550\n",
      "iteration:  83 loss: 0.61081684\n",
      "iteration:  84 loss: 3.18083835\n",
      "iteration:  85 loss: 0.18534130\n",
      "iteration:  86 loss: 0.63984376\n",
      "iteration:  87 loss: 3.29556894\n",
      "iteration:  88 loss: 0.22668888\n",
      "iteration:  89 loss: 2.78482437\n",
      "iteration:  90 loss: 1.04317975\n",
      "iteration:  91 loss: 0.39337105\n",
      "iteration:  92 loss: 0.63062423\n",
      "iteration:  93 loss: 1.63389289\n",
      "iteration:  94 loss: 0.90507030\n",
      "iteration:  95 loss: 0.83790404\n",
      "iteration:  96 loss: 4.19236517\n",
      "iteration:  97 loss: 0.66331571\n",
      "iteration:  98 loss: 0.61223465\n",
      "iteration:  99 loss: 0.29139653\n",
      "iteration: 100 loss: 0.57247025\n",
      "iteration: 101 loss: 0.24949670\n",
      "iteration: 102 loss: 0.92105156\n",
      "iteration: 103 loss: 2.56546712\n",
      "iteration: 104 loss: 0.69851333\n",
      "iteration: 105 loss: 0.75497353\n",
      "iteration: 106 loss: 0.30456907\n",
      "iteration: 107 loss: 0.82007462\n",
      "iteration: 108 loss: 1.64392209\n",
      "iteration: 109 loss: 0.47443736\n",
      "iteration: 110 loss: 2.47755814\n",
      "iteration: 111 loss: 3.11346579\n",
      "iteration: 112 loss: 3.04374075\n",
      "iteration: 113 loss: 1.79032934\n",
      "iteration: 114 loss: 2.37305927\n",
      "iteration: 115 loss: 3.27533746\n",
      "iteration: 116 loss: 2.40495324\n",
      "iteration: 117 loss: 0.28649154\n",
      "iteration: 118 loss: 0.49987948\n",
      "iteration: 119 loss: 0.58824641\n",
      "iteration: 120 loss: 0.53111553\n",
      "iteration: 121 loss: 0.78147745\n",
      "iteration: 122 loss: 0.61768258\n",
      "iteration: 123 loss: 2.46654058\n",
      "iteration: 124 loss: 1.62105572\n",
      "iteration: 125 loss: 1.90073073\n",
      "iteration: 126 loss: 3.28132057\n",
      "iteration: 127 loss: 0.50645459\n",
      "iteration: 128 loss: 1.48367190\n",
      "iteration: 129 loss: 0.32808924\n",
      "iteration: 130 loss: 2.84207892\n",
      "iteration: 131 loss: 2.04013753\n",
      "iteration: 132 loss: 1.62117290\n",
      "iteration: 133 loss: 0.46064657\n",
      "iteration: 134 loss: 0.48292154\n",
      "iteration: 135 loss: 2.11601400\n",
      "iteration: 136 loss: 0.97562051\n",
      "iteration: 137 loss: 1.35126448\n",
      "iteration: 138 loss: 1.10327172\n",
      "iteration: 139 loss: 0.91637427\n",
      "iteration: 140 loss: 1.61835802\n",
      "iteration: 141 loss: 0.95470595\n",
      "iteration: 142 loss: 0.75067902\n",
      "iteration: 143 loss: 1.94994926\n",
      "iteration: 144 loss: 3.51900601\n",
      "iteration: 145 loss: 2.23929548\n",
      "iteration: 146 loss: 1.05930996\n",
      "iteration: 147 loss: 1.04641426\n",
      "iteration: 148 loss: 0.93855250\n",
      "iteration: 149 loss: 1.87059677\n",
      "iteration: 150 loss: 1.79207432\n",
      "iteration: 151 loss: 1.80454314\n",
      "iteration: 152 loss: 3.14259148\n",
      "iteration: 153 loss: 3.25148964\n",
      "iteration: 154 loss: 1.47985506\n",
      "iteration: 155 loss: 2.57249951\n",
      "iteration: 156 loss: 0.23087317\n",
      "iteration: 157 loss: 0.89387625\n",
      "iteration: 158 loss: 0.87655264\n",
      "iteration: 159 loss: 0.37854511\n",
      "iteration: 160 loss: 0.77581489\n",
      "iteration: 161 loss: 1.66061723\n",
      "iteration: 162 loss: 0.30951512\n",
      "iteration: 163 loss: 3.23356986\n",
      "iteration: 164 loss: 3.04771018\n",
      "iteration: 165 loss: 2.48423910\n",
      "iteration: 166 loss: 0.27542993\n",
      "iteration: 167 loss: 0.19447371\n",
      "iteration: 168 loss: 0.74391013\n",
      "iteration: 169 loss: 0.64148676\n",
      "iteration: 170 loss: 3.16045046\n",
      "iteration: 171 loss: 0.90498996\n",
      "iteration: 172 loss: 0.79893434\n",
      "iteration: 173 loss: 0.15099788\n",
      "iteration: 174 loss: 3.59928775\n",
      "iteration: 175 loss: 1.71008241\n",
      "iteration: 176 loss: 2.62090778\n",
      "iteration: 177 loss: 0.30601037\n",
      "iteration: 178 loss: 0.33174554\n",
      "iteration: 179 loss: 0.20417979\n",
      "iteration: 180 loss: 0.51596618\n",
      "iteration: 181 loss: 1.05897176\n",
      "iteration: 182 loss: 0.34155750\n",
      "iteration: 183 loss: 1.79646182\n",
      "iteration: 184 loss: 0.76945710\n",
      "iteration: 185 loss: 2.31150937\n",
      "iteration: 186 loss: 1.40539300\n",
      "iteration: 187 loss: 0.92638510\n",
      "iteration: 188 loss: 2.94634938\n",
      "iteration: 189 loss: 2.09316421\n",
      "iteration: 190 loss: 1.74108243\n",
      "iteration: 191 loss: 0.86043102\n",
      "iteration: 192 loss: 0.82926679\n",
      "iteration: 193 loss: 1.15710175\n",
      "iteration: 194 loss: 0.59797436\n",
      "iteration: 195 loss: 2.95836282\n",
      "iteration: 196 loss: 3.34433126\n",
      "iteration: 197 loss: 1.03898478\n",
      "iteration: 198 loss: 0.72399020\n",
      "iteration: 199 loss: 0.55159658\n",
      "epoch:  57 mean loss training: 1.32313120\n",
      "epoch:  57 mean loss validation: 1.43054020\n",
      "iteration:   0 loss: 3.65584946\n",
      "iteration:   1 loss: 1.01697147\n",
      "iteration:   2 loss: 0.98814917\n",
      "iteration:   3 loss: 0.86760181\n",
      "iteration:   4 loss: 2.01242018\n",
      "iteration:   5 loss: 1.89292192\n",
      "iteration:   6 loss: 1.99066913\n",
      "iteration:   7 loss: 1.17038810\n",
      "iteration:   8 loss: 2.75400352\n",
      "iteration:   9 loss: 3.55796504\n",
      "iteration:  10 loss: 1.14796734\n",
      "iteration:  11 loss: 0.49350572\n",
      "iteration:  12 loss: 1.24488556\n",
      "iteration:  13 loss: 1.23957837\n",
      "iteration:  14 loss: 1.05423129\n",
      "iteration:  15 loss: 0.69970500\n",
      "iteration:  16 loss: 1.42326450\n",
      "iteration:  17 loss: 1.15929115\n",
      "iteration:  18 loss: 1.62644053\n",
      "iteration:  19 loss: 0.82212216\n",
      "iteration:  20 loss: 0.77522498\n",
      "iteration:  21 loss: 0.79703856\n",
      "iteration:  22 loss: 0.30005690\n",
      "iteration:  23 loss: 1.07773781\n",
      "iteration:  24 loss: 0.21204126\n",
      "iteration:  25 loss: 2.64084148\n",
      "iteration:  26 loss: 1.26603460\n",
      "iteration:  27 loss: 2.73620820\n",
      "iteration:  28 loss: 0.19412394\n",
      "iteration:  29 loss: 0.59245086\n",
      "iteration:  30 loss: 0.45368212\n",
      "iteration:  31 loss: 0.22256288\n",
      "iteration:  32 loss: 0.30811021\n",
      "iteration:  33 loss: 0.39717719\n",
      "iteration:  34 loss: 2.24551797\n",
      "iteration:  35 loss: 1.12270665\n",
      "iteration:  36 loss: 0.14084005\n",
      "iteration:  37 loss: 0.24546854\n",
      "iteration:  38 loss: 1.16579187\n",
      "iteration:  39 loss: 3.26638913\n",
      "iteration:  40 loss: 1.96317327\n",
      "iteration:  41 loss: 0.74573839\n",
      "iteration:  42 loss: 1.08563113\n",
      "iteration:  43 loss: 3.13334465\n",
      "iteration:  44 loss: 0.40203002\n",
      "iteration:  45 loss: 0.15504971\n",
      "iteration:  46 loss: 2.04994106\n",
      "iteration:  47 loss: 0.72882485\n",
      "iteration:  48 loss: 0.73343313\n",
      "iteration:  49 loss: 0.45779243\n",
      "iteration:  50 loss: 2.54921389\n",
      "iteration:  51 loss: 0.29689804\n",
      "iteration:  52 loss: 0.96773452\n",
      "iteration:  53 loss: 0.18935946\n",
      "iteration:  54 loss: 0.59019047\n",
      "iteration:  55 loss: 4.66022491\n",
      "iteration:  56 loss: 0.68302166\n",
      "iteration:  57 loss: 2.63322663\n",
      "iteration:  58 loss: 3.39629078\n",
      "iteration:  59 loss: 1.42697871\n",
      "iteration:  60 loss: 0.18169723\n",
      "iteration:  61 loss: 0.15748686\n",
      "iteration:  62 loss: 0.28213337\n",
      "iteration:  63 loss: 0.40253964\n",
      "iteration:  64 loss: 0.33008447\n",
      "iteration:  65 loss: 1.95479429\n",
      "iteration:  66 loss: 0.45392492\n",
      "iteration:  67 loss: 0.18430686\n",
      "iteration:  68 loss: 0.15676674\n",
      "iteration:  69 loss: 0.31307897\n",
      "iteration:  70 loss: 0.21849801\n",
      "iteration:  71 loss: 0.73395771\n",
      "iteration:  72 loss: 0.90472782\n",
      "iteration:  73 loss: 3.27097392\n",
      "iteration:  74 loss: 1.00308526\n",
      "iteration:  75 loss: 0.68881202\n",
      "iteration:  76 loss: 0.80008918\n",
      "iteration:  77 loss: 0.73479980\n",
      "iteration:  78 loss: 1.63951087\n",
      "iteration:  79 loss: 0.77482009\n",
      "iteration:  80 loss: 1.95615041\n",
      "iteration:  81 loss: 0.18657079\n",
      "iteration:  82 loss: 0.81811553\n",
      "iteration:  83 loss: 0.35715675\n",
      "iteration:  84 loss: 3.18095636\n",
      "iteration:  85 loss: 0.17957814\n",
      "iteration:  86 loss: 0.69225681\n",
      "iteration:  87 loss: 3.31805491\n",
      "iteration:  88 loss: 0.24522524\n",
      "iteration:  89 loss: 2.69004250\n",
      "iteration:  90 loss: 1.04412091\n",
      "iteration:  91 loss: 0.36169907\n",
      "iteration:  92 loss: 0.59449035\n",
      "iteration:  93 loss: 1.37990034\n",
      "iteration:  94 loss: 1.05599344\n",
      "iteration:  95 loss: 0.80678856\n",
      "iteration:  96 loss: 4.18630600\n",
      "iteration:  97 loss: 0.67063385\n",
      "iteration:  98 loss: 0.63572687\n",
      "iteration:  99 loss: 0.37326181\n",
      "iteration: 100 loss: 0.48965105\n",
      "iteration: 101 loss: 0.23625791\n",
      "iteration: 102 loss: 0.96739179\n",
      "iteration: 103 loss: 2.64138269\n",
      "iteration: 104 loss: 0.67997342\n",
      "iteration: 105 loss: 0.78355062\n",
      "iteration: 106 loss: 0.25927910\n",
      "iteration: 107 loss: 0.83991897\n",
      "iteration: 108 loss: 1.51297677\n",
      "iteration: 109 loss: 0.44234374\n",
      "iteration: 110 loss: 2.48004723\n",
      "iteration: 111 loss: 3.12048984\n",
      "iteration: 112 loss: 3.10238910\n",
      "iteration: 113 loss: 1.84405267\n",
      "iteration: 114 loss: 2.21051693\n",
      "iteration: 115 loss: 3.38891768\n",
      "iteration: 116 loss: 2.52514124\n",
      "iteration: 117 loss: 0.19406018\n",
      "iteration: 118 loss: 0.56401789\n",
      "iteration: 119 loss: 0.61495006\n",
      "iteration: 120 loss: 0.43230030\n",
      "iteration: 121 loss: 0.75955462\n",
      "iteration: 122 loss: 0.63160491\n",
      "iteration: 123 loss: 2.65193486\n",
      "iteration: 124 loss: 1.74365544\n",
      "iteration: 125 loss: 2.45646191\n",
      "iteration: 126 loss: 2.99371290\n",
      "iteration: 127 loss: 0.48152462\n",
      "iteration: 128 loss: 1.59288538\n",
      "iteration: 129 loss: 0.32002351\n",
      "iteration: 130 loss: 2.84328008\n",
      "iteration: 131 loss: 2.07676125\n",
      "iteration: 132 loss: 1.11181986\n",
      "iteration: 133 loss: 0.49378723\n",
      "iteration: 134 loss: 0.48437670\n",
      "iteration: 135 loss: 2.17966223\n",
      "iteration: 136 loss: 1.02569532\n",
      "iteration: 137 loss: 1.35614431\n",
      "iteration: 138 loss: 1.36325669\n",
      "iteration: 139 loss: 1.09651637\n",
      "iteration: 140 loss: 1.39870405\n",
      "iteration: 141 loss: 1.15774930\n",
      "iteration: 142 loss: 0.86496484\n",
      "iteration: 143 loss: 1.32743645\n",
      "iteration: 144 loss: 3.59421659\n",
      "iteration: 145 loss: 2.35388517\n",
      "iteration: 146 loss: 0.52667797\n",
      "iteration: 147 loss: 0.99713647\n",
      "iteration: 148 loss: 1.04090190\n",
      "iteration: 149 loss: 1.20215714\n",
      "iteration: 150 loss: 1.79072654\n",
      "iteration: 151 loss: 2.35022116\n",
      "iteration: 152 loss: 3.33788443\n",
      "iteration: 153 loss: 3.22197533\n",
      "iteration: 154 loss: 1.76583147\n",
      "iteration: 155 loss: 2.85870671\n",
      "iteration: 156 loss: 0.20639057\n",
      "iteration: 157 loss: 0.82503176\n",
      "iteration: 158 loss: 0.73710972\n",
      "iteration: 159 loss: 0.37960315\n",
      "iteration: 160 loss: 0.94098705\n",
      "iteration: 161 loss: 1.32396364\n",
      "iteration: 162 loss: 0.30852073\n",
      "iteration: 163 loss: 3.22425294\n",
      "iteration: 164 loss: 3.27540040\n",
      "iteration: 165 loss: 1.24718654\n",
      "iteration: 166 loss: 0.65937275\n",
      "iteration: 167 loss: 0.16767886\n",
      "iteration: 168 loss: 0.99191850\n",
      "iteration: 169 loss: 0.75912988\n",
      "iteration: 170 loss: 2.42321181\n",
      "iteration: 171 loss: 0.62851715\n",
      "iteration: 172 loss: 0.63389772\n",
      "iteration: 173 loss: 0.10585143\n",
      "iteration: 174 loss: 3.65438461\n",
      "iteration: 175 loss: 1.90260029\n",
      "iteration: 176 loss: 1.80240726\n",
      "iteration: 177 loss: 0.97746336\n",
      "iteration: 178 loss: 0.26831645\n",
      "iteration: 179 loss: 0.28295985\n",
      "iteration: 180 loss: 0.55159372\n",
      "iteration: 181 loss: 0.95488137\n",
      "iteration: 182 loss: 0.26473081\n",
      "iteration: 183 loss: 1.60432768\n",
      "iteration: 184 loss: 0.63431031\n",
      "iteration: 185 loss: 1.94951797\n",
      "iteration: 186 loss: 1.38725007\n",
      "iteration: 187 loss: 0.61370045\n",
      "iteration: 188 loss: 3.94467497\n",
      "iteration: 189 loss: 2.06671715\n",
      "iteration: 190 loss: 3.08314824\n",
      "iteration: 191 loss: 0.64300680\n",
      "iteration: 192 loss: 0.67268473\n",
      "iteration: 193 loss: 1.99439633\n",
      "iteration: 194 loss: 0.69286877\n",
      "iteration: 195 loss: 2.97304797\n",
      "iteration: 196 loss: 3.06107187\n",
      "iteration: 197 loss: 0.76924115\n",
      "iteration: 198 loss: 0.31415722\n",
      "iteration: 199 loss: 0.81442899\n",
      "epoch:  58 mean loss training: 1.31621230\n",
      "epoch:  58 mean loss validation: 1.34986854\n",
      "iteration:   0 loss: 3.01759338\n",
      "iteration:   1 loss: 1.68473291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:   2 loss: 0.94323701\n",
      "iteration:   3 loss: 0.66763419\n",
      "iteration:   4 loss: 2.77494812\n",
      "iteration:   5 loss: 2.76352572\n",
      "iteration:   6 loss: 1.76128757\n",
      "iteration:   7 loss: 1.32092619\n",
      "iteration:   8 loss: 2.88722563\n",
      "iteration:   9 loss: 3.55486321\n",
      "iteration:  10 loss: 0.83167386\n",
      "iteration:  11 loss: 0.45355159\n",
      "iteration:  12 loss: 1.05796778\n",
      "iteration:  13 loss: 1.32406652\n",
      "iteration:  14 loss: 1.07239568\n",
      "iteration:  15 loss: 0.62281811\n",
      "iteration:  16 loss: 1.13934982\n",
      "iteration:  17 loss: 1.32872534\n",
      "iteration:  18 loss: 1.60547638\n",
      "iteration:  19 loss: 0.80297434\n",
      "iteration:  20 loss: 0.88200825\n",
      "iteration:  21 loss: 0.74619931\n",
      "iteration:  22 loss: 0.42391804\n",
      "iteration:  23 loss: 1.05031943\n",
      "iteration:  24 loss: 0.31816107\n",
      "iteration:  25 loss: 2.59190631\n",
      "iteration:  26 loss: 1.42166018\n",
      "iteration:  27 loss: 3.00780392\n",
      "iteration:  28 loss: 0.22029182\n",
      "iteration:  29 loss: 0.70433003\n",
      "iteration:  30 loss: 0.43117681\n",
      "iteration:  31 loss: 0.27046847\n",
      "iteration:  32 loss: 0.34138811\n",
      "iteration:  33 loss: 0.38982815\n",
      "iteration:  34 loss: 2.20791125\n",
      "iteration:  35 loss: 1.07206357\n",
      "iteration:  36 loss: 0.12565629\n",
      "iteration:  37 loss: 0.24530749\n",
      "iteration:  38 loss: 1.16390562\n",
      "iteration:  39 loss: 3.30042028\n",
      "iteration:  40 loss: 2.06818438\n",
      "iteration:  41 loss: 1.26109087\n",
      "iteration:  42 loss: 0.92675036\n",
      "iteration:  43 loss: 3.13051319\n",
      "iteration:  44 loss: 0.34965032\n",
      "iteration:  45 loss: 0.15949354\n",
      "iteration:  46 loss: 2.04960132\n",
      "iteration:  47 loss: 0.78250140\n",
      "iteration:  48 loss: 0.73485720\n",
      "iteration:  49 loss: 0.45364419\n",
      "iteration:  50 loss: 2.48157120\n",
      "iteration:  51 loss: 0.31859791\n",
      "iteration:  52 loss: 0.89716125\n",
      "iteration:  53 loss: 0.19050975\n",
      "iteration:  54 loss: 0.54345554\n",
      "iteration:  55 loss: 4.61765051\n",
      "iteration:  56 loss: 0.68735915\n",
      "iteration:  57 loss: 2.63767982\n",
      "iteration:  58 loss: 3.37454295\n",
      "iteration:  59 loss: 1.46477270\n",
      "iteration:  60 loss: 0.17643678\n",
      "iteration:  61 loss: 0.16125870\n",
      "iteration:  62 loss: 0.28785124\n",
      "iteration:  63 loss: 0.37564093\n",
      "iteration:  64 loss: 0.29306248\n",
      "iteration:  65 loss: 2.05417585\n",
      "iteration:  66 loss: 0.44102857\n",
      "iteration:  67 loss: 0.22226261\n",
      "iteration:  68 loss: 0.15885632\n",
      "iteration:  69 loss: 0.27656373\n",
      "iteration:  70 loss: 0.23998506\n",
      "iteration:  71 loss: 0.58691674\n",
      "iteration:  72 loss: 0.71656841\n",
      "iteration:  73 loss: 3.31186152\n",
      "iteration:  74 loss: 0.94602925\n",
      "iteration:  75 loss: 0.80641323\n",
      "iteration:  76 loss: 0.75149900\n",
      "iteration:  77 loss: 0.66639268\n",
      "iteration:  78 loss: 1.56059802\n",
      "iteration:  79 loss: 0.63939768\n",
      "iteration:  80 loss: 1.94714653\n",
      "iteration:  81 loss: 0.18626094\n",
      "iteration:  82 loss: 0.76810837\n",
      "iteration:  83 loss: 0.32719040\n",
      "iteration:  84 loss: 3.17328525\n",
      "iteration:  85 loss: 0.17983423\n",
      "iteration:  86 loss: 0.64263994\n",
      "iteration:  87 loss: 3.29403234\n",
      "iteration:  88 loss: 0.20837042\n",
      "iteration:  89 loss: 2.63057685\n",
      "iteration:  90 loss: 1.01795900\n",
      "iteration:  91 loss: 0.35050890\n",
      "iteration:  92 loss: 0.34065822\n",
      "iteration:  93 loss: 1.42832172\n",
      "iteration:  94 loss: 1.20205796\n",
      "iteration:  95 loss: 0.76419103\n",
      "iteration:  96 loss: 4.18365240\n",
      "iteration:  97 loss: 0.67209768\n",
      "iteration:  98 loss: 0.61448687\n",
      "iteration:  99 loss: 0.32587695\n",
      "iteration: 100 loss: 0.44871271\n",
      "iteration: 101 loss: 0.22904737\n",
      "iteration: 102 loss: 0.96509457\n",
      "iteration: 103 loss: 2.76088834\n",
      "iteration: 104 loss: 0.69345528\n",
      "iteration: 105 loss: 0.98242712\n",
      "iteration: 106 loss: 0.35171750\n",
      "iteration: 107 loss: 0.79490739\n",
      "iteration: 108 loss: 1.70638382\n",
      "iteration: 109 loss: 0.49588692\n",
      "iteration: 110 loss: 2.50549269\n",
      "iteration: 111 loss: 3.10831404\n",
      "iteration: 112 loss: 3.05966520\n",
      "iteration: 113 loss: 1.62266421\n",
      "iteration: 114 loss: 2.14790058\n",
      "iteration: 115 loss: 2.94427943\n",
      "iteration: 116 loss: 2.40992665\n",
      "iteration: 117 loss: 0.37402526\n",
      "iteration: 118 loss: 0.37180540\n",
      "iteration: 119 loss: 0.60581160\n",
      "iteration: 120 loss: 0.63457400\n",
      "iteration: 121 loss: 0.77505952\n",
      "iteration: 122 loss: 0.61226124\n",
      "iteration: 123 loss: 2.65436602\n",
      "iteration: 124 loss: 1.28686559\n",
      "iteration: 125 loss: 1.97074020\n",
      "iteration: 126 loss: 3.04883623\n",
      "iteration: 127 loss: 0.41345972\n",
      "iteration: 128 loss: 1.19366443\n",
      "iteration: 129 loss: 0.21151622\n",
      "iteration: 130 loss: 2.76283145\n",
      "iteration: 131 loss: 2.08904219\n",
      "iteration: 132 loss: 1.56187809\n",
      "iteration: 133 loss: 0.45030600\n",
      "iteration: 134 loss: 0.45934090\n",
      "iteration: 135 loss: 1.87442183\n",
      "iteration: 136 loss: 0.98728168\n",
      "iteration: 137 loss: 1.28362143\n",
      "iteration: 138 loss: 1.08315611\n",
      "iteration: 139 loss: 1.07310152\n",
      "iteration: 140 loss: 1.72292435\n",
      "iteration: 141 loss: 0.94002604\n",
      "iteration: 142 loss: 0.80250043\n",
      "iteration: 143 loss: 1.68531358\n",
      "iteration: 144 loss: 3.71481013\n",
      "iteration: 145 loss: 1.99276507\n",
      "iteration: 146 loss: 0.42441106\n",
      "iteration: 147 loss: 0.88638335\n",
      "iteration: 148 loss: 1.04172575\n",
      "iteration: 149 loss: 1.78689551\n",
      "iteration: 150 loss: 2.04654908\n",
      "iteration: 151 loss: 1.53254783\n",
      "iteration: 152 loss: 3.34477973\n",
      "iteration: 153 loss: 3.30904865\n",
      "iteration: 154 loss: 1.53417778\n",
      "iteration: 155 loss: 2.60730648\n",
      "iteration: 156 loss: 0.22706729\n",
      "iteration: 157 loss: 0.92853642\n",
      "iteration: 158 loss: 1.28725815\n",
      "iteration: 159 loss: 0.33761925\n",
      "iteration: 160 loss: 1.00295365\n",
      "iteration: 161 loss: 1.21808016\n",
      "iteration: 162 loss: 0.28202513\n",
      "iteration: 163 loss: 2.57091212\n",
      "iteration: 164 loss: 3.01091790\n",
      "iteration: 165 loss: 2.63424420\n",
      "iteration: 166 loss: 0.26112375\n",
      "iteration: 167 loss: 0.28725138\n",
      "iteration: 168 loss: 0.84925658\n",
      "iteration: 169 loss: 0.64467251\n",
      "iteration: 170 loss: 2.90272522\n",
      "iteration: 171 loss: 0.71756268\n",
      "iteration: 172 loss: 0.66529959\n",
      "iteration: 173 loss: 0.21949087\n",
      "iteration: 174 loss: 3.91724062\n",
      "iteration: 175 loss: 2.45275640\n",
      "iteration: 176 loss: 1.80328107\n",
      "iteration: 177 loss: 0.34385026\n",
      "iteration: 178 loss: 0.28382805\n",
      "iteration: 179 loss: 0.20880552\n",
      "iteration: 180 loss: 0.20605594\n",
      "iteration: 181 loss: 1.09149694\n",
      "iteration: 182 loss: 0.22811359\n",
      "iteration: 183 loss: 1.25132942\n",
      "iteration: 184 loss: 0.74756563\n",
      "iteration: 185 loss: 2.58246398\n",
      "iteration: 186 loss: 1.43619406\n",
      "iteration: 187 loss: 0.70338190\n",
      "iteration: 188 loss: 3.21213508\n",
      "iteration: 189 loss: 2.08623457\n",
      "iteration: 190 loss: 2.34243178\n",
      "iteration: 191 loss: 0.74920332\n",
      "iteration: 192 loss: 0.67864853\n",
      "iteration: 193 loss: 1.25232697\n",
      "iteration: 194 loss: 0.64091653\n",
      "iteration: 195 loss: 2.87522388\n",
      "iteration: 196 loss: 3.13238621\n",
      "iteration: 197 loss: 1.02705193\n",
      "iteration: 198 loss: 0.67530501\n",
      "iteration: 199 loss: 0.58456725\n",
      "epoch:  59 mean loss training: 1.30763006\n",
      "epoch:  59 mean loss validation: 1.34273887\n",
      "iteration:   0 loss: 3.21738243\n",
      "iteration:   1 loss: 0.93457258\n",
      "iteration:   2 loss: 0.88991779\n",
      "iteration:   3 loss: 0.70077938\n",
      "iteration:   4 loss: 2.50797224\n",
      "iteration:   5 loss: 2.35890222\n",
      "iteration:   6 loss: 1.79716444\n",
      "iteration:   7 loss: 1.29455423\n",
      "iteration:   8 loss: 2.77452660\n",
      "iteration:   9 loss: 3.45600319\n",
      "iteration:  10 loss: 0.87141639\n",
      "iteration:  11 loss: 0.45204997\n",
      "iteration:  12 loss: 1.13732016\n",
      "iteration:  13 loss: 1.26383746\n",
      "iteration:  14 loss: 1.06395149\n",
      "iteration:  15 loss: 0.63923931\n",
      "iteration:  16 loss: 1.24903834\n",
      "iteration:  17 loss: 1.14235532\n",
      "iteration:  18 loss: 1.59742367\n",
      "iteration:  19 loss: 0.96516013\n",
      "iteration:  20 loss: 0.85421854\n",
      "iteration:  21 loss: 0.97194630\n",
      "iteration:  22 loss: 0.33500436\n",
      "iteration:  23 loss: 1.09668732\n",
      "iteration:  24 loss: 0.25009468\n",
      "iteration:  25 loss: 2.20744824\n",
      "iteration:  26 loss: 1.28231239\n",
      "iteration:  27 loss: 2.85485721\n",
      "iteration:  28 loss: 0.29684594\n",
      "iteration:  29 loss: 0.50927246\n",
      "iteration:  30 loss: 0.47400549\n",
      "iteration:  31 loss: 0.27542257\n",
      "iteration:  32 loss: 0.47456297\n",
      "iteration:  33 loss: 0.37272155\n",
      "iteration:  34 loss: 2.26043487\n",
      "iteration:  35 loss: 1.23541641\n",
      "iteration:  36 loss: 0.16275954\n",
      "iteration:  37 loss: 0.25743333\n",
      "iteration:  38 loss: 1.18116152\n",
      "iteration:  39 loss: 3.29338503\n",
      "iteration:  40 loss: 1.92978275\n",
      "iteration:  41 loss: 0.90067565\n",
      "iteration:  42 loss: 1.11716259\n",
      "iteration:  43 loss: 3.10044765\n",
      "iteration:  44 loss: 0.91609544\n",
      "iteration:  45 loss: 0.16791035\n",
      "iteration:  46 loss: 2.11713004\n",
      "iteration:  47 loss: 0.68237275\n",
      "iteration:  48 loss: 0.75157577\n",
      "iteration:  49 loss: 0.62079883\n",
      "iteration:  50 loss: 2.75703526\n",
      "iteration:  51 loss: 0.32478297\n",
      "iteration:  52 loss: 0.94437706\n",
      "iteration:  53 loss: 0.19041023\n",
      "iteration:  54 loss: 0.43001831\n",
      "iteration:  55 loss: 4.00941610\n",
      "iteration:  56 loss: 0.65343595\n",
      "iteration:  57 loss: 2.87344980\n",
      "iteration:  58 loss: 2.99219251\n",
      "iteration:  59 loss: 1.04311037\n",
      "iteration:  60 loss: 0.27672738\n",
      "iteration:  61 loss: 0.16158389\n",
      "iteration:  62 loss: 0.21539207\n",
      "iteration:  63 loss: 0.59846658\n",
      "iteration:  64 loss: 0.25189337\n",
      "iteration:  65 loss: 2.35889769\n",
      "iteration:  66 loss: 0.40115538\n",
      "iteration:  67 loss: 0.20768611\n",
      "iteration:  68 loss: 0.14997764\n",
      "iteration:  69 loss: 0.27934393\n",
      "iteration:  70 loss: 0.24554233\n",
      "iteration:  71 loss: 0.72798812\n",
      "iteration:  72 loss: 0.80344880\n",
      "iteration:  73 loss: 2.79629803\n",
      "iteration:  74 loss: 0.62603265\n",
      "iteration:  75 loss: 0.43549675\n",
      "iteration:  76 loss: 0.63120914\n",
      "iteration:  77 loss: 0.57983363\n",
      "iteration:  78 loss: 1.54529846\n",
      "iteration:  79 loss: 0.56759816\n",
      "iteration:  80 loss: 1.88606870\n",
      "iteration:  81 loss: 0.17604509\n",
      "iteration:  82 loss: 0.59926671\n",
      "iteration:  83 loss: 1.05450416\n",
      "iteration:  84 loss: 3.17544699\n",
      "iteration:  85 loss: 0.18350734\n",
      "iteration:  86 loss: 0.53545392\n",
      "iteration:  87 loss: 3.27804303\n",
      "iteration:  88 loss: 0.21033336\n",
      "iteration:  89 loss: 2.55205655\n",
      "iteration:  90 loss: 1.61769235\n",
      "iteration:  91 loss: 0.40082824\n",
      "iteration:  92 loss: 0.40691251\n",
      "iteration:  93 loss: 1.71240604\n",
      "iteration:  94 loss: 0.83661187\n",
      "iteration:  95 loss: 0.77453983\n",
      "iteration:  96 loss: 4.17833090\n",
      "iteration:  97 loss: 0.87422806\n",
      "iteration:  98 loss: 0.52000618\n",
      "iteration:  99 loss: 0.20850569\n",
      "iteration: 100 loss: 0.35020581\n",
      "iteration: 101 loss: 0.29881597\n",
      "iteration: 102 loss: 0.87916607\n",
      "iteration: 103 loss: 2.92900109\n",
      "iteration: 104 loss: 0.29508296\n",
      "iteration: 105 loss: 0.77964628\n",
      "iteration: 106 loss: 0.21976872\n",
      "iteration: 107 loss: 0.68469340\n",
      "iteration: 108 loss: 1.84135616\n",
      "iteration: 109 loss: 0.54057389\n",
      "iteration: 110 loss: 2.37908340\n",
      "iteration: 111 loss: 3.10001659\n",
      "iteration: 112 loss: 2.88832664\n",
      "iteration: 113 loss: 1.11906421\n",
      "iteration: 114 loss: 1.35492384\n",
      "iteration: 115 loss: 2.36122441\n",
      "iteration: 116 loss: 2.30443668\n",
      "iteration: 117 loss: 0.19458586\n",
      "iteration: 118 loss: 0.28283903\n",
      "iteration: 119 loss: 0.55533552\n",
      "iteration: 120 loss: 0.20820287\n",
      "iteration: 121 loss: 0.77022576\n",
      "iteration: 122 loss: 0.35370845\n",
      "iteration: 123 loss: 2.42754269\n",
      "iteration: 124 loss: 1.74195576\n",
      "iteration: 125 loss: 2.79879498\n",
      "iteration: 126 loss: 3.39258218\n",
      "iteration: 127 loss: 0.59587681\n",
      "iteration: 128 loss: 1.69865870\n",
      "iteration: 129 loss: 0.28343230\n",
      "iteration: 130 loss: 2.69636774\n",
      "iteration: 131 loss: 2.33728004\n",
      "iteration: 132 loss: 1.82531846\n",
      "iteration: 133 loss: 0.45157391\n",
      "iteration: 134 loss: 0.51950312\n",
      "iteration: 135 loss: 2.13965034\n",
      "iteration: 136 loss: 0.99635816\n",
      "iteration: 137 loss: 2.22254372\n",
      "iteration: 138 loss: 1.01563537\n",
      "iteration: 139 loss: 0.99000019\n",
      "iteration: 140 loss: 1.51982236\n",
      "iteration: 141 loss: 1.04214633\n",
      "iteration: 142 loss: 0.82384282\n",
      "iteration: 143 loss: 0.69623721\n",
      "iteration: 144 loss: 3.55475473\n",
      "iteration: 145 loss: 1.99721396\n",
      "iteration: 146 loss: 0.38983679\n",
      "iteration: 147 loss: 1.13961077\n",
      "iteration: 148 loss: 0.98548412\n",
      "iteration: 149 loss: 1.55930781\n",
      "iteration: 150 loss: 2.23717380\n",
      "iteration: 151 loss: 1.48862219\n",
      "iteration: 152 loss: 3.31294036\n",
      "iteration: 153 loss: 3.56123734\n",
      "iteration: 154 loss: 1.75489557\n",
      "iteration: 155 loss: 3.01332617\n",
      "iteration: 156 loss: 0.14067110\n",
      "iteration: 157 loss: 0.82713777\n",
      "iteration: 158 loss: 0.65851933\n",
      "iteration: 159 loss: 0.39929202\n",
      "iteration: 160 loss: 0.89303970\n",
      "iteration: 161 loss: 1.02910542\n",
      "iteration: 162 loss: 0.26411042\n",
      "iteration: 163 loss: 2.72407317\n",
      "iteration: 164 loss: 3.41747451\n",
      "iteration: 165 loss: 1.95043242\n",
      "iteration: 166 loss: 0.55760890\n",
      "iteration: 167 loss: 0.21922807\n",
      "iteration: 168 loss: 0.89782816\n",
      "iteration: 169 loss: 0.40221140\n",
      "iteration: 170 loss: 2.73506188\n",
      "iteration: 171 loss: 0.60203034\n",
      "iteration: 172 loss: 0.57646608\n",
      "iteration: 173 loss: 0.14538898\n",
      "iteration: 174 loss: 3.60333228\n",
      "iteration: 175 loss: 2.16129112\n",
      "iteration: 176 loss: 1.47011077\n",
      "iteration: 177 loss: 0.80514556\n",
      "iteration: 178 loss: 0.26401639\n",
      "iteration: 179 loss: 0.26401347\n",
      "iteration: 180 loss: 0.52748799\n",
      "iteration: 181 loss: 1.06049943\n",
      "iteration: 182 loss: 0.24615155\n",
      "iteration: 183 loss: 1.25928068\n",
      "iteration: 184 loss: 0.60321546\n",
      "iteration: 185 loss: 2.01102018\n",
      "iteration: 186 loss: 1.53492785\n",
      "iteration: 187 loss: 0.67310715\n",
      "iteration: 188 loss: 3.35856104\n",
      "iteration: 189 loss: 2.14042163\n",
      "iteration: 190 loss: 2.26170015\n",
      "iteration: 191 loss: 0.70990670\n",
      "iteration: 192 loss: 0.59841263\n",
      "iteration: 193 loss: 1.29273427\n",
      "iteration: 194 loss: 0.70955813\n",
      "iteration: 195 loss: 2.94143152\n",
      "iteration: 196 loss: 3.16096401\n",
      "iteration: 197 loss: 0.97383398\n",
      "iteration: 198 loss: 0.64391226\n",
      "iteration: 199 loss: 0.87542260\n",
      "epoch:  60 mean loss training: 1.28506196\n",
      "epoch:  60 mean loss validation: 1.36190915\n",
      "iteration:   0 loss: 2.92350817\n",
      "iteration:   1 loss: 0.95639586\n",
      "iteration:   2 loss: 0.88749725\n",
      "iteration:   3 loss: 0.73872161\n",
      "iteration:   4 loss: 2.17582226\n",
      "iteration:   5 loss: 2.16767621\n",
      "iteration:   6 loss: 1.99023139\n",
      "iteration:   7 loss: 1.24276114\n",
      "iteration:   8 loss: 2.85525489\n",
      "iteration:   9 loss: 3.48432660\n",
      "iteration:  10 loss: 0.95566887\n",
      "iteration:  11 loss: 0.48173755\n",
      "iteration:  12 loss: 1.07977128\n",
      "iteration:  13 loss: 1.36429513\n",
      "iteration:  14 loss: 1.06857407\n",
      "iteration:  15 loss: 0.67823106\n",
      "iteration:  16 loss: 1.41563749\n",
      "iteration:  17 loss: 1.25595188\n",
      "iteration:  18 loss: 1.62475908\n",
      "iteration:  19 loss: 0.72913545\n",
      "iteration:  20 loss: 0.88192201\n",
      "iteration:  21 loss: 0.65608978\n",
      "iteration:  22 loss: 0.41890982\n",
      "iteration:  23 loss: 1.11539519\n",
      "iteration:  24 loss: 0.23514836\n",
      "iteration:  25 loss: 1.85845089\n",
      "iteration:  26 loss: 1.27638602\n",
      "iteration:  27 loss: 3.01201439\n",
      "iteration:  28 loss: 0.23502633\n",
      "iteration:  29 loss: 0.62576157\n",
      "iteration:  30 loss: 0.39434102\n",
      "iteration:  31 loss: 0.24623019\n",
      "iteration:  32 loss: 0.51927418\n",
      "iteration:  33 loss: 0.37583259\n",
      "iteration:  34 loss: 2.21406889\n",
      "iteration:  35 loss: 1.13565361\n",
      "iteration:  36 loss: 0.14883555\n",
      "iteration:  37 loss: 0.24839218\n",
      "iteration:  38 loss: 1.17758060\n",
      "iteration:  39 loss: 3.32470202\n",
      "iteration:  40 loss: 1.94978821\n",
      "iteration:  41 loss: 0.59730440\n",
      "iteration:  42 loss: 0.85564983\n",
      "iteration:  43 loss: 3.12188983\n",
      "iteration:  44 loss: 0.48895347\n",
      "iteration:  45 loss: 0.16479233\n",
      "iteration:  46 loss: 2.05850744\n",
      "iteration:  47 loss: 0.73744452\n",
      "iteration:  48 loss: 0.76124799\n",
      "iteration:  49 loss: 0.69416243\n",
      "iteration:  50 loss: 2.57840347\n",
      "iteration:  51 loss: 0.32512116\n",
      "iteration:  52 loss: 0.77493668\n",
      "iteration:  53 loss: 0.24794519\n",
      "iteration:  54 loss: 0.46131423\n",
      "iteration:  55 loss: 4.56178951\n",
      "iteration:  56 loss: 0.68613905\n",
      "iteration:  57 loss: 2.87578702\n",
      "iteration:  58 loss: 2.98699403\n",
      "iteration:  59 loss: 1.14850974\n",
      "iteration:  60 loss: 0.20325807\n",
      "iteration:  61 loss: 0.15397738\n",
      "iteration:  62 loss: 0.43079925\n",
      "iteration:  63 loss: 0.63850355\n",
      "iteration:  64 loss: 0.30931067\n",
      "iteration:  65 loss: 2.17965579\n",
      "iteration:  66 loss: 0.42665455\n",
      "iteration:  67 loss: 0.18645085\n",
      "iteration:  68 loss: 0.14518109\n",
      "iteration:  69 loss: 0.32896370\n",
      "iteration:  70 loss: 0.39424688\n",
      "iteration:  71 loss: 0.71242285\n",
      "iteration:  72 loss: 0.75804698\n",
      "iteration:  73 loss: 2.79668307\n",
      "iteration:  74 loss: 0.46923998\n",
      "iteration:  75 loss: 0.48863903\n",
      "iteration:  76 loss: 0.72085857\n",
      "iteration:  77 loss: 0.89585972\n",
      "iteration:  78 loss: 1.55305374\n",
      "iteration:  79 loss: 0.57859600\n",
      "iteration:  80 loss: 1.86470366\n",
      "iteration:  81 loss: 0.18696140\n",
      "iteration:  82 loss: 0.77498394\n",
      "iteration:  83 loss: 0.45146099\n",
      "iteration:  84 loss: 3.12386179\n",
      "iteration:  85 loss: 0.17843786\n",
      "iteration:  86 loss: 0.56271905\n",
      "iteration:  87 loss: 3.30680418\n",
      "iteration:  88 loss: 0.27883476\n",
      "iteration:  89 loss: 2.77334976\n",
      "iteration:  90 loss: 1.37588322\n",
      "iteration:  91 loss: 0.39275599\n",
      "iteration:  92 loss: 0.27129710\n",
      "iteration:  93 loss: 1.97123241\n",
      "iteration:  94 loss: 1.36174428\n",
      "iteration:  95 loss: 0.75149304\n",
      "iteration:  96 loss: 4.19322872\n",
      "iteration:  97 loss: 0.83068871\n",
      "iteration:  98 loss: 0.53373939\n",
      "iteration:  99 loss: 0.29676217\n",
      "iteration: 100 loss: 0.33462116\n",
      "iteration: 101 loss: 0.21327856\n",
      "iteration: 102 loss: 1.00061667\n",
      "iteration: 103 loss: 2.90312195\n",
      "iteration: 104 loss: 0.36230505\n",
      "iteration: 105 loss: 0.82158399\n",
      "iteration: 106 loss: 0.22975962\n",
      "iteration: 107 loss: 0.67668200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 108 loss: 1.71051133\n",
      "iteration: 109 loss: 0.50816840\n",
      "iteration: 110 loss: 2.85381269\n",
      "iteration: 111 loss: 3.09937763\n",
      "iteration: 112 loss: 2.93708229\n",
      "iteration: 113 loss: 1.14407516\n",
      "iteration: 114 loss: 1.42662549\n",
      "iteration: 115 loss: 2.27624059\n",
      "iteration: 116 loss: 2.21879888\n",
      "iteration: 117 loss: 0.19676466\n",
      "iteration: 118 loss: 0.32324097\n",
      "iteration: 119 loss: 0.57003951\n",
      "iteration: 120 loss: 0.20636903\n",
      "iteration: 121 loss: 0.74100548\n",
      "iteration: 122 loss: 0.39405370\n",
      "iteration: 123 loss: 2.44944620\n",
      "iteration: 124 loss: 1.27227092\n",
      "iteration: 125 loss: 2.04096174\n",
      "iteration: 126 loss: 3.24880600\n",
      "iteration: 127 loss: 0.37319943\n",
      "iteration: 128 loss: 1.23749745\n",
      "iteration: 129 loss: 0.21115758\n",
      "iteration: 130 loss: 2.79505873\n",
      "iteration: 131 loss: 2.26139307\n",
      "iteration: 132 loss: 1.40135276\n",
      "iteration: 133 loss: 0.44175515\n",
      "iteration: 134 loss: 0.45818400\n",
      "iteration: 135 loss: 2.06539559\n",
      "iteration: 136 loss: 0.98652685\n",
      "iteration: 137 loss: 1.33695483\n",
      "iteration: 138 loss: 1.18097031\n",
      "iteration: 139 loss: 1.10894895\n",
      "iteration: 140 loss: 1.46094549\n",
      "iteration: 141 loss: 1.03950489\n",
      "iteration: 142 loss: 0.88017148\n",
      "iteration: 143 loss: 0.96095365\n",
      "iteration: 144 loss: 3.72712207\n",
      "iteration: 145 loss: 2.12156940\n",
      "iteration: 146 loss: 0.54532361\n",
      "iteration: 147 loss: 0.91648287\n",
      "iteration: 148 loss: 1.13282013\n",
      "iteration: 149 loss: 1.32874811\n",
      "iteration: 150 loss: 2.11342406\n",
      "iteration: 151 loss: 1.50122213\n",
      "iteration: 152 loss: 3.37603617\n",
      "iteration: 153 loss: 3.24829483\n",
      "iteration: 154 loss: 1.56683171\n",
      "iteration: 155 loss: 2.76095676\n",
      "iteration: 156 loss: 0.18181401\n",
      "iteration: 157 loss: 0.83389813\n",
      "iteration: 158 loss: 1.24236643\n",
      "iteration: 159 loss: 0.42014873\n",
      "iteration: 160 loss: 1.01249290\n",
      "iteration: 161 loss: 1.25033689\n",
      "iteration: 162 loss: 0.29769084\n",
      "iteration: 163 loss: 2.88364506\n",
      "iteration: 164 loss: 3.01147294\n",
      "iteration: 165 loss: 1.31797588\n",
      "iteration: 166 loss: 0.24190241\n",
      "iteration: 167 loss: 0.23465791\n",
      "iteration: 168 loss: 0.89004034\n",
      "iteration: 169 loss: 0.82911032\n",
      "iteration: 170 loss: 2.89755130\n",
      "iteration: 171 loss: 1.04161179\n",
      "iteration: 172 loss: 0.65118688\n",
      "iteration: 173 loss: 0.17395334\n",
      "iteration: 174 loss: 3.48190284\n",
      "iteration: 175 loss: 1.76254475\n",
      "iteration: 176 loss: 1.75454259\n",
      "iteration: 177 loss: 0.36785945\n",
      "iteration: 178 loss: 0.25222421\n",
      "iteration: 179 loss: 0.12604879\n",
      "iteration: 180 loss: 0.20526271\n",
      "iteration: 181 loss: 1.02115512\n",
      "iteration: 182 loss: 0.19995494\n",
      "iteration: 183 loss: 0.87565571\n",
      "iteration: 184 loss: 0.60154027\n",
      "iteration: 185 loss: 2.08504987\n",
      "iteration: 186 loss: 1.73014235\n",
      "iteration: 187 loss: 0.60427004\n",
      "iteration: 188 loss: 3.94538689\n",
      "iteration: 189 loss: 2.05657291\n",
      "iteration: 190 loss: 2.73268127\n",
      "iteration: 191 loss: 0.57490766\n",
      "iteration: 192 loss: 0.68422633\n",
      "iteration: 193 loss: 2.13671398\n",
      "iteration: 194 loss: 0.65133828\n",
      "iteration: 195 loss: 2.85193014\n",
      "iteration: 196 loss: 2.95523286\n",
      "iteration: 197 loss: 0.76261765\n",
      "iteration: 198 loss: 0.25694442\n",
      "iteration: 199 loss: 0.60866898\n",
      "epoch:  61 mean loss training: 1.26749825\n",
      "epoch:  61 mean loss validation: 1.32186043\n",
      "iteration:   0 loss: 2.88607645\n",
      "iteration:   1 loss: 1.76504481\n",
      "iteration:   2 loss: 0.85539532\n",
      "iteration:   3 loss: 0.61544949\n",
      "iteration:   4 loss: 2.84544873\n",
      "iteration:   5 loss: 2.71833467\n",
      "iteration:   6 loss: 1.48273671\n",
      "iteration:   7 loss: 1.43055964\n",
      "iteration:   8 loss: 2.72441101\n",
      "iteration:   9 loss: 3.44427109\n",
      "iteration:  10 loss: 0.73121274\n",
      "iteration:  11 loss: 0.40357766\n",
      "iteration:  12 loss: 1.27920461\n",
      "iteration:  13 loss: 1.34937334\n",
      "iteration:  14 loss: 1.09798717\n",
      "iteration:  15 loss: 0.57746100\n",
      "iteration:  16 loss: 1.10832071\n",
      "iteration:  17 loss: 1.38525915\n",
      "iteration:  18 loss: 1.70746386\n",
      "iteration:  19 loss: 0.83033454\n",
      "iteration:  20 loss: 0.98022246\n",
      "iteration:  21 loss: 0.90491670\n",
      "iteration:  22 loss: 0.52348882\n",
      "iteration:  23 loss: 1.01558769\n",
      "iteration:  24 loss: 0.32926023\n",
      "iteration:  25 loss: 2.89657354\n",
      "iteration:  26 loss: 1.76336527\n",
      "iteration:  27 loss: 3.07919478\n",
      "iteration:  28 loss: 0.22430682\n",
      "iteration:  29 loss: 0.56710142\n",
      "iteration:  30 loss: 0.45901766\n",
      "iteration:  31 loss: 0.29329544\n",
      "iteration:  32 loss: 0.19712813\n",
      "iteration:  33 loss: 0.33490857\n",
      "iteration:  34 loss: 2.25890207\n",
      "iteration:  35 loss: 1.02748692\n",
      "iteration:  36 loss: 0.12343648\n",
      "iteration:  37 loss: 0.24786213\n",
      "iteration:  38 loss: 1.15798199\n",
      "iteration:  39 loss: 3.28674102\n",
      "iteration:  40 loss: 1.83193290\n",
      "iteration:  41 loss: 1.25440812\n",
      "iteration:  42 loss: 0.92106432\n",
      "iteration:  43 loss: 3.11698222\n",
      "iteration:  44 loss: 0.32850048\n",
      "iteration:  45 loss: 0.15854242\n",
      "iteration:  46 loss: 1.80090141\n",
      "iteration:  47 loss: 0.75750059\n",
      "iteration:  48 loss: 0.71604520\n",
      "iteration:  49 loss: 0.40932137\n",
      "iteration:  50 loss: 2.09125805\n",
      "iteration:  51 loss: 0.28697166\n",
      "iteration:  52 loss: 0.91483545\n",
      "iteration:  53 loss: 0.35736269\n",
      "iteration:  54 loss: 0.69277072\n",
      "iteration:  55 loss: 3.35708714\n",
      "iteration:  56 loss: 0.89483953\n",
      "iteration:  57 loss: 2.64922118\n",
      "iteration:  58 loss: 3.46936226\n",
      "iteration:  59 loss: 2.03235722\n",
      "iteration:  60 loss: 0.38082907\n",
      "iteration:  61 loss: 0.36287850\n",
      "iteration:  62 loss: 0.56843138\n",
      "iteration:  63 loss: 0.59790671\n",
      "iteration:  64 loss: 0.25903314\n",
      "iteration:  65 loss: 0.61174911\n",
      "iteration:  66 loss: 0.46347076\n",
      "iteration:  67 loss: 0.16621381\n",
      "iteration:  68 loss: 0.14408185\n",
      "iteration:  69 loss: 0.29035786\n",
      "iteration:  70 loss: 0.21758617\n",
      "iteration:  71 loss: 0.52031237\n",
      "iteration:  72 loss: 0.95610607\n",
      "iteration:  73 loss: 3.80486369\n",
      "iteration:  74 loss: 0.50939637\n",
      "iteration:  75 loss: 0.43852150\n",
      "iteration:  76 loss: 0.68958652\n",
      "iteration:  77 loss: 0.58348352\n",
      "iteration:  78 loss: 1.65132368\n",
      "iteration:  79 loss: 1.48366821\n",
      "iteration:  80 loss: 1.55397987\n",
      "iteration:  81 loss: 0.11621334\n",
      "iteration:  82 loss: 0.69103038\n",
      "iteration:  83 loss: 0.69894302\n",
      "iteration:  84 loss: 3.62710381\n",
      "iteration:  85 loss: 0.14713547\n",
      "iteration:  86 loss: 0.51541430\n",
      "iteration:  87 loss: 3.27311850\n",
      "iteration:  88 loss: 0.16063243\n",
      "iteration:  89 loss: 2.71144652\n",
      "iteration:  90 loss: 1.25250542\n",
      "iteration:  91 loss: 0.38293937\n",
      "iteration:  92 loss: 0.13640065\n",
      "iteration:  93 loss: 2.78661203\n",
      "iteration:  94 loss: 1.71302748\n",
      "iteration:  95 loss: 0.21276745\n",
      "iteration:  96 loss: 4.91911030\n",
      "iteration:  97 loss: 0.72299427\n",
      "iteration:  98 loss: 0.54572928\n",
      "iteration:  99 loss: 0.16352648\n",
      "iteration: 100 loss: 0.34269303\n",
      "iteration: 101 loss: 0.17750065\n",
      "iteration: 102 loss: 0.70326370\n",
      "iteration: 103 loss: 2.59562588\n",
      "iteration: 104 loss: 0.66465735\n",
      "iteration: 105 loss: 0.71808034\n",
      "iteration: 106 loss: 0.19521011\n",
      "iteration: 107 loss: 1.31031728\n",
      "iteration: 108 loss: 1.88661015\n",
      "iteration: 109 loss: 0.56114054\n",
      "iteration: 110 loss: 3.12682414\n",
      "iteration: 111 loss: 3.45904660\n",
      "iteration: 112 loss: 3.01325583\n",
      "iteration: 113 loss: 1.32029104\n",
      "iteration: 114 loss: 1.03874612\n",
      "iteration: 115 loss: 1.81212294\n",
      "iteration: 116 loss: 2.29131007\n",
      "iteration: 117 loss: 0.19851798\n",
      "iteration: 118 loss: 0.44500646\n",
      "iteration: 119 loss: 0.61572570\n",
      "iteration: 120 loss: 0.22320469\n",
      "iteration: 121 loss: 0.83765459\n",
      "iteration: 122 loss: 0.40249044\n",
      "iteration: 123 loss: 2.55097556\n",
      "iteration: 124 loss: 1.27138293\n",
      "iteration: 125 loss: 2.35254979\n",
      "iteration: 126 loss: 2.84628177\n",
      "iteration: 127 loss: 0.36137417\n",
      "iteration: 128 loss: 1.60279536\n",
      "iteration: 129 loss: 0.24961823\n",
      "iteration: 130 loss: 2.77941847\n",
      "iteration: 131 loss: 1.81155813\n",
      "iteration: 132 loss: 0.72008824\n",
      "iteration: 133 loss: 0.41659015\n",
      "iteration: 134 loss: 0.47711068\n",
      "iteration: 135 loss: 2.13070440\n",
      "iteration: 136 loss: 1.22874928\n",
      "iteration: 137 loss: 1.21841323\n",
      "iteration: 138 loss: 1.68594146\n",
      "iteration: 139 loss: 1.24212337\n",
      "iteration: 140 loss: 0.95602471\n",
      "iteration: 141 loss: 1.44886231\n",
      "iteration: 142 loss: 1.15530050\n",
      "iteration: 143 loss: 0.66679186\n",
      "iteration: 144 loss: 3.63562322\n",
      "iteration: 145 loss: 1.41486287\n",
      "iteration: 146 loss: 0.55353719\n",
      "iteration: 147 loss: 1.25093496\n",
      "iteration: 148 loss: 1.27464485\n",
      "iteration: 149 loss: 0.80803013\n",
      "iteration: 150 loss: 2.28371263\n",
      "iteration: 151 loss: 2.32241368\n",
      "iteration: 152 loss: 3.59526920\n",
      "iteration: 153 loss: 3.23616552\n",
      "iteration: 154 loss: 2.16680455\n",
      "iteration: 155 loss: 2.85889435\n",
      "iteration: 156 loss: 0.42552522\n",
      "iteration: 157 loss: 0.92482311\n",
      "iteration: 158 loss: 1.22493410\n",
      "iteration: 159 loss: 0.45746061\n",
      "iteration: 160 loss: 1.22438741\n",
      "iteration: 161 loss: 0.90761447\n",
      "iteration: 162 loss: 0.26845726\n",
      "iteration: 163 loss: 3.53354931\n",
      "iteration: 164 loss: 3.24231625\n",
      "iteration: 165 loss: 2.37137151\n",
      "iteration: 166 loss: 0.26635951\n",
      "iteration: 167 loss: 0.15512039\n",
      "iteration: 168 loss: 0.81133115\n",
      "iteration: 169 loss: 0.34710622\n",
      "iteration: 170 loss: 3.06992912\n",
      "iteration: 171 loss: 0.47411957\n",
      "iteration: 172 loss: 0.65030158\n",
      "iteration: 173 loss: 0.13668738\n",
      "iteration: 174 loss: 3.73322964\n",
      "iteration: 175 loss: 2.00813985\n",
      "iteration: 176 loss: 2.27559519\n",
      "iteration: 177 loss: 0.49558079\n",
      "iteration: 178 loss: 0.31461367\n",
      "iteration: 179 loss: 0.12929295\n",
      "iteration: 180 loss: 0.21874936\n",
      "iteration: 181 loss: 1.12505233\n",
      "iteration: 182 loss: 0.24269100\n",
      "iteration: 183 loss: 0.88707513\n",
      "iteration: 184 loss: 0.49560195\n",
      "iteration: 185 loss: 2.70455050\n",
      "iteration: 186 loss: 1.26012015\n",
      "iteration: 187 loss: 0.86960030\n",
      "iteration: 188 loss: 3.81934404\n",
      "iteration: 189 loss: 2.16956329\n",
      "iteration: 190 loss: 2.23406959\n",
      "iteration: 191 loss: 0.48591810\n",
      "iteration: 192 loss: 0.58904237\n",
      "iteration: 193 loss: 1.88770306\n",
      "iteration: 194 loss: 0.59077024\n",
      "iteration: 195 loss: 3.31706023\n",
      "iteration: 196 loss: 2.94607949\n",
      "iteration: 197 loss: 0.76411605\n",
      "iteration: 198 loss: 0.22194023\n",
      "iteration: 199 loss: 0.45293590\n",
      "epoch:  62 mean loss training: 1.30611539\n",
      "epoch:  62 mean loss validation: 1.35038304\n",
      "iteration:   0 loss: 2.54766154\n",
      "iteration:   1 loss: 1.83472824\n",
      "iteration:   2 loss: 0.77020341\n",
      "iteration:   3 loss: 0.65672851\n",
      "iteration:   4 loss: 3.28273821\n",
      "iteration:   5 loss: 2.85134888\n",
      "iteration:   6 loss: 1.63279510\n",
      "iteration:   7 loss: 1.19590366\n",
      "iteration:   8 loss: 2.91365838\n",
      "iteration:   9 loss: 3.47164845\n",
      "iteration:  10 loss: 0.65427107\n",
      "iteration:  11 loss: 0.39466977\n",
      "iteration:  12 loss: 1.07975233\n",
      "iteration:  13 loss: 1.06055295\n",
      "iteration:  14 loss: 1.03274024\n",
      "iteration:  15 loss: 0.56093693\n",
      "iteration:  16 loss: 0.93498963\n",
      "iteration:  17 loss: 1.01673210\n",
      "iteration:  18 loss: 1.68625259\n",
      "iteration:  19 loss: 0.59935701\n",
      "iteration:  20 loss: 0.82467723\n",
      "iteration:  21 loss: 0.60638255\n",
      "iteration:  22 loss: 0.32306319\n",
      "iteration:  23 loss: 1.05845094\n",
      "iteration:  24 loss: 0.18725929\n",
      "iteration:  25 loss: 2.45550990\n",
      "iteration:  26 loss: 1.27775073\n",
      "iteration:  27 loss: 2.68855667\n",
      "iteration:  28 loss: 0.21538696\n",
      "iteration:  29 loss: 0.62409955\n",
      "iteration:  30 loss: 0.40328878\n",
      "iteration:  31 loss: 0.23445013\n",
      "iteration:  32 loss: 0.32892710\n",
      "iteration:  33 loss: 0.36376074\n",
      "iteration:  34 loss: 2.26428485\n",
      "iteration:  35 loss: 1.21892738\n",
      "iteration:  36 loss: 0.12783143\n",
      "iteration:  37 loss: 0.25400254\n",
      "iteration:  38 loss: 1.18197834\n",
      "iteration:  39 loss: 3.27179599\n",
      "iteration:  40 loss: 1.81434584\n",
      "iteration:  41 loss: 0.74159074\n",
      "iteration:  42 loss: 0.85962379\n",
      "iteration:  43 loss: 3.21458006\n",
      "iteration:  44 loss: 0.51824802\n",
      "iteration:  45 loss: 0.15486324\n",
      "iteration:  46 loss: 1.79317951\n",
      "iteration:  47 loss: 0.81350654\n",
      "iteration:  48 loss: 0.76017410\n",
      "iteration:  49 loss: 0.52249134\n",
      "iteration:  50 loss: 2.35164380\n",
      "iteration:  51 loss: 0.36855486\n",
      "iteration:  52 loss: 0.98217118\n",
      "iteration:  53 loss: 0.75145346\n",
      "iteration:  54 loss: 0.64105844\n",
      "iteration:  55 loss: 4.07040644\n",
      "iteration:  56 loss: 1.18123615\n",
      "iteration:  57 loss: 2.91687322\n",
      "iteration:  58 loss: 3.16844583\n",
      "iteration:  59 loss: 2.28078198\n",
      "iteration:  60 loss: 0.62305695\n",
      "iteration:  61 loss: 0.44168165\n",
      "iteration:  62 loss: 0.58973229\n",
      "iteration:  63 loss: 1.08063376\n",
      "iteration:  64 loss: 0.29165176\n",
      "iteration:  65 loss: 0.99414963\n",
      "iteration:  66 loss: 0.42174694\n",
      "iteration:  67 loss: 0.25698644\n",
      "iteration:  68 loss: 0.17093590\n",
      "iteration:  69 loss: 0.67224169\n",
      "iteration:  70 loss: 0.65744889\n",
      "iteration:  71 loss: 0.49484703\n",
      "iteration:  72 loss: 0.85427743\n",
      "iteration:  73 loss: 3.21552062\n",
      "iteration:  74 loss: 0.56563151\n",
      "iteration:  75 loss: 0.81876945\n",
      "iteration:  76 loss: 0.75010878\n",
      "iteration:  77 loss: 0.56939161\n",
      "iteration:  78 loss: 1.66213441\n",
      "iteration:  79 loss: 0.57386917\n",
      "iteration:  80 loss: 1.82508910\n",
      "iteration:  81 loss: 0.24204463\n",
      "iteration:  82 loss: 0.57577497\n",
      "iteration:  83 loss: 0.82897049\n",
      "iteration:  84 loss: 3.21440792\n",
      "iteration:  85 loss: 0.16242842\n",
      "iteration:  86 loss: 0.56754595\n",
      "iteration:  87 loss: 3.33476925\n",
      "iteration:  88 loss: 0.29071835\n",
      "iteration:  89 loss: 2.81207514\n",
      "iteration:  90 loss: 1.71092689\n",
      "iteration:  91 loss: 0.37101233\n",
      "iteration:  92 loss: 0.27324319\n",
      "iteration:  93 loss: 2.14706135\n",
      "iteration:  94 loss: 1.28358305\n",
      "iteration:  95 loss: 0.73671848\n",
      "iteration:  96 loss: 3.52133250\n",
      "iteration:  97 loss: 0.78163707\n",
      "iteration:  98 loss: 0.56213760\n",
      "iteration:  99 loss: 0.30040881\n",
      "iteration: 100 loss: 0.46527597\n",
      "iteration: 101 loss: 0.24650872\n",
      "iteration: 102 loss: 0.96582365\n",
      "iteration: 103 loss: 2.93911242\n",
      "iteration: 104 loss: 0.38737777\n",
      "iteration: 105 loss: 0.86158949\n",
      "iteration: 106 loss: 0.51795387\n",
      "iteration: 107 loss: 0.49678490\n",
      "iteration: 108 loss: 1.44176459\n",
      "iteration: 109 loss: 0.44538280\n",
      "iteration: 110 loss: 2.13656616\n",
      "iteration: 111 loss: 3.10403514\n",
      "iteration: 112 loss: 2.95000720\n",
      "iteration: 113 loss: 0.48790729\n",
      "iteration: 114 loss: 0.78375614\n",
      "iteration: 115 loss: 2.37296009\n",
      "iteration: 116 loss: 2.11862993\n",
      "iteration: 117 loss: 0.54310709\n",
      "iteration: 118 loss: 0.55483884\n",
      "iteration: 119 loss: 0.55610812\n",
      "iteration: 120 loss: 0.46888116\n",
      "iteration: 121 loss: 0.60130316\n",
      "iteration: 122 loss: 0.26553711\n",
      "iteration: 123 loss: 2.50786567\n",
      "iteration: 124 loss: 1.30943000\n",
      "iteration: 125 loss: 1.81232345\n",
      "iteration: 126 loss: 3.22054887\n",
      "iteration: 127 loss: 0.90519953\n",
      "iteration: 128 loss: 1.79673278\n",
      "iteration: 129 loss: 0.41676244\n",
      "iteration: 130 loss: 2.64336848\n",
      "iteration: 131 loss: 2.04761147\n",
      "iteration: 132 loss: 1.16343641\n",
      "iteration: 133 loss: 0.40947366\n",
      "iteration: 134 loss: 0.46867606\n",
      "iteration: 135 loss: 2.10138607\n",
      "iteration: 136 loss: 0.96783113\n",
      "iteration: 137 loss: 1.14698970\n",
      "iteration: 138 loss: 1.09112728\n",
      "iteration: 139 loss: 1.02840233\n",
      "iteration: 140 loss: 1.08427656\n",
      "iteration: 141 loss: 1.10498512\n",
      "iteration: 142 loss: 0.86997819\n",
      "iteration: 143 loss: 0.72584337\n",
      "iteration: 144 loss: 3.61629725\n",
      "iteration: 145 loss: 1.02405000\n",
      "iteration: 146 loss: 0.30732787\n",
      "iteration: 147 loss: 1.16861641\n",
      "iteration: 148 loss: 1.02971482\n",
      "iteration: 149 loss: 1.20088875\n",
      "iteration: 150 loss: 2.23969555\n",
      "iteration: 151 loss: 1.20713866\n",
      "iteration: 152 loss: 3.38308692\n",
      "iteration: 153 loss: 3.38028693\n",
      "iteration: 154 loss: 1.81812572\n",
      "iteration: 155 loss: 2.65339017\n",
      "iteration: 156 loss: 0.20302227\n",
      "iteration: 157 loss: 0.81677681\n",
      "iteration: 158 loss: 0.64128435\n",
      "iteration: 159 loss: 0.27150327\n",
      "iteration: 160 loss: 0.75482559\n",
      "iteration: 161 loss: 1.03089988\n",
      "iteration: 162 loss: 0.25380543\n",
      "iteration: 163 loss: 2.84460068\n",
      "iteration: 164 loss: 3.47635722\n",
      "iteration: 165 loss: 2.52864957\n",
      "iteration: 166 loss: 0.49086559\n",
      "iteration: 167 loss: 0.18553659\n",
      "iteration: 168 loss: 0.71936184\n",
      "iteration: 169 loss: 0.35980359\n",
      "iteration: 170 loss: 2.98579764\n",
      "iteration: 171 loss: 0.67835170\n",
      "iteration: 172 loss: 0.35030174\n",
      "iteration: 173 loss: 0.13817620\n",
      "iteration: 174 loss: 3.57632518\n",
      "iteration: 175 loss: 2.22222114\n",
      "iteration: 176 loss: 1.58473611\n",
      "iteration: 177 loss: 0.61306417\n",
      "iteration: 178 loss: 0.26727542\n",
      "iteration: 179 loss: 0.16857424\n",
      "iteration: 180 loss: 0.33037704\n",
      "iteration: 181 loss: 1.28874707\n",
      "iteration: 182 loss: 0.19749954\n",
      "iteration: 183 loss: 1.08295143\n",
      "iteration: 184 loss: 0.64133990\n",
      "iteration: 185 loss: 2.08012676\n",
      "iteration: 186 loss: 1.39697778\n",
      "iteration: 187 loss: 0.65249538\n",
      "iteration: 188 loss: 3.18572474\n",
      "iteration: 189 loss: 2.11050415\n",
      "iteration: 190 loss: 2.11044359\n",
      "iteration: 191 loss: 0.69129574\n",
      "iteration: 192 loss: 0.67337894\n",
      "iteration: 193 loss: 1.28087413\n",
      "iteration: 194 loss: 0.63613999\n",
      "iteration: 195 loss: 2.85707903\n",
      "iteration: 196 loss: 3.25950313\n",
      "iteration: 197 loss: 0.86637932\n",
      "iteration: 198 loss: 0.68325055\n",
      "iteration: 199 loss: 0.51045692\n",
      "epoch:  63 mean loss training: 1.26249862\n",
      "epoch:  63 mean loss validation: 1.36491048\n",
      "iteration:   0 loss: 2.80476761\n",
      "iteration:   1 loss: 0.89545000\n",
      "iteration:   2 loss: 0.88837928\n",
      "iteration:   3 loss: 0.68060887\n",
      "iteration:   4 loss: 1.92023242\n",
      "iteration:   5 loss: 1.89757001\n",
      "iteration:   6 loss: 1.64017320\n",
      "iteration:   7 loss: 1.18890023\n",
      "iteration:   8 loss: 2.66120458\n",
      "iteration:   9 loss: 3.52739000\n",
      "iteration:  10 loss: 0.77172315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  11 loss: 0.39361632\n",
      "iteration:  12 loss: 1.09511042\n",
      "iteration:  13 loss: 1.20714796\n",
      "iteration:  14 loss: 1.07187665\n",
      "iteration:  15 loss: 0.57726872\n",
      "iteration:  16 loss: 1.19697082\n",
      "iteration:  17 loss: 1.16132557\n",
      "iteration:  18 loss: 1.59521258\n",
      "iteration:  19 loss: 0.62320644\n",
      "iteration:  20 loss: 0.49614629\n",
      "iteration:  21 loss: 0.62202227\n",
      "iteration:  22 loss: 0.24593861\n",
      "iteration:  23 loss: 0.94216007\n",
      "iteration:  24 loss: 0.15994839\n",
      "iteration:  25 loss: 2.87338567\n",
      "iteration:  26 loss: 1.23571289\n",
      "iteration:  27 loss: 2.88086796\n",
      "iteration:  28 loss: 0.14812818\n",
      "iteration:  29 loss: 0.55676097\n",
      "iteration:  30 loss: 0.40607196\n",
      "iteration:  31 loss: 0.21360490\n",
      "iteration:  32 loss: 0.23609239\n",
      "iteration:  33 loss: 0.29615816\n",
      "iteration:  34 loss: 2.15854025\n",
      "iteration:  35 loss: 1.00025964\n",
      "iteration:  36 loss: 0.10678722\n",
      "iteration:  37 loss: 0.24310175\n",
      "iteration:  38 loss: 1.03048527\n",
      "iteration:  39 loss: 3.25942636\n",
      "iteration:  40 loss: 1.84903228\n",
      "iteration:  41 loss: 0.61868322\n",
      "iteration:  42 loss: 0.82292891\n",
      "iteration:  43 loss: 3.13934350\n",
      "iteration:  44 loss: 0.29522815\n",
      "iteration:  45 loss: 0.14342713\n",
      "iteration:  46 loss: 1.99947500\n",
      "iteration:  47 loss: 1.38781369\n",
      "iteration:  48 loss: 0.71316862\n",
      "iteration:  49 loss: 0.69193673\n",
      "iteration:  50 loss: 2.37686849\n",
      "iteration:  51 loss: 0.27654946\n",
      "iteration:  52 loss: 0.67790449\n",
      "iteration:  53 loss: 0.31112614\n",
      "iteration:  54 loss: 0.59507388\n",
      "iteration:  55 loss: 4.02363729\n",
      "iteration:  56 loss: 0.74785638\n",
      "iteration:  57 loss: 2.62244749\n",
      "iteration:  58 loss: 3.23480129\n",
      "iteration:  59 loss: 2.04835200\n",
      "iteration:  60 loss: 0.38566592\n",
      "iteration:  61 loss: 0.37649167\n",
      "iteration:  62 loss: 0.55793333\n",
      "iteration:  63 loss: 0.78797680\n",
      "iteration:  64 loss: 0.29596046\n",
      "iteration:  65 loss: 0.65255392\n",
      "iteration:  66 loss: 0.46195933\n",
      "iteration:  67 loss: 0.18314663\n",
      "iteration:  68 loss: 0.15414813\n",
      "iteration:  69 loss: 0.71695817\n",
      "iteration:  70 loss: 0.54445142\n",
      "iteration:  71 loss: 0.66816872\n",
      "iteration:  72 loss: 1.04658711\n",
      "iteration:  73 loss: 2.38366890\n",
      "iteration:  74 loss: 0.47120655\n",
      "iteration:  75 loss: 0.89894748\n",
      "iteration:  76 loss: 0.72119427\n",
      "iteration:  77 loss: 1.33728743\n",
      "iteration:  78 loss: 1.40670323\n",
      "iteration:  79 loss: 0.45359901\n",
      "iteration:  80 loss: 1.93128288\n",
      "iteration:  81 loss: 0.16993347\n",
      "iteration:  82 loss: 0.33674997\n",
      "iteration:  83 loss: 0.82097858\n",
      "iteration:  84 loss: 3.06529427\n",
      "iteration:  85 loss: 0.16364776\n",
      "iteration:  86 loss: 0.55172366\n",
      "iteration:  87 loss: 3.20244718\n",
      "iteration:  88 loss: 0.27853364\n",
      "iteration:  89 loss: 2.70831275\n",
      "iteration:  90 loss: 1.61228001\n",
      "iteration:  91 loss: 0.33582452\n",
      "iteration:  92 loss: 0.11483214\n",
      "iteration:  93 loss: 2.50625157\n",
      "iteration:  94 loss: 1.67674470\n",
      "iteration:  95 loss: 0.57726192\n",
      "iteration:  96 loss: 3.81724072\n",
      "iteration:  97 loss: 0.63558495\n",
      "iteration:  98 loss: 0.58096266\n",
      "iteration:  99 loss: 0.17042458\n",
      "iteration: 100 loss: 0.31461376\n",
      "iteration: 101 loss: 0.18345091\n",
      "iteration: 102 loss: 0.99961692\n",
      "iteration: 103 loss: 2.98194742\n",
      "iteration: 104 loss: 0.41734472\n",
      "iteration: 105 loss: 0.82442284\n",
      "iteration: 106 loss: 0.43521801\n",
      "iteration: 107 loss: 0.54439420\n",
      "iteration: 108 loss: 1.62766671\n",
      "iteration: 109 loss: 0.49964949\n",
      "iteration: 110 loss: 2.09509706\n",
      "iteration: 111 loss: 3.10893822\n",
      "iteration: 112 loss: 3.07398319\n",
      "iteration: 113 loss: 0.45335239\n",
      "iteration: 114 loss: 0.78910899\n",
      "iteration: 115 loss: 2.04430890\n",
      "iteration: 116 loss: 2.67906523\n",
      "iteration: 117 loss: 0.48414311\n",
      "iteration: 118 loss: 0.50894529\n",
      "iteration: 119 loss: 0.54588360\n",
      "iteration: 120 loss: 0.26375493\n",
      "iteration: 121 loss: 0.65735507\n",
      "iteration: 122 loss: 0.24878734\n",
      "iteration: 123 loss: 2.53121090\n",
      "iteration: 124 loss: 1.02852619\n",
      "iteration: 125 loss: 1.28363121\n",
      "iteration: 126 loss: 3.35934591\n",
      "iteration: 127 loss: 0.37389523\n",
      "iteration: 128 loss: 1.41378784\n",
      "iteration: 129 loss: 0.18785007\n",
      "iteration: 130 loss: 2.69679427\n",
      "iteration: 131 loss: 2.14799476\n",
      "iteration: 132 loss: 0.98185849\n",
      "iteration: 133 loss: 0.68651760\n",
      "iteration: 134 loss: 0.40986151\n",
      "iteration: 135 loss: 1.74390364\n",
      "iteration: 136 loss: 1.14986396\n",
      "iteration: 137 loss: 0.93423891\n",
      "iteration: 138 loss: 1.16790092\n",
      "iteration: 139 loss: 1.27114952\n",
      "iteration: 140 loss: 1.12634718\n",
      "iteration: 141 loss: 1.48513889\n",
      "iteration: 142 loss: 1.20558381\n",
      "iteration: 143 loss: 1.23001659\n",
      "iteration: 144 loss: 3.83507156\n",
      "iteration: 145 loss: 1.08459544\n",
      "iteration: 146 loss: 0.34080809\n",
      "iteration: 147 loss: 1.14577377\n",
      "iteration: 148 loss: 1.32502699\n",
      "iteration: 149 loss: 1.02907801\n",
      "iteration: 150 loss: 2.41345739\n",
      "iteration: 151 loss: 1.54783547\n",
      "iteration: 152 loss: 3.45713449\n",
      "iteration: 153 loss: 3.56609797\n",
      "iteration: 154 loss: 1.65821385\n",
      "iteration: 155 loss: 2.70626140\n",
      "iteration: 156 loss: 0.17281564\n",
      "iteration: 157 loss: 0.85969388\n",
      "iteration: 158 loss: 1.17248034\n",
      "iteration: 159 loss: 0.37090459\n",
      "iteration: 160 loss: 0.99456233\n",
      "iteration: 161 loss: 0.94592535\n",
      "iteration: 162 loss: 0.25182414\n",
      "iteration: 163 loss: 2.69368458\n",
      "iteration: 164 loss: 2.99433112\n",
      "iteration: 165 loss: 1.28870702\n",
      "iteration: 166 loss: 0.21590073\n",
      "iteration: 167 loss: 0.27058128\n",
      "iteration: 168 loss: 0.65244853\n",
      "iteration: 169 loss: 0.59421146\n",
      "iteration: 170 loss: 2.88294458\n",
      "iteration: 171 loss: 0.66203302\n",
      "iteration: 172 loss: 0.27901816\n",
      "iteration: 173 loss: 0.17859863\n",
      "iteration: 174 loss: 3.45537710\n",
      "iteration: 175 loss: 2.65562010\n",
      "iteration: 176 loss: 1.54572952\n",
      "iteration: 177 loss: 0.29427993\n",
      "iteration: 178 loss: 0.23304625\n",
      "iteration: 179 loss: 0.14126047\n",
      "iteration: 180 loss: 0.20313460\n",
      "iteration: 181 loss: 1.02035844\n",
      "iteration: 182 loss: 0.18711513\n",
      "iteration: 183 loss: 1.85333073\n",
      "iteration: 184 loss: 0.64433128\n",
      "iteration: 185 loss: 2.17977047\n",
      "iteration: 186 loss: 1.18250680\n",
      "iteration: 187 loss: 0.60915995\n",
      "iteration: 188 loss: 3.12657762\n",
      "iteration: 189 loss: 2.07958651\n",
      "iteration: 190 loss: 2.11503220\n",
      "iteration: 191 loss: 0.81312865\n",
      "iteration: 192 loss: 0.87420112\n",
      "iteration: 193 loss: 0.95438910\n",
      "iteration: 194 loss: 0.66341233\n",
      "iteration: 195 loss: 2.82673049\n",
      "iteration: 196 loss: 3.48328257\n",
      "iteration: 197 loss: 0.98074031\n",
      "iteration: 198 loss: 0.79378188\n",
      "iteration: 199 loss: 0.64261812\n",
      "epoch:  64 mean loss training: 1.23295641\n",
      "epoch:  64 mean loss validation: 1.38418233\n",
      "iteration:   0 loss: 3.29091763\n",
      "iteration:   1 loss: 0.84853196\n",
      "iteration:   2 loss: 0.94594717\n",
      "iteration:   3 loss: 0.80034620\n",
      "iteration:   4 loss: 1.81893051\n",
      "iteration:   5 loss: 2.02394009\n",
      "iteration:   6 loss: 1.94725788\n",
      "iteration:   7 loss: 1.10772085\n",
      "iteration:   8 loss: 2.76035380\n",
      "iteration:   9 loss: 3.32811260\n",
      "iteration:  10 loss: 0.85156304\n",
      "iteration:  11 loss: 0.46835038\n",
      "iteration:  12 loss: 1.02913356\n",
      "iteration:  13 loss: 1.35623431\n",
      "iteration:  14 loss: 1.05978167\n",
      "iteration:  15 loss: 0.58052552\n",
      "iteration:  16 loss: 0.86136782\n",
      "iteration:  17 loss: 0.91178274\n",
      "iteration:  18 loss: 1.69770074\n",
      "iteration:  19 loss: 0.69941026\n",
      "iteration:  20 loss: 0.95688248\n",
      "iteration:  21 loss: 0.90705192\n",
      "iteration:  22 loss: 0.60984087\n",
      "iteration:  23 loss: 1.14046860\n",
      "iteration:  24 loss: 0.16348128\n",
      "iteration:  25 loss: 1.27989066\n",
      "iteration:  26 loss: 1.29023218\n",
      "iteration:  27 loss: 2.41928387\n",
      "iteration:  28 loss: 0.18497865\n",
      "iteration:  29 loss: 0.56579649\n",
      "iteration:  30 loss: 0.42824987\n",
      "iteration:  31 loss: 0.22253327\n",
      "iteration:  32 loss: 0.54183251\n",
      "iteration:  33 loss: 0.36648002\n",
      "iteration:  34 loss: 2.15889573\n",
      "iteration:  35 loss: 1.18979967\n",
      "iteration:  36 loss: 0.11307826\n",
      "iteration:  37 loss: 0.25093359\n",
      "iteration:  38 loss: 1.17705166\n",
      "iteration:  39 loss: 3.28498483\n",
      "iteration:  40 loss: 2.04763031\n",
      "iteration:  41 loss: 1.17883313\n",
      "iteration:  42 loss: 0.98430568\n",
      "iteration:  43 loss: 3.11296153\n",
      "iteration:  44 loss: 0.46788856\n",
      "iteration:  45 loss: 0.15344970\n",
      "iteration:  46 loss: 1.73988080\n",
      "iteration:  47 loss: 0.72674865\n",
      "iteration:  48 loss: 0.75178647\n",
      "iteration:  49 loss: 0.51189560\n",
      "iteration:  50 loss: 2.37642479\n",
      "iteration:  51 loss: 0.36452770\n",
      "iteration:  52 loss: 0.95409143\n",
      "iteration:  53 loss: 0.68574244\n",
      "iteration:  54 loss: 0.57202047\n",
      "iteration:  55 loss: 3.77587652\n",
      "iteration:  56 loss: 1.06180286\n",
      "iteration:  57 loss: 2.91721344\n",
      "iteration:  58 loss: 2.95946169\n",
      "iteration:  59 loss: 2.05635738\n",
      "iteration:  60 loss: 0.56717670\n",
      "iteration:  61 loss: 0.36944732\n",
      "iteration:  62 loss: 0.54160714\n",
      "iteration:  63 loss: 0.90938687\n",
      "iteration:  64 loss: 0.28128028\n",
      "iteration:  65 loss: 0.83767349\n",
      "iteration:  66 loss: 0.41714743\n",
      "iteration:  67 loss: 0.19358328\n",
      "iteration:  68 loss: 0.15403844\n",
      "iteration:  69 loss: 0.56940120\n",
      "iteration:  70 loss: 0.49537417\n",
      "iteration:  71 loss: 0.49869114\n",
      "iteration:  72 loss: 0.77455848\n",
      "iteration:  73 loss: 3.23014498\n",
      "iteration:  74 loss: 0.53060389\n",
      "iteration:  75 loss: 0.59747273\n",
      "iteration:  76 loss: 0.69140881\n",
      "iteration:  77 loss: 0.57197016\n",
      "iteration:  78 loss: 1.65379107\n",
      "iteration:  79 loss: 0.57641107\n",
      "iteration:  80 loss: 1.57450628\n",
      "iteration:  81 loss: 0.22629845\n",
      "iteration:  82 loss: 0.64335757\n",
      "iteration:  83 loss: 0.58055514\n",
      "iteration:  84 loss: 3.09063268\n",
      "iteration:  85 loss: 0.16485013\n",
      "iteration:  86 loss: 0.57983154\n",
      "iteration:  87 loss: 3.35628295\n",
      "iteration:  88 loss: 0.30773047\n",
      "iteration:  89 loss: 2.57818270\n",
      "iteration:  90 loss: 1.62344825\n",
      "iteration:  91 loss: 0.48894310\n",
      "iteration:  92 loss: 0.20948328\n",
      "iteration:  93 loss: 2.12762260\n",
      "iteration:  94 loss: 0.93066549\n",
      "iteration:  95 loss: 0.75725091\n",
      "iteration:  96 loss: 3.00598192\n",
      "iteration:  97 loss: 0.62740916\n",
      "iteration:  98 loss: 0.57839853\n",
      "iteration:  99 loss: 0.31175604\n",
      "iteration: 100 loss: 0.45678827\n",
      "iteration: 101 loss: 0.23089051\n",
      "iteration: 102 loss: 0.99841797\n",
      "iteration: 103 loss: 3.01909137\n",
      "iteration: 104 loss: 0.39905143\n",
      "iteration: 105 loss: 0.93870229\n",
      "iteration: 106 loss: 0.47149554\n",
      "iteration: 107 loss: 0.53233135\n",
      "iteration: 108 loss: 2.06065798\n",
      "iteration: 109 loss: 0.59898609\n",
      "iteration: 110 loss: 2.16316319\n",
      "iteration: 111 loss: 3.11341333\n",
      "iteration: 112 loss: 2.95526052\n",
      "iteration: 113 loss: 0.50957268\n",
      "iteration: 114 loss: 0.91001964\n",
      "iteration: 115 loss: 2.64995241\n",
      "iteration: 116 loss: 2.13542819\n",
      "iteration: 117 loss: 0.46870267\n",
      "iteration: 118 loss: 0.33355719\n",
      "iteration: 119 loss: 0.60365838\n",
      "iteration: 120 loss: 0.22025527\n",
      "iteration: 121 loss: 0.72530556\n",
      "iteration: 122 loss: 0.24865180\n",
      "iteration: 123 loss: 2.48308587\n",
      "iteration: 124 loss: 1.32865763\n",
      "iteration: 125 loss: 1.70248532\n",
      "iteration: 126 loss: 3.19205284\n",
      "iteration: 127 loss: 0.38780600\n",
      "iteration: 128 loss: 1.91492534\n",
      "iteration: 129 loss: 0.31126764\n",
      "iteration: 130 loss: 2.66579556\n",
      "iteration: 131 loss: 2.01612329\n",
      "iteration: 132 loss: 0.91447550\n",
      "iteration: 133 loss: 0.42335936\n",
      "iteration: 134 loss: 0.41579467\n",
      "iteration: 135 loss: 2.13702464\n",
      "iteration: 136 loss: 1.08474100\n",
      "iteration: 137 loss: 1.10346079\n",
      "iteration: 138 loss: 1.19933689\n",
      "iteration: 139 loss: 1.10150731\n",
      "iteration: 140 loss: 1.02383721\n",
      "iteration: 141 loss: 1.40927958\n",
      "iteration: 142 loss: 1.06457746\n",
      "iteration: 143 loss: 0.69059139\n",
      "iteration: 144 loss: 3.63349724\n",
      "iteration: 145 loss: 1.21032298\n",
      "iteration: 146 loss: 0.33176988\n",
      "iteration: 147 loss: 1.01701736\n",
      "iteration: 148 loss: 1.05909872\n",
      "iteration: 149 loss: 0.85035884\n",
      "iteration: 150 loss: 2.04852343\n",
      "iteration: 151 loss: 2.51075363\n",
      "iteration: 152 loss: 3.31601644\n",
      "iteration: 153 loss: 3.38525414\n",
      "iteration: 154 loss: 1.49439049\n",
      "iteration: 155 loss: 2.57686162\n",
      "iteration: 156 loss: 0.17023003\n",
      "iteration: 157 loss: 0.80566818\n",
      "iteration: 158 loss: 0.81246269\n",
      "iteration: 159 loss: 0.23869178\n",
      "iteration: 160 loss: 1.13930416\n",
      "iteration: 161 loss: 1.19566584\n",
      "iteration: 162 loss: 0.25035498\n",
      "iteration: 163 loss: 2.85653472\n",
      "iteration: 164 loss: 3.09217739\n",
      "iteration: 165 loss: 2.21847343\n",
      "iteration: 166 loss: 0.20583428\n",
      "iteration: 167 loss: 0.16310140\n",
      "iteration: 168 loss: 0.63302153\n",
      "iteration: 169 loss: 0.35498407\n",
      "iteration: 170 loss: 3.09827042\n",
      "iteration: 171 loss: 0.85331511\n",
      "iteration: 172 loss: 0.34477365\n",
      "iteration: 173 loss: 0.11304291\n",
      "iteration: 174 loss: 3.59532404\n",
      "iteration: 175 loss: 2.68368530\n",
      "iteration: 176 loss: 1.94471908\n",
      "iteration: 177 loss: 0.38297674\n",
      "iteration: 178 loss: 0.25910369\n",
      "iteration: 179 loss: 0.13683662\n",
      "iteration: 180 loss: 0.24157852\n",
      "iteration: 181 loss: 1.24390304\n",
      "iteration: 182 loss: 0.20479871\n",
      "iteration: 183 loss: 1.68211973\n",
      "iteration: 184 loss: 0.56054658\n",
      "iteration: 185 loss: 1.93424273\n",
      "iteration: 186 loss: 1.71294141\n",
      "iteration: 187 loss: 0.57157576\n",
      "iteration: 188 loss: 3.11009407\n",
      "iteration: 189 loss: 2.09417248\n",
      "iteration: 190 loss: 2.09046149\n",
      "iteration: 191 loss: 0.68615466\n",
      "iteration: 192 loss: 0.77257043\n",
      "iteration: 193 loss: 1.43824792\n",
      "iteration: 194 loss: 0.58779544\n",
      "iteration: 195 loss: 2.74936628\n",
      "iteration: 196 loss: 3.26306772\n",
      "iteration: 197 loss: 0.93207872\n",
      "iteration: 198 loss: 0.56369865\n",
      "iteration: 199 loss: 0.52199829\n",
      "epoch:  65 mean loss training: 1.24260151\n",
      "epoch:  65 mean loss validation: 1.33667374\n",
      "iteration:   0 loss: 2.76800084\n",
      "iteration:   1 loss: 0.84606779\n",
      "iteration:   2 loss: 0.88128436\n",
      "iteration:   3 loss: 0.62850410\n",
      "iteration:   4 loss: 1.63996422\n",
      "iteration:   5 loss: 1.86280394\n",
      "iteration:   6 loss: 1.58937764\n",
      "iteration:   7 loss: 1.28766584\n",
      "iteration:   8 loss: 2.61871958\n",
      "iteration:   9 loss: 3.54235220\n",
      "iteration:  10 loss: 0.56312126\n",
      "iteration:  11 loss: 0.34233683\n",
      "iteration:  12 loss: 1.23962605\n",
      "iteration:  13 loss: 1.06485868\n",
      "iteration:  14 loss: 1.06549013\n",
      "iteration:  15 loss: 0.53846133\n",
      "iteration:  16 loss: 0.76664257\n",
      "iteration:  17 loss: 1.15531015\n",
      "iteration:  18 loss: 1.66009474\n",
      "iteration:  19 loss: 0.45047534\n",
      "iteration:  20 loss: 0.56384289\n",
      "iteration:  21 loss: 0.51927918\n",
      "iteration:  22 loss: 0.19796391\n",
      "iteration:  23 loss: 0.91537529\n",
      "iteration:  24 loss: 0.16156374\n",
      "iteration:  25 loss: 3.23255420\n",
      "iteration:  26 loss: 1.26151204\n",
      "iteration:  27 loss: 3.11633778\n",
      "iteration:  28 loss: 0.19069090\n",
      "iteration:  29 loss: 0.47946182\n",
      "iteration:  30 loss: 0.39284855\n",
      "iteration:  31 loss: 0.26317716\n",
      "iteration:  32 loss: 0.21976306\n",
      "iteration:  33 loss: 0.26545662\n",
      "iteration:  34 loss: 2.14275837\n",
      "iteration:  35 loss: 1.05219936\n",
      "iteration:  36 loss: 0.13163723\n",
      "iteration:  37 loss: 0.23961139\n",
      "iteration:  38 loss: 1.15759671\n",
      "iteration:  39 loss: 3.30352616\n",
      "iteration:  40 loss: 1.74041867\n",
      "iteration:  41 loss: 0.88544309\n",
      "iteration:  42 loss: 0.74933654\n",
      "iteration:  43 loss: 3.14841795\n",
      "iteration:  44 loss: 0.49874249\n",
      "iteration:  45 loss: 0.15911372\n",
      "iteration:  46 loss: 1.20107079\n",
      "iteration:  47 loss: 0.92418700\n",
      "iteration:  48 loss: 0.72324383\n",
      "iteration:  49 loss: 0.51044881\n",
      "iteration:  50 loss: 2.15479469\n",
      "iteration:  51 loss: 0.33083349\n",
      "iteration:  52 loss: 0.74984205\n",
      "iteration:  53 loss: 0.35271639\n",
      "iteration:  54 loss: 0.52894145\n",
      "iteration:  55 loss: 4.17880535\n",
      "iteration:  56 loss: 0.83225495\n",
      "iteration:  57 loss: 2.63708830\n",
      "iteration:  58 loss: 3.19895720\n",
      "iteration:  59 loss: 2.00286603\n",
      "iteration:  60 loss: 0.30382302\n",
      "iteration:  61 loss: 0.30373302\n",
      "iteration:  62 loss: 0.57272494\n",
      "iteration:  63 loss: 0.76115096\n",
      "iteration:  64 loss: 0.31908485\n",
      "iteration:  65 loss: 0.71972686\n",
      "iteration:  66 loss: 0.42739391\n",
      "iteration:  67 loss: 0.21539943\n",
      "iteration:  68 loss: 0.17739290\n",
      "iteration:  69 loss: 0.48625028\n",
      "iteration:  70 loss: 0.40068817\n",
      "iteration:  71 loss: 0.59415919\n",
      "iteration:  72 loss: 0.91563755\n",
      "iteration:  73 loss: 3.02748394\n",
      "iteration:  74 loss: 0.51864874\n",
      "iteration:  75 loss: 0.81992632\n",
      "iteration:  76 loss: 0.70733035\n",
      "iteration:  77 loss: 0.49848443\n",
      "iteration:  78 loss: 1.67121565\n",
      "iteration:  79 loss: 1.29502523\n",
      "iteration:  80 loss: 1.55426455\n",
      "iteration:  81 loss: 0.16188700\n",
      "iteration:  82 loss: 0.56577069\n",
      "iteration:  83 loss: 0.23459654\n",
      "iteration:  84 loss: 3.65636253\n",
      "iteration:  85 loss: 0.15597063\n",
      "iteration:  86 loss: 0.55228800\n",
      "iteration:  87 loss: 3.39212322\n",
      "iteration:  88 loss: 0.20976254\n",
      "iteration:  89 loss: 2.55538273\n",
      "iteration:  90 loss: 1.33116961\n",
      "iteration:  91 loss: 0.40701896\n",
      "iteration:  92 loss: 0.09740044\n",
      "iteration:  93 loss: 2.79058504\n",
      "iteration:  94 loss: 1.79117417\n",
      "iteration:  95 loss: 0.32567677\n",
      "iteration:  96 loss: 3.65016198\n",
      "iteration:  97 loss: 0.52213496\n",
      "iteration:  98 loss: 0.56034136\n",
      "iteration:  99 loss: 0.16430461\n",
      "iteration: 100 loss: 0.28756550\n",
      "iteration: 101 loss: 0.17546347\n",
      "iteration: 102 loss: 0.94794273\n",
      "iteration: 103 loss: 3.17749667\n",
      "iteration: 104 loss: 0.29444733\n",
      "iteration: 105 loss: 0.75814945\n",
      "iteration: 106 loss: 0.25751665\n",
      "iteration: 107 loss: 0.43739009\n",
      "iteration: 108 loss: 1.95036900\n",
      "iteration: 109 loss: 0.55844516\n",
      "iteration: 110 loss: 2.24909878\n",
      "iteration: 111 loss: 3.10539985\n",
      "iteration: 112 loss: 2.90455461\n",
      "iteration: 113 loss: 0.39153570\n",
      "iteration: 114 loss: 0.65882993\n",
      "iteration: 115 loss: 1.85582340\n",
      "iteration: 116 loss: 2.05046010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 117 loss: 0.44235823\n",
      "iteration: 118 loss: 0.30894509\n",
      "iteration: 119 loss: 0.57449126\n",
      "iteration: 120 loss: 0.16282542\n",
      "iteration: 121 loss: 0.71393752\n",
      "iteration: 122 loss: 0.23197919\n",
      "iteration: 123 loss: 2.41113067\n",
      "iteration: 124 loss: 1.25014484\n",
      "iteration: 125 loss: 1.62306452\n",
      "iteration: 126 loss: 3.44405270\n",
      "iteration: 127 loss: 0.33609274\n",
      "iteration: 128 loss: 1.58680427\n",
      "iteration: 129 loss: 0.25690851\n",
      "iteration: 130 loss: 2.69995403\n",
      "iteration: 131 loss: 2.20553827\n",
      "iteration: 132 loss: 1.04902089\n",
      "iteration: 133 loss: 0.41724232\n",
      "iteration: 134 loss: 0.38928601\n",
      "iteration: 135 loss: 2.10858154\n",
      "iteration: 136 loss: 1.06641638\n",
      "iteration: 137 loss: 0.90156394\n",
      "iteration: 138 loss: 1.10478687\n",
      "iteration: 139 loss: 1.08122253\n",
      "iteration: 140 loss: 1.11497271\n",
      "iteration: 141 loss: 1.40912020\n",
      "iteration: 142 loss: 1.03613019\n",
      "iteration: 143 loss: 1.08736300\n",
      "iteration: 144 loss: 3.60548639\n",
      "iteration: 145 loss: 0.95954990\n",
      "iteration: 146 loss: 0.33548895\n",
      "iteration: 147 loss: 1.17198396\n",
      "iteration: 148 loss: 1.24403989\n",
      "iteration: 149 loss: 0.77884299\n",
      "iteration: 150 loss: 2.21390462\n",
      "iteration: 151 loss: 2.40878129\n",
      "iteration: 152 loss: 3.48024654\n",
      "iteration: 153 loss: 3.55788684\n",
      "iteration: 154 loss: 1.79143703\n",
      "iteration: 155 loss: 2.81477666\n",
      "iteration: 156 loss: 0.16831495\n",
      "iteration: 157 loss: 0.87098914\n",
      "iteration: 158 loss: 0.85664266\n",
      "iteration: 159 loss: 0.31724131\n",
      "iteration: 160 loss: 1.16539609\n",
      "iteration: 161 loss: 0.69496393\n",
      "iteration: 162 loss: 0.24348722\n",
      "iteration: 163 loss: 2.72761440\n",
      "iteration: 164 loss: 3.08560824\n",
      "iteration: 165 loss: 1.18991792\n",
      "iteration: 166 loss: 0.22659038\n",
      "iteration: 167 loss: 0.15650941\n",
      "iteration: 168 loss: 0.61806518\n",
      "iteration: 169 loss: 0.38814312\n",
      "iteration: 170 loss: 3.23129201\n",
      "iteration: 171 loss: 0.57220477\n",
      "iteration: 172 loss: 0.29306558\n",
      "iteration: 173 loss: 0.11805144\n",
      "iteration: 174 loss: 4.05597162\n",
      "iteration: 175 loss: 3.22483993\n",
      "iteration: 176 loss: 1.46404874\n",
      "iteration: 177 loss: 0.30435160\n",
      "iteration: 178 loss: 0.22073732\n",
      "iteration: 179 loss: 0.12464613\n",
      "iteration: 180 loss: 0.26110718\n",
      "iteration: 181 loss: 1.03262341\n",
      "iteration: 182 loss: 0.14977293\n",
      "iteration: 183 loss: 2.04726720\n",
      "iteration: 184 loss: 0.68942100\n",
      "iteration: 185 loss: 2.57772827\n",
      "iteration: 186 loss: 1.81287408\n",
      "iteration: 187 loss: 0.54747707\n",
      "iteration: 188 loss: 3.08436346\n",
      "iteration: 189 loss: 2.08825064\n",
      "iteration: 190 loss: 2.23377585\n",
      "iteration: 191 loss: 0.73777789\n",
      "iteration: 192 loss: 1.00527751\n",
      "iteration: 193 loss: 1.19925189\n",
      "iteration: 194 loss: 0.63119912\n",
      "iteration: 195 loss: 2.81716847\n",
      "iteration: 196 loss: 3.53009653\n",
      "iteration: 197 loss: 0.89253783\n",
      "iteration: 198 loss: 0.91463393\n",
      "iteration: 199 loss: 0.66370058\n",
      "epoch:  66 mean loss training: 1.22955656\n",
      "epoch:  66 mean loss validation: 1.38792574\n",
      "iteration:   0 loss: 2.73038673\n",
      "iteration:   1 loss: 0.89666396\n",
      "iteration:   2 loss: 1.04117739\n",
      "iteration:   3 loss: 0.65644318\n",
      "iteration:   4 loss: 1.74812973\n",
      "iteration:   5 loss: 2.15838170\n",
      "iteration:   6 loss: 1.65080750\n",
      "iteration:   7 loss: 1.20973539\n",
      "iteration:   8 loss: 2.57047796\n",
      "iteration:   9 loss: 3.47079253\n",
      "iteration:  10 loss: 0.60096639\n",
      "iteration:  11 loss: 0.43032888\n",
      "iteration:  12 loss: 1.43575203\n",
      "iteration:  13 loss: 1.05762589\n",
      "iteration:  14 loss: 1.07271338\n",
      "iteration:  15 loss: 0.51241040\n",
      "iteration:  16 loss: 0.59571445\n",
      "iteration:  17 loss: 1.01348197\n",
      "iteration:  18 loss: 1.66822600\n",
      "iteration:  19 loss: 0.52756709\n",
      "iteration:  20 loss: 0.49348822\n",
      "iteration:  21 loss: 0.55131871\n",
      "iteration:  22 loss: 0.26767877\n",
      "iteration:  23 loss: 1.01323950\n",
      "iteration:  24 loss: 0.53761226\n",
      "iteration:  25 loss: 2.85095525\n",
      "iteration:  26 loss: 1.35033619\n",
      "iteration:  27 loss: 3.10995364\n",
      "iteration:  28 loss: 0.17193975\n",
      "iteration:  29 loss: 0.64910781\n",
      "iteration:  30 loss: 0.37181011\n",
      "iteration:  31 loss: 0.27038234\n",
      "iteration:  32 loss: 0.32531106\n",
      "iteration:  33 loss: 0.29107705\n",
      "iteration:  34 loss: 2.20145440\n",
      "iteration:  35 loss: 1.21704578\n",
      "iteration:  36 loss: 0.16387239\n",
      "iteration:  37 loss: 0.23901406\n",
      "iteration:  38 loss: 1.17255199\n",
      "iteration:  39 loss: 3.29769921\n",
      "iteration:  40 loss: 1.76347125\n",
      "iteration:  41 loss: 0.64363611\n",
      "iteration:  42 loss: 0.79858762\n",
      "iteration:  43 loss: 3.14419484\n",
      "iteration:  44 loss: 0.49810326\n",
      "iteration:  45 loss: 0.15496640\n",
      "iteration:  46 loss: 1.31505084\n",
      "iteration:  47 loss: 0.68196088\n",
      "iteration:  48 loss: 0.74496871\n",
      "iteration:  49 loss: 0.51752001\n",
      "iteration:  50 loss: 2.27969766\n",
      "iteration:  51 loss: 0.33006066\n",
      "iteration:  52 loss: 0.78227574\n",
      "iteration:  53 loss: 0.36596587\n",
      "iteration:  54 loss: 0.42660838\n",
      "iteration:  55 loss: 4.25220966\n",
      "iteration:  56 loss: 0.75583261\n",
      "iteration:  57 loss: 2.89383459\n",
      "iteration:  58 loss: 2.85560560\n",
      "iteration:  59 loss: 1.76396418\n",
      "iteration:  60 loss: 0.49343470\n",
      "iteration:  61 loss: 0.26961356\n",
      "iteration:  62 loss: 1.10176027\n",
      "iteration:  63 loss: 0.73770595\n",
      "iteration:  64 loss: 0.30816391\n",
      "iteration:  65 loss: 0.69477844\n",
      "iteration:  66 loss: 0.46513411\n",
      "iteration:  67 loss: 0.14490156\n",
      "iteration:  68 loss: 0.12506174\n",
      "iteration:  69 loss: 0.50294501\n",
      "iteration:  70 loss: 0.41809991\n",
      "iteration:  71 loss: 0.43695349\n",
      "iteration:  72 loss: 0.95135880\n",
      "iteration:  73 loss: 2.69386172\n",
      "iteration:  74 loss: 0.36358228\n",
      "iteration:  75 loss: 0.59838581\n",
      "iteration:  76 loss: 0.74579954\n",
      "iteration:  77 loss: 0.87922621\n",
      "iteration:  78 loss: 1.44944906\n",
      "iteration:  79 loss: 0.50046581\n",
      "iteration:  80 loss: 1.73896766\n",
      "iteration:  81 loss: 0.17682248\n",
      "iteration:  82 loss: 0.32106411\n",
      "iteration:  83 loss: 0.89793491\n",
      "iteration:  84 loss: 3.19902325\n",
      "iteration:  85 loss: 0.15096040\n",
      "iteration:  86 loss: 0.53242904\n",
      "iteration:  87 loss: 3.23052740\n",
      "iteration:  88 loss: 0.26867226\n",
      "iteration:  89 loss: 2.43631387\n",
      "iteration:  90 loss: 1.45041108\n",
      "iteration:  91 loss: 0.32960165\n",
      "iteration:  92 loss: 0.26875332\n",
      "iteration:  93 loss: 1.98747694\n",
      "iteration:  94 loss: 0.89114743\n",
      "iteration:  95 loss: 0.71216404\n",
      "iteration:  96 loss: 3.09015012\n",
      "iteration:  97 loss: 0.67544466\n",
      "iteration:  98 loss: 0.55417842\n",
      "iteration:  99 loss: 0.31719428\n",
      "iteration: 100 loss: 0.44795111\n",
      "iteration: 101 loss: 0.27373415\n",
      "iteration: 102 loss: 0.96690643\n",
      "iteration: 103 loss: 2.90877485\n",
      "iteration: 104 loss: 0.37021676\n",
      "iteration: 105 loss: 0.77350599\n",
      "iteration: 106 loss: 0.44140449\n",
      "iteration: 107 loss: 0.45362315\n",
      "iteration: 108 loss: 1.04302406\n",
      "iteration: 109 loss: 0.39081663\n",
      "iteration: 110 loss: 2.07839942\n",
      "iteration: 111 loss: 3.11071587\n",
      "iteration: 112 loss: 2.91521502\n",
      "iteration: 113 loss: 0.42461288\n",
      "iteration: 114 loss: 0.74425501\n",
      "iteration: 115 loss: 2.26673102\n",
      "iteration: 116 loss: 2.39817691\n",
      "iteration: 117 loss: 0.36746314\n",
      "iteration: 118 loss: 0.52147704\n",
      "iteration: 119 loss: 0.55006087\n",
      "iteration: 120 loss: 0.36111054\n",
      "iteration: 121 loss: 0.61120844\n",
      "iteration: 122 loss: 0.21568590\n",
      "iteration: 123 loss: 2.67043972\n",
      "iteration: 124 loss: 1.22163022\n",
      "iteration: 125 loss: 1.69391632\n",
      "iteration: 126 loss: 3.26285458\n",
      "iteration: 127 loss: 0.60336590\n",
      "iteration: 128 loss: 1.80153811\n",
      "iteration: 129 loss: 0.42099521\n",
      "iteration: 130 loss: 2.71748567\n",
      "iteration: 131 loss: 2.23205733\n",
      "iteration: 132 loss: 1.12067091\n",
      "iteration: 133 loss: 0.47258952\n",
      "iteration: 134 loss: 0.41663086\n",
      "iteration: 135 loss: 2.11385155\n",
      "iteration: 136 loss: 1.09248829\n",
      "iteration: 137 loss: 0.90807879\n",
      "iteration: 138 loss: 1.22413349\n",
      "iteration: 139 loss: 1.13548267\n",
      "iteration: 140 loss: 1.11508751\n",
      "iteration: 141 loss: 1.46727991\n",
      "iteration: 142 loss: 0.93648213\n",
      "iteration: 143 loss: 0.62082523\n",
      "iteration: 144 loss: 3.59794259\n",
      "iteration: 145 loss: 1.28763926\n",
      "iteration: 146 loss: 0.29005668\n",
      "iteration: 147 loss: 1.03281760\n",
      "iteration: 148 loss: 1.10541689\n",
      "iteration: 149 loss: 0.93849808\n",
      "iteration: 150 loss: 2.14021707\n",
      "iteration: 151 loss: 1.76336801\n",
      "iteration: 152 loss: 3.34917688\n",
      "iteration: 153 loss: 3.36938000\n",
      "iteration: 154 loss: 1.28880763\n",
      "iteration: 155 loss: 2.54043794\n",
      "iteration: 156 loss: 0.16379915\n",
      "iteration: 157 loss: 0.77438986\n",
      "iteration: 158 loss: 0.97711086\n",
      "iteration: 159 loss: 0.24003652\n",
      "iteration: 160 loss: 1.08017516\n",
      "iteration: 161 loss: 1.09466898\n",
      "iteration: 162 loss: 0.23377115\n",
      "iteration: 163 loss: 3.04341435\n",
      "iteration: 164 loss: 3.25236487\n",
      "iteration: 165 loss: 2.08832097\n",
      "iteration: 166 loss: 0.19283609\n",
      "iteration: 167 loss: 0.15802048\n",
      "iteration: 168 loss: 0.69887674\n",
      "iteration: 169 loss: 0.38249686\n",
      "iteration: 170 loss: 2.86884499\n",
      "iteration: 171 loss: 0.63671488\n",
      "iteration: 172 loss: 0.45625329\n",
      "iteration: 173 loss: 0.14980586\n",
      "iteration: 174 loss: 3.49906564\n",
      "iteration: 175 loss: 2.25905061\n",
      "iteration: 176 loss: 1.55318964\n",
      "iteration: 177 loss: 0.41978365\n",
      "iteration: 178 loss: 0.24836455\n",
      "iteration: 179 loss: 0.13859627\n",
      "iteration: 180 loss: 0.21063748\n",
      "iteration: 181 loss: 1.28331685\n",
      "iteration: 182 loss: 0.28032669\n",
      "iteration: 183 loss: 1.10347843\n",
      "iteration: 184 loss: 0.68361402\n",
      "iteration: 185 loss: 2.08119798\n",
      "iteration: 186 loss: 0.97218531\n",
      "iteration: 187 loss: 0.73674548\n",
      "iteration: 188 loss: 2.91156483\n",
      "iteration: 189 loss: 2.10183406\n",
      "iteration: 190 loss: 1.92348957\n",
      "iteration: 191 loss: 0.71654427\n",
      "iteration: 192 loss: 0.89487153\n",
      "iteration: 193 loss: 1.16925168\n",
      "iteration: 194 loss: 0.40341905\n",
      "iteration: 195 loss: 2.88680124\n",
      "iteration: 196 loss: 3.35908151\n",
      "iteration: 197 loss: 1.03713369\n",
      "iteration: 198 loss: 0.70090723\n",
      "iteration: 199 loss: 0.57324022\n",
      "epoch:  67 mean loss training: 1.20449185\n",
      "epoch:  67 mean loss validation: 1.42452168\n",
      "iteration:   0 loss: 3.31908059\n",
      "iteration:   1 loss: 0.84311396\n",
      "iteration:   2 loss: 0.91973257\n",
      "iteration:   3 loss: 0.91040826\n",
      "iteration:   4 loss: 1.55334091\n",
      "iteration:   5 loss: 1.43803167\n",
      "iteration:   6 loss: 1.84175074\n",
      "iteration:   7 loss: 1.06835520\n",
      "iteration:   8 loss: 2.90391421\n",
      "iteration:   9 loss: 3.90340185\n",
      "iteration:  10 loss: 0.81121004\n",
      "iteration:  11 loss: 0.38729173\n",
      "iteration:  12 loss: 1.11133778\n",
      "iteration:  13 loss: 1.21067369\n",
      "iteration:  14 loss: 1.12523580\n",
      "iteration:  15 loss: 0.53025711\n",
      "iteration:  16 loss: 1.10093081\n",
      "iteration:  17 loss: 0.72737843\n",
      "iteration:  18 loss: 1.49085665\n",
      "iteration:  19 loss: 1.04596269\n",
      "iteration:  20 loss: 0.50527263\n",
      "iteration:  21 loss: 1.57384288\n",
      "iteration:  22 loss: 0.21302404\n",
      "iteration:  23 loss: 1.02592361\n",
      "iteration:  24 loss: 0.35141692\n",
      "iteration:  25 loss: 2.66695499\n",
      "iteration:  26 loss: 0.92763019\n",
      "iteration:  27 loss: 3.11061168\n",
      "iteration:  28 loss: 0.22992069\n",
      "iteration:  29 loss: 0.44129360\n",
      "iteration:  30 loss: 0.66083902\n",
      "iteration:  31 loss: 0.46481627\n",
      "iteration:  32 loss: 0.20777448\n",
      "iteration:  33 loss: 0.30191982\n",
      "iteration:  34 loss: 1.41052008\n",
      "iteration:  35 loss: 0.94917816\n",
      "iteration:  36 loss: 0.09963150\n",
      "iteration:  37 loss: 0.44318509\n",
      "iteration:  38 loss: 1.17319798\n",
      "iteration:  39 loss: 3.50703144\n",
      "iteration:  40 loss: 2.18557405\n",
      "iteration:  41 loss: 0.62633407\n",
      "iteration:  42 loss: 0.93308932\n",
      "iteration:  43 loss: 2.89918852\n",
      "iteration:  44 loss: 0.37647966\n",
      "iteration:  45 loss: 0.60098201\n",
      "iteration:  46 loss: 1.39134634\n",
      "iteration:  47 loss: 0.81023353\n",
      "iteration:  48 loss: 0.77227998\n",
      "iteration:  49 loss: 0.53961271\n",
      "iteration:  50 loss: 2.20005274\n",
      "iteration:  51 loss: 0.31016141\n",
      "iteration:  52 loss: 0.68526250\n",
      "iteration:  53 loss: 0.62601817\n",
      "iteration:  54 loss: 0.61559093\n",
      "iteration:  55 loss: 4.34385920\n",
      "iteration:  56 loss: 1.00842547\n",
      "iteration:  57 loss: 2.73969817\n",
      "iteration:  58 loss: 3.24735904\n",
      "iteration:  59 loss: 1.71474838\n",
      "iteration:  60 loss: 0.34142241\n",
      "iteration:  61 loss: 0.39648777\n",
      "iteration:  62 loss: 0.74209237\n",
      "iteration:  63 loss: 0.93271416\n",
      "iteration:  64 loss: 0.14071751\n",
      "iteration:  65 loss: 0.70316124\n",
      "iteration:  66 loss: 0.22993489\n",
      "iteration:  67 loss: 0.21086228\n",
      "iteration:  68 loss: 0.35218555\n",
      "iteration:  69 loss: 0.58907539\n",
      "iteration:  70 loss: 0.53126276\n",
      "iteration:  71 loss: 0.81900889\n",
      "iteration:  72 loss: 0.87023860\n",
      "iteration:  73 loss: 2.74143434\n",
      "iteration:  74 loss: 0.64827663\n",
      "iteration:  75 loss: 0.70869690\n",
      "iteration:  76 loss: 0.67589897\n",
      "iteration:  77 loss: 0.59113944\n",
      "iteration:  78 loss: 1.48885942\n",
      "iteration:  79 loss: 1.38408279\n",
      "iteration:  80 loss: 1.00523794\n",
      "iteration:  81 loss: 0.23815244\n",
      "iteration:  82 loss: 0.99688196\n",
      "iteration:  83 loss: 0.25301707\n",
      "iteration:  84 loss: 3.35090899\n",
      "iteration:  85 loss: 0.28705347\n",
      "iteration:  86 loss: 0.61289668\n",
      "iteration:  87 loss: 2.79685378\n",
      "iteration:  88 loss: 0.48933625\n",
      "iteration:  89 loss: 3.08078504\n",
      "iteration:  90 loss: 1.91019630\n",
      "iteration:  91 loss: 1.02351367\n",
      "iteration:  92 loss: 0.36428025\n",
      "iteration:  93 loss: 2.64305091\n",
      "iteration:  94 loss: 2.18515468\n",
      "iteration:  95 loss: 0.57634681\n",
      "iteration:  96 loss: 4.24607372\n",
      "iteration:  97 loss: 1.03193629\n",
      "iteration:  98 loss: 0.56410658\n",
      "iteration:  99 loss: 0.42354494\n",
      "iteration: 100 loss: 0.29561028\n",
      "iteration: 101 loss: 0.55427325\n",
      "iteration: 102 loss: 1.13124514\n",
      "iteration: 103 loss: 2.88767838\n",
      "iteration: 104 loss: 0.47085142\n",
      "iteration: 105 loss: 0.86177289\n",
      "iteration: 106 loss: 0.45036224\n",
      "iteration: 107 loss: 0.61776656\n",
      "iteration: 108 loss: 1.95850325\n",
      "iteration: 109 loss: 0.56882632\n",
      "iteration: 110 loss: 2.71847129\n",
      "iteration: 111 loss: 3.13436460\n",
      "iteration: 112 loss: 2.70966697\n",
      "iteration: 113 loss: 0.41717932\n",
      "iteration: 114 loss: 0.74462444\n",
      "iteration: 115 loss: 2.33290243\n",
      "iteration: 116 loss: 2.16177678\n",
      "iteration: 117 loss: 0.36248600\n",
      "iteration: 118 loss: 0.59300214\n",
      "iteration: 119 loss: 1.34420180\n",
      "iteration: 120 loss: 0.41337734\n",
      "iteration: 121 loss: 0.62980711\n",
      "iteration: 122 loss: 0.31245077\n",
      "iteration: 123 loss: 1.50271523\n",
      "iteration: 124 loss: 1.27033603\n",
      "iteration: 125 loss: 1.45061898\n",
      "iteration: 126 loss: 2.93409634\n",
      "iteration: 127 loss: 0.40667462\n",
      "iteration: 128 loss: 1.58873844\n",
      "iteration: 129 loss: 0.32473719\n",
      "iteration: 130 loss: 2.69996548\n",
      "iteration: 131 loss: 1.89621341\n",
      "iteration: 132 loss: 1.08226919\n",
      "iteration: 133 loss: 0.39733589\n",
      "iteration: 134 loss: 0.44466272\n",
      "iteration: 135 loss: 2.12426829\n",
      "iteration: 136 loss: 1.30619121\n",
      "iteration: 137 loss: 0.96929413\n",
      "iteration: 138 loss: 1.78829157\n",
      "iteration: 139 loss: 1.17817593\n",
      "iteration: 140 loss: 0.89424354\n",
      "iteration: 141 loss: 1.20214236\n",
      "iteration: 142 loss: 1.01162648\n",
      "iteration: 143 loss: 2.01544881\n",
      "iteration: 144 loss: 3.57311940\n",
      "iteration: 145 loss: 1.04483652\n",
      "iteration: 146 loss: 0.42320266\n",
      "iteration: 147 loss: 1.05618858\n",
      "iteration: 148 loss: 1.12834513\n",
      "iteration: 149 loss: 1.16965008\n",
      "iteration: 150 loss: 1.89913094\n",
      "iteration: 151 loss: 1.80677485\n",
      "iteration: 152 loss: 3.24111629\n",
      "iteration: 153 loss: 3.27857351\n",
      "iteration: 154 loss: 1.22646463\n",
      "iteration: 155 loss: 2.63714409\n",
      "iteration: 156 loss: 0.24042462\n",
      "iteration: 157 loss: 0.84328347\n",
      "iteration: 158 loss: 0.95632476\n",
      "iteration: 159 loss: 0.32416502\n",
      "iteration: 160 loss: 0.82501572\n",
      "iteration: 161 loss: 1.25862277\n",
      "iteration: 162 loss: 0.24040402\n",
      "iteration: 163 loss: 3.16886950\n",
      "iteration: 164 loss: 3.28497219\n",
      "iteration: 165 loss: 2.85154772\n",
      "iteration: 166 loss: 0.35172561\n",
      "iteration: 167 loss: 0.20961387\n",
      "iteration: 168 loss: 0.61083680\n",
      "iteration: 169 loss: 0.30111411\n",
      "iteration: 170 loss: 3.36191320\n",
      "iteration: 171 loss: 0.95113760\n",
      "iteration: 172 loss: 0.49460244\n",
      "iteration: 173 loss: 0.12743017\n",
      "iteration: 174 loss: 3.64148164\n",
      "iteration: 175 loss: 2.82428336\n",
      "iteration: 176 loss: 2.32044005\n",
      "iteration: 177 loss: 0.28609583\n",
      "iteration: 178 loss: 0.32032734\n",
      "iteration: 179 loss: 0.18950972\n",
      "iteration: 180 loss: 1.15691912\n",
      "iteration: 181 loss: 1.11304832\n",
      "iteration: 182 loss: 0.22378045\n",
      "iteration: 183 loss: 2.20910263\n",
      "iteration: 184 loss: 0.98246372\n",
      "iteration: 185 loss: 2.90425372\n",
      "iteration: 186 loss: 1.94048858\n",
      "iteration: 187 loss: 0.61832356\n",
      "iteration: 188 loss: 2.88816619\n",
      "iteration: 189 loss: 2.60800147\n",
      "iteration: 190 loss: 2.61729288\n",
      "iteration: 191 loss: 0.53246421\n",
      "iteration: 192 loss: 0.53098083\n",
      "iteration: 193 loss: 1.52538836\n",
      "iteration: 194 loss: 0.56085938\n",
      "iteration: 195 loss: 2.98199487\n",
      "iteration: 196 loss: 3.23675704\n",
      "iteration: 197 loss: 1.06426919\n",
      "iteration: 198 loss: 0.67248797\n",
      "iteration: 199 loss: 0.58933318\n",
      "epoch:  68 mean loss training: 1.29946852\n",
      "epoch:  68 mean loss validation: 1.32180190\n",
      "iteration:   0 loss: 3.00886369\n",
      "iteration:   1 loss: 0.76150805\n",
      "iteration:   2 loss: 0.83226407\n",
      "iteration:   3 loss: 0.58233386\n",
      "iteration:   4 loss: 2.30404639\n",
      "iteration:   5 loss: 1.78011966\n",
      "iteration:   6 loss: 1.64508390\n",
      "iteration:   7 loss: 1.05243182\n",
      "iteration:   8 loss: 2.46813607\n",
      "iteration:   9 loss: 3.43772101\n",
      "iteration:  10 loss: 0.73076677\n",
      "iteration:  11 loss: 1.35931873\n",
      "iteration:  12 loss: 1.52745700\n",
      "iteration:  13 loss: 1.17017066\n",
      "iteration:  14 loss: 1.01595712\n",
      "iteration:  15 loss: 0.52774394\n",
      "iteration:  16 loss: 0.83560205\n",
      "iteration:  17 loss: 1.10971439\n",
      "iteration:  18 loss: 1.55654299\n",
      "iteration:  19 loss: 0.63939798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  20 loss: 0.55913776\n",
      "iteration:  21 loss: 0.79555047\n",
      "iteration:  22 loss: 0.27146021\n",
      "iteration:  23 loss: 0.91516829\n",
      "iteration:  24 loss: 0.37233233\n",
      "iteration:  25 loss: 2.67063284\n",
      "iteration:  26 loss: 1.32287049\n",
      "iteration:  27 loss: 3.06213832\n",
      "iteration:  28 loss: 0.17825900\n",
      "iteration:  29 loss: 0.43113011\n",
      "iteration:  30 loss: 0.39210945\n",
      "iteration:  31 loss: 0.38034540\n",
      "iteration:  32 loss: 0.30892929\n",
      "iteration:  33 loss: 0.22211303\n",
      "iteration:  34 loss: 1.76246107\n",
      "iteration:  35 loss: 1.04793239\n",
      "iteration:  36 loss: 0.11417260\n",
      "iteration:  37 loss: 0.43847433\n",
      "iteration:  38 loss: 1.15105975\n",
      "iteration:  39 loss: 3.27436519\n",
      "iteration:  40 loss: 1.69896030\n",
      "iteration:  41 loss: 0.61279798\n",
      "iteration:  42 loss: 0.79810119\n",
      "iteration:  43 loss: 2.93166447\n",
      "iteration:  44 loss: 0.39478517\n",
      "iteration:  45 loss: 0.12993711\n",
      "iteration:  46 loss: 1.28554809\n",
      "iteration:  47 loss: 0.64718670\n",
      "iteration:  48 loss: 0.72781861\n",
      "iteration:  49 loss: 0.49326906\n",
      "iteration:  50 loss: 2.20600009\n",
      "iteration:  51 loss: 0.28990480\n",
      "iteration:  52 loss: 0.64297110\n",
      "iteration:  53 loss: 0.51884013\n",
      "iteration:  54 loss: 0.60522121\n",
      "iteration:  55 loss: 4.32122135\n",
      "iteration:  56 loss: 0.86595750\n",
      "iteration:  57 loss: 2.70300913\n",
      "iteration:  58 loss: 3.09901714\n",
      "iteration:  59 loss: 2.07374859\n",
      "iteration:  60 loss: 0.29179958\n",
      "iteration:  61 loss: 0.35370216\n",
      "iteration:  62 loss: 0.69209111\n",
      "iteration:  63 loss: 0.82399857\n",
      "iteration:  64 loss: 0.28832585\n",
      "iteration:  65 loss: 0.75136787\n",
      "iteration:  66 loss: 0.46628091\n",
      "iteration:  67 loss: 0.23862006\n",
      "iteration:  68 loss: 0.36010891\n",
      "iteration:  69 loss: 0.54597652\n",
      "iteration:  70 loss: 0.52587813\n",
      "iteration:  71 loss: 0.64558768\n",
      "iteration:  72 loss: 0.80855572\n",
      "iteration:  73 loss: 2.58830738\n",
      "iteration:  74 loss: 0.78586137\n",
      "iteration:  75 loss: 0.82741284\n",
      "iteration:  76 loss: 0.73076093\n",
      "iteration:  77 loss: 0.52242780\n",
      "iteration:  78 loss: 1.56965280\n",
      "iteration:  79 loss: 1.70810330\n",
      "iteration:  80 loss: 1.40760863\n",
      "iteration:  81 loss: 0.16703953\n",
      "iteration:  82 loss: 1.00268912\n",
      "iteration:  83 loss: 0.26364046\n",
      "iteration:  84 loss: 3.49333692\n",
      "iteration:  85 loss: 0.28545585\n",
      "iteration:  86 loss: 0.38732642\n",
      "iteration:  87 loss: 2.61949897\n",
      "iteration:  88 loss: 0.42319673\n",
      "iteration:  89 loss: 2.69845557\n",
      "iteration:  90 loss: 1.21079409\n",
      "iteration:  91 loss: 0.46421728\n",
      "iteration:  92 loss: 0.17866141\n",
      "iteration:  93 loss: 2.69260097\n",
      "iteration:  94 loss: 1.96777272\n",
      "iteration:  95 loss: 0.24980927\n",
      "iteration:  96 loss: 4.46197081\n",
      "iteration:  97 loss: 0.74900883\n",
      "iteration:  98 loss: 0.56980145\n",
      "iteration:  99 loss: 0.25501871\n",
      "iteration: 100 loss: 0.34346646\n",
      "iteration: 101 loss: 0.26097083\n",
      "iteration: 102 loss: 1.20940578\n",
      "iteration: 103 loss: 2.79117513\n",
      "iteration: 104 loss: 0.50541908\n",
      "iteration: 105 loss: 0.67631316\n",
      "iteration: 106 loss: 0.31669161\n",
      "iteration: 107 loss: 1.13253164\n",
      "iteration: 108 loss: 1.11281967\n",
      "iteration: 109 loss: 0.60530764\n",
      "iteration: 110 loss: 2.59914207\n",
      "iteration: 111 loss: 3.11330223\n",
      "iteration: 112 loss: 2.87432480\n",
      "iteration: 113 loss: 0.38554233\n",
      "iteration: 114 loss: 0.78925955\n",
      "iteration: 115 loss: 1.71657324\n",
      "iteration: 116 loss: 2.33917522\n",
      "iteration: 117 loss: 0.44600692\n",
      "iteration: 118 loss: 0.57150298\n",
      "iteration: 119 loss: 0.66372758\n",
      "iteration: 120 loss: 0.30918244\n",
      "iteration: 121 loss: 0.57848376\n",
      "iteration: 122 loss: 0.28698507\n",
      "iteration: 123 loss: 1.46003067\n",
      "iteration: 124 loss: 1.15325403\n",
      "iteration: 125 loss: 1.61665225\n",
      "iteration: 126 loss: 3.12747526\n",
      "iteration: 127 loss: 0.37492707\n",
      "iteration: 128 loss: 2.15642238\n",
      "iteration: 129 loss: 0.22664861\n",
      "iteration: 130 loss: 2.63582873\n",
      "iteration: 131 loss: 2.09553313\n",
      "iteration: 132 loss: 1.26262164\n",
      "iteration: 133 loss: 0.37341893\n",
      "iteration: 134 loss: 0.39498514\n",
      "iteration: 135 loss: 1.53025889\n",
      "iteration: 136 loss: 1.17771065\n",
      "iteration: 137 loss: 0.61168587\n",
      "iteration: 138 loss: 1.35532391\n",
      "iteration: 139 loss: 1.29109859\n",
      "iteration: 140 loss: 1.18857574\n",
      "iteration: 141 loss: 1.04527915\n",
      "iteration: 142 loss: 0.90412152\n",
      "iteration: 143 loss: 1.89677691\n",
      "iteration: 144 loss: 3.65244675\n",
      "iteration: 145 loss: 1.30948150\n",
      "iteration: 146 loss: 0.36264497\n",
      "iteration: 147 loss: 1.04241693\n",
      "iteration: 148 loss: 1.11653602\n",
      "iteration: 149 loss: 1.31885350\n",
      "iteration: 150 loss: 2.32204843\n",
      "iteration: 151 loss: 1.46941328\n",
      "iteration: 152 loss: 3.40775108\n",
      "iteration: 153 loss: 3.18028593\n",
      "iteration: 154 loss: 1.58953440\n",
      "iteration: 155 loss: 2.76397514\n",
      "iteration: 156 loss: 0.25188661\n",
      "iteration: 157 loss: 0.88730168\n",
      "iteration: 158 loss: 1.11903918\n",
      "iteration: 159 loss: 0.34864160\n",
      "iteration: 160 loss: 0.75663811\n",
      "iteration: 161 loss: 1.22248328\n",
      "iteration: 162 loss: 0.25150511\n",
      "iteration: 163 loss: 2.74025393\n",
      "iteration: 164 loss: 3.23128724\n",
      "iteration: 165 loss: 2.79158497\n",
      "iteration: 166 loss: 0.34864372\n",
      "iteration: 167 loss: 0.28023914\n",
      "iteration: 168 loss: 0.68833905\n",
      "iteration: 169 loss: 0.42197624\n",
      "iteration: 170 loss: 3.26217079\n",
      "iteration: 171 loss: 0.98793066\n",
      "iteration: 172 loss: 0.57436317\n",
      "iteration: 173 loss: 0.15524115\n",
      "iteration: 174 loss: 3.55316067\n",
      "iteration: 175 loss: 2.52917218\n",
      "iteration: 176 loss: 2.26727962\n",
      "iteration: 177 loss: 0.23605840\n",
      "iteration: 178 loss: 0.33364600\n",
      "iteration: 179 loss: 0.20143200\n",
      "iteration: 180 loss: 1.13515031\n",
      "iteration: 181 loss: 1.16878140\n",
      "iteration: 182 loss: 0.21461290\n",
      "iteration: 183 loss: 2.12260270\n",
      "iteration: 184 loss: 0.89782798\n",
      "iteration: 185 loss: 2.90655398\n",
      "iteration: 186 loss: 2.02419806\n",
      "iteration: 187 loss: 0.56913775\n",
      "iteration: 188 loss: 2.99158311\n",
      "iteration: 189 loss: 2.61181021\n",
      "iteration: 190 loss: 2.80864358\n",
      "iteration: 191 loss: 0.48805067\n",
      "iteration: 192 loss: 0.58411103\n",
      "iteration: 193 loss: 1.57445014\n",
      "iteration: 194 loss: 0.61596519\n",
      "iteration: 195 loss: 2.97235894\n",
      "iteration: 196 loss: 3.21536040\n",
      "iteration: 197 loss: 1.02601123\n",
      "iteration: 198 loss: 0.61466056\n",
      "iteration: 199 loss: 0.57684684\n",
      "epoch:  69 mean loss training: 1.26867414\n",
      "epoch:  69 mean loss validation: 1.30584383\n",
      "iteration:   0 loss: 2.98615456\n",
      "iteration:   1 loss: 0.70097017\n",
      "iteration:   2 loss: 0.66915739\n",
      "iteration:   3 loss: 0.60773242\n",
      "iteration:   4 loss: 2.25275159\n",
      "iteration:   5 loss: 1.64717877\n",
      "iteration:   6 loss: 1.69605339\n",
      "iteration:   7 loss: 1.31215978\n",
      "iteration:   8 loss: 2.57103968\n",
      "iteration:   9 loss: 3.32482123\n",
      "iteration:  10 loss: 0.61510795\n",
      "iteration:  11 loss: 1.35901606\n",
      "iteration:  12 loss: 1.62076199\n",
      "iteration:  13 loss: 1.16917431\n",
      "iteration:  14 loss: 1.02851784\n",
      "iteration:  15 loss: 0.47860894\n",
      "iteration:  16 loss: 0.87036180\n",
      "iteration:  17 loss: 1.02445281\n",
      "iteration:  18 loss: 1.46477032\n",
      "iteration:  19 loss: 0.76440442\n",
      "iteration:  20 loss: 0.62462193\n",
      "iteration:  21 loss: 1.15438557\n",
      "iteration:  22 loss: 0.21640576\n",
      "iteration:  23 loss: 0.91236895\n",
      "iteration:  24 loss: 0.13524218\n",
      "iteration:  25 loss: 2.89539981\n",
      "iteration:  26 loss: 1.29965591\n",
      "iteration:  27 loss: 3.22205472\n",
      "iteration:  28 loss: 0.21206687\n",
      "iteration:  29 loss: 0.43662286\n",
      "iteration:  30 loss: 0.49918583\n",
      "iteration:  31 loss: 0.75087583\n",
      "iteration:  32 loss: 0.23113453\n",
      "iteration:  33 loss: 0.22145289\n",
      "iteration:  34 loss: 1.61632562\n",
      "iteration:  35 loss: 1.01181996\n",
      "iteration:  36 loss: 0.11534587\n",
      "iteration:  37 loss: 0.41134694\n",
      "iteration:  38 loss: 1.14897037\n",
      "iteration:  39 loss: 3.31331515\n",
      "iteration:  40 loss: 1.67606437\n",
      "iteration:  41 loss: 0.91319782\n",
      "iteration:  42 loss: 1.06730700\n",
      "iteration:  43 loss: 2.91716313\n",
      "iteration:  44 loss: 0.38733387\n",
      "iteration:  45 loss: 0.17361487\n",
      "iteration:  46 loss: 1.24812567\n",
      "iteration:  47 loss: 0.52707189\n",
      "iteration:  48 loss: 0.71414745\n",
      "iteration:  49 loss: 0.51137823\n",
      "iteration:  50 loss: 2.16851282\n",
      "iteration:  51 loss: 0.28033504\n",
      "iteration:  52 loss: 0.60863411\n",
      "iteration:  53 loss: 0.30563992\n",
      "iteration:  54 loss: 0.49486732\n",
      "iteration:  55 loss: 3.68153763\n",
      "iteration:  56 loss: 0.77900541\n",
      "iteration:  57 loss: 2.72043872\n",
      "iteration:  58 loss: 2.98919272\n",
      "iteration:  59 loss: 1.81619132\n",
      "iteration:  60 loss: 0.28671071\n",
      "iteration:  61 loss: 0.28723472\n",
      "iteration:  62 loss: 0.51531422\n",
      "iteration:  63 loss: 0.62196892\n",
      "iteration:  64 loss: 0.24580145\n",
      "iteration:  65 loss: 0.63329208\n",
      "iteration:  66 loss: 0.36555320\n",
      "iteration:  67 loss: 0.17301323\n",
      "iteration:  68 loss: 0.13403200\n",
      "iteration:  69 loss: 0.49030653\n",
      "iteration:  70 loss: 0.37949950\n",
      "iteration:  71 loss: 0.75370920\n",
      "iteration:  72 loss: 0.87006277\n",
      "iteration:  73 loss: 2.53170943\n",
      "iteration:  74 loss: 0.43773794\n",
      "iteration:  75 loss: 0.87205744\n",
      "iteration:  76 loss: 0.58540303\n",
      "iteration:  77 loss: 0.45297778\n",
      "iteration:  78 loss: 1.62300968\n",
      "iteration:  79 loss: 0.55926156\n",
      "iteration:  80 loss: 1.85024202\n",
      "iteration:  81 loss: 0.13683005\n",
      "iteration:  82 loss: 0.39140704\n",
      "iteration:  83 loss: 0.37551540\n",
      "iteration:  84 loss: 3.46073961\n",
      "iteration:  85 loss: 0.15952286\n",
      "iteration:  86 loss: 0.54089075\n",
      "iteration:  87 loss: 3.35894942\n",
      "iteration:  88 loss: 0.23821634\n",
      "iteration:  89 loss: 2.17397237\n",
      "iteration:  90 loss: 1.45669234\n",
      "iteration:  91 loss: 0.41750005\n",
      "iteration:  92 loss: 0.09662436\n",
      "iteration:  93 loss: 2.61996579\n",
      "iteration:  94 loss: 1.39969838\n",
      "iteration:  95 loss: 0.70691323\n",
      "iteration:  96 loss: 3.07419729\n",
      "iteration:  97 loss: 0.52347124\n",
      "iteration:  98 loss: 0.57476419\n",
      "iteration:  99 loss: 0.32017770\n",
      "iteration: 100 loss: 0.29334250\n",
      "iteration: 101 loss: 0.21154922\n",
      "iteration: 102 loss: 1.06333482\n",
      "iteration: 103 loss: 2.85511756\n",
      "iteration: 104 loss: 0.52784908\n",
      "iteration: 105 loss: 0.87287104\n",
      "iteration: 106 loss: 0.42319894\n",
      "iteration: 107 loss: 0.41933104\n",
      "iteration: 108 loss: 2.12482810\n",
      "iteration: 109 loss: 0.59652650\n",
      "iteration: 110 loss: 2.06409025\n",
      "iteration: 111 loss: 3.12013316\n",
      "iteration: 112 loss: 2.86861420\n",
      "iteration: 113 loss: 0.42648780\n",
      "iteration: 114 loss: 0.67938411\n",
      "iteration: 115 loss: 2.03125691\n",
      "iteration: 116 loss: 2.16066098\n",
      "iteration: 117 loss: 0.37883082\n",
      "iteration: 118 loss: 0.28963235\n",
      "iteration: 119 loss: 0.57528740\n",
      "iteration: 120 loss: 0.14742170\n",
      "iteration: 121 loss: 0.73018336\n",
      "iteration: 122 loss: 0.22080928\n",
      "iteration: 123 loss: 2.64328933\n",
      "iteration: 124 loss: 1.27311969\n",
      "iteration: 125 loss: 1.79368532\n",
      "iteration: 126 loss: 3.19465423\n",
      "iteration: 127 loss: 0.33677515\n",
      "iteration: 128 loss: 1.89570749\n",
      "iteration: 129 loss: 0.28968430\n",
      "iteration: 130 loss: 2.70822668\n",
      "iteration: 131 loss: 2.15862584\n",
      "iteration: 132 loss: 1.02031291\n",
      "iteration: 133 loss: 0.41496974\n",
      "iteration: 134 loss: 0.37283391\n",
      "iteration: 135 loss: 2.13190627\n",
      "iteration: 136 loss: 1.09999955\n",
      "iteration: 137 loss: 0.83639944\n",
      "iteration: 138 loss: 1.26562858\n",
      "iteration: 139 loss: 1.11161578\n",
      "iteration: 140 loss: 1.05594277\n",
      "iteration: 141 loss: 1.27584434\n",
      "iteration: 142 loss: 1.05353665\n",
      "iteration: 143 loss: 0.66760939\n",
      "iteration: 144 loss: 3.61559248\n",
      "iteration: 145 loss: 1.04387677\n",
      "iteration: 146 loss: 0.28752151\n",
      "iteration: 147 loss: 0.98201954\n",
      "iteration: 148 loss: 1.01970828\n",
      "iteration: 149 loss: 0.93635631\n",
      "iteration: 150 loss: 2.09746742\n",
      "iteration: 151 loss: 2.40327930\n",
      "iteration: 152 loss: 3.31636500\n",
      "iteration: 153 loss: 3.40198541\n",
      "iteration: 154 loss: 1.19743717\n",
      "iteration: 155 loss: 2.53379846\n",
      "iteration: 156 loss: 0.12795401\n",
      "iteration: 157 loss: 0.75317699\n",
      "iteration: 158 loss: 0.78910083\n",
      "iteration: 159 loss: 0.20659663\n",
      "iteration: 160 loss: 1.10847068\n",
      "iteration: 161 loss: 1.12012577\n",
      "iteration: 162 loss: 0.21551235\n",
      "iteration: 163 loss: 3.12693620\n",
      "iteration: 164 loss: 3.52126622\n",
      "iteration: 165 loss: 2.71911550\n",
      "iteration: 166 loss: 0.40359053\n",
      "iteration: 167 loss: 0.16292360\n",
      "iteration: 168 loss: 0.65572488\n",
      "iteration: 169 loss: 0.34565410\n",
      "iteration: 170 loss: 3.09284258\n",
      "iteration: 171 loss: 0.72375870\n",
      "iteration: 172 loss: 0.37173763\n",
      "iteration: 173 loss: 0.11348861\n",
      "iteration: 174 loss: 3.64698434\n",
      "iteration: 175 loss: 2.92408824\n",
      "iteration: 176 loss: 1.64159477\n",
      "iteration: 177 loss: 0.42190516\n",
      "iteration: 178 loss: 0.25597003\n",
      "iteration: 179 loss: 0.17238650\n",
      "iteration: 180 loss: 0.36242124\n",
      "iteration: 181 loss: 1.12884426\n",
      "iteration: 182 loss: 0.20488355\n",
      "iteration: 183 loss: 1.56994390\n",
      "iteration: 184 loss: 0.51019454\n",
      "iteration: 185 loss: 1.85852027\n",
      "iteration: 186 loss: 1.84573936\n",
      "iteration: 187 loss: 0.60532230\n",
      "iteration: 188 loss: 3.03620315\n",
      "iteration: 189 loss: 2.12326694\n",
      "iteration: 190 loss: 1.96469033\n",
      "iteration: 191 loss: 0.56555140\n",
      "iteration: 192 loss: 0.89315724\n",
      "iteration: 193 loss: 1.08117259\n",
      "iteration: 194 loss: 0.55774093\n",
      "iteration: 195 loss: 2.78866839\n",
      "iteration: 196 loss: 3.35604191\n",
      "iteration: 197 loss: 0.73548496\n",
      "iteration: 198 loss: 0.74141675\n",
      "iteration: 199 loss: 0.53441775\n",
      "epoch:  70 mean loss training: 1.21433806\n",
      "epoch:  70 mean loss validation: 1.34742415\n",
      "iteration:   0 loss: 2.87295461\n",
      "iteration:   1 loss: 0.78317493\n",
      "iteration:   2 loss: 0.78187686\n",
      "iteration:   3 loss: 0.71520191\n",
      "iteration:   4 loss: 2.18212962\n",
      "iteration:   5 loss: 1.63553154\n",
      "iteration:   6 loss: 1.47965252\n",
      "iteration:   7 loss: 1.02191365\n",
      "iteration:   8 loss: 2.69761086\n",
      "iteration:   9 loss: 4.00289249\n",
      "iteration:  10 loss: 0.72450554\n",
      "iteration:  11 loss: 0.40747112\n",
      "iteration:  12 loss: 1.00521851\n",
      "iteration:  13 loss: 1.22868764\n",
      "iteration:  14 loss: 1.06317687\n",
      "iteration:  15 loss: 0.51033872\n",
      "iteration:  16 loss: 1.04632509\n",
      "iteration:  17 loss: 1.12296140\n",
      "iteration:  18 loss: 1.50101721\n",
      "iteration:  19 loss: 0.36274385\n",
      "iteration:  20 loss: 0.69110787\n",
      "iteration:  21 loss: 0.50400853\n",
      "iteration:  22 loss: 0.22201993\n",
      "iteration:  23 loss: 0.95256007\n",
      "iteration:  24 loss: 0.26746672\n",
      "iteration:  25 loss: 2.74675345\n",
      "iteration:  26 loss: 1.30863428\n",
      "iteration:  27 loss: 3.19137454\n",
      "iteration:  28 loss: 0.26745731\n",
      "iteration:  29 loss: 0.42321953\n",
      "iteration:  30 loss: 0.44270864\n",
      "iteration:  31 loss: 0.27869895\n",
      "iteration:  32 loss: 0.23047119\n",
      "iteration:  33 loss: 0.25149161\n",
      "iteration:  34 loss: 2.13774753\n",
      "iteration:  35 loss: 1.17187524\n",
      "iteration:  36 loss: 0.08825530\n",
      "iteration:  37 loss: 0.29793942\n",
      "iteration:  38 loss: 1.16299212\n",
      "iteration:  39 loss: 3.28973961\n",
      "iteration:  40 loss: 1.78455949\n",
      "iteration:  41 loss: 0.67629236\n",
      "iteration:  42 loss: 0.90894002\n",
      "iteration:  43 loss: 2.96649694\n",
      "iteration:  44 loss: 0.77179486\n",
      "iteration:  45 loss: 0.14189982\n",
      "iteration:  46 loss: 1.37066269\n",
      "iteration:  47 loss: 0.66654694\n",
      "iteration:  48 loss: 0.73595047\n",
      "iteration:  49 loss: 0.52145225\n",
      "iteration:  50 loss: 2.33228707\n",
      "iteration:  51 loss: 0.31743082\n",
      "iteration:  52 loss: 0.75918674\n",
      "iteration:  53 loss: 0.48929229\n",
      "iteration:  54 loss: 0.41563562\n",
      "iteration:  55 loss: 4.35182810\n",
      "iteration:  56 loss: 0.70893282\n",
      "iteration:  57 loss: 2.92998791\n",
      "iteration:  58 loss: 2.67978001\n",
      "iteration:  59 loss: 1.72263718\n",
      "iteration:  60 loss: 0.45749763\n",
      "iteration:  61 loss: 0.28741208\n",
      "iteration:  62 loss: 0.43873987\n",
      "iteration:  63 loss: 0.75680125\n",
      "iteration:  64 loss: 0.29021314\n",
      "iteration:  65 loss: 0.64094406\n",
      "iteration:  66 loss: 0.39680415\n",
      "iteration:  67 loss: 0.14566977\n",
      "iteration:  68 loss: 0.11784440\n",
      "iteration:  69 loss: 0.49047843\n",
      "iteration:  70 loss: 0.38162985\n",
      "iteration:  71 loss: 0.73597795\n",
      "iteration:  72 loss: 0.87133968\n",
      "iteration:  73 loss: 2.54971743\n",
      "iteration:  74 loss: 0.28515205\n",
      "iteration:  75 loss: 0.50339311\n",
      "iteration:  76 loss: 0.69982797\n",
      "iteration:  77 loss: 1.06127584\n",
      "iteration:  78 loss: 1.36198461\n",
      "iteration:  79 loss: 0.49086931\n",
      "iteration:  80 loss: 1.63589346\n",
      "iteration:  81 loss: 0.16034117\n",
      "iteration:  82 loss: 0.29000810\n",
      "iteration:  83 loss: 0.97522634\n",
      "iteration:  84 loss: 3.16008997\n",
      "iteration:  85 loss: 0.15220329\n",
      "iteration:  86 loss: 0.51681608\n",
      "iteration:  87 loss: 3.22701550\n",
      "iteration:  88 loss: 0.20280114\n",
      "iteration:  89 loss: 2.48619413\n",
      "iteration:  90 loss: 1.67624855\n",
      "iteration:  91 loss: 0.33590776\n",
      "iteration:  92 loss: 0.13763991\n",
      "iteration:  93 loss: 2.17419028\n",
      "iteration:  94 loss: 1.28285563\n",
      "iteration:  95 loss: 0.69655430\n",
      "iteration:  96 loss: 3.29112077\n",
      "iteration:  97 loss: 0.71746689\n",
      "iteration:  98 loss: 0.54058611\n",
      "iteration:  99 loss: 0.25975934\n",
      "iteration: 100 loss: 0.33628383\n",
      "iteration: 101 loss: 0.20331398\n",
      "iteration: 102 loss: 0.99588013\n",
      "iteration: 103 loss: 2.92090964\n",
      "iteration: 104 loss: 0.38098660\n",
      "iteration: 105 loss: 0.84110188\n",
      "iteration: 106 loss: 0.42594472\n",
      "iteration: 107 loss: 0.34162840\n",
      "iteration: 108 loss: 1.62870169\n",
      "iteration: 109 loss: 0.49454400\n",
      "iteration: 110 loss: 2.04961944\n",
      "iteration: 111 loss: 3.12355399\n",
      "iteration: 112 loss: 2.82175064\n",
      "iteration: 113 loss: 0.42046309\n",
      "iteration: 114 loss: 0.67133522\n",
      "iteration: 115 loss: 1.84050834\n",
      "iteration: 116 loss: 2.15667748\n",
      "iteration: 117 loss: 0.36746001\n",
      "iteration: 118 loss: 0.34687164\n",
      "iteration: 119 loss: 0.49036536\n",
      "iteration: 120 loss: 0.17293279\n",
      "iteration: 121 loss: 0.65750813\n",
      "iteration: 122 loss: 0.19520999\n",
      "iteration: 123 loss: 2.62600374\n",
      "iteration: 124 loss: 1.19515097\n",
      "iteration: 125 loss: 1.73419034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 126 loss: 3.21069717\n",
      "iteration: 127 loss: 0.33956155\n",
      "iteration: 128 loss: 2.03317475\n",
      "iteration: 129 loss: 0.17762165\n",
      "iteration: 130 loss: 2.69768167\n",
      "iteration: 131 loss: 2.23110032\n",
      "iteration: 132 loss: 1.02800858\n",
      "iteration: 133 loss: 0.48633167\n",
      "iteration: 134 loss: 0.36240414\n",
      "iteration: 135 loss: 2.10293555\n",
      "iteration: 136 loss: 1.09128261\n",
      "iteration: 137 loss: 0.90193516\n",
      "iteration: 138 loss: 1.02187157\n",
      "iteration: 139 loss: 1.17529309\n",
      "iteration: 140 loss: 1.04323149\n",
      "iteration: 141 loss: 1.41028118\n",
      "iteration: 142 loss: 0.99395847\n",
      "iteration: 143 loss: 0.72344059\n",
      "iteration: 144 loss: 3.58504891\n",
      "iteration: 145 loss: 1.10547388\n",
      "iteration: 146 loss: 0.22450079\n",
      "iteration: 147 loss: 0.89982206\n",
      "iteration: 148 loss: 0.89842975\n",
      "iteration: 149 loss: 0.85170418\n",
      "iteration: 150 loss: 1.74127746\n",
      "iteration: 151 loss: 1.90633380\n",
      "iteration: 152 loss: 3.07241940\n",
      "iteration: 153 loss: 3.47058797\n",
      "iteration: 154 loss: 0.96330124\n",
      "iteration: 155 loss: 2.47896099\n",
      "iteration: 156 loss: 0.13544892\n",
      "iteration: 157 loss: 0.75449216\n",
      "iteration: 158 loss: 0.86517167\n",
      "iteration: 159 loss: 0.20721175\n",
      "iteration: 160 loss: 1.05051482\n",
      "iteration: 161 loss: 1.09981883\n",
      "iteration: 162 loss: 0.23495048\n",
      "iteration: 163 loss: 3.39575219\n",
      "iteration: 164 loss: 3.10282660\n",
      "iteration: 165 loss: 1.51391935\n",
      "iteration: 166 loss: 0.21576327\n",
      "iteration: 167 loss: 0.17488118\n",
      "iteration: 168 loss: 0.69379330\n",
      "iteration: 169 loss: 0.38726264\n",
      "iteration: 170 loss: 2.95759487\n",
      "iteration: 171 loss: 0.55802494\n",
      "iteration: 172 loss: 0.56423628\n",
      "iteration: 173 loss: 0.15904036\n",
      "iteration: 174 loss: 3.50656104\n",
      "iteration: 175 loss: 2.02354717\n",
      "iteration: 176 loss: 1.36105394\n",
      "iteration: 177 loss: 0.50643659\n",
      "iteration: 178 loss: 0.22600804\n",
      "iteration: 179 loss: 0.11920112\n",
      "iteration: 180 loss: 0.22755723\n",
      "iteration: 181 loss: 1.14526141\n",
      "iteration: 182 loss: 0.18983686\n",
      "iteration: 183 loss: 0.97149718\n",
      "iteration: 184 loss: 0.56678838\n",
      "iteration: 185 loss: 2.44556856\n",
      "iteration: 186 loss: 1.65257430\n",
      "iteration: 187 loss: 0.61234766\n",
      "iteration: 188 loss: 3.15942335\n",
      "iteration: 189 loss: 2.14603090\n",
      "iteration: 190 loss: 2.04942417\n",
      "iteration: 191 loss: 0.57642967\n",
      "iteration: 192 loss: 0.86104620\n",
      "iteration: 193 loss: 1.09282517\n",
      "iteration: 194 loss: 0.28730768\n",
      "iteration: 195 loss: 2.85349584\n",
      "iteration: 196 loss: 3.39212465\n",
      "iteration: 197 loss: 0.80283320\n",
      "iteration: 198 loss: 0.75269395\n",
      "iteration: 199 loss: 0.56239438\n",
      "epoch:  71 mean loss training: 1.18517268\n",
      "epoch:  71 mean loss validation: 1.40449643\n",
      "iteration:   0 loss: 2.77316189\n",
      "iteration:   1 loss: 0.78113955\n",
      "iteration:   2 loss: 0.81086528\n",
      "iteration:   3 loss: 0.79717523\n",
      "iteration:   4 loss: 1.89849126\n",
      "iteration:   5 loss: 1.61212039\n",
      "iteration:   6 loss: 1.44450212\n",
      "iteration:   7 loss: 0.95079827\n",
      "iteration:   8 loss: 2.94264507\n",
      "iteration:   9 loss: 4.30457544\n",
      "iteration:  10 loss: 0.73613739\n",
      "iteration:  11 loss: 0.39454722\n",
      "iteration:  12 loss: 0.90279979\n",
      "iteration:  13 loss: 1.47163081\n",
      "iteration:  14 loss: 1.13544405\n",
      "iteration:  15 loss: 0.67701167\n",
      "iteration:  16 loss: 1.38540506\n",
      "iteration:  17 loss: 0.88810688\n",
      "iteration:  18 loss: 1.69879115\n",
      "iteration:  19 loss: 1.01176834\n",
      "iteration:  20 loss: 0.96765924\n",
      "iteration:  21 loss: 0.53993040\n",
      "iteration:  22 loss: 0.21444324\n",
      "iteration:  23 loss: 0.99902791\n",
      "iteration:  24 loss: 0.20916471\n",
      "iteration:  25 loss: 2.45827103\n",
      "iteration:  26 loss: 1.25704825\n",
      "iteration:  27 loss: 3.20726657\n",
      "iteration:  28 loss: 0.22840130\n",
      "iteration:  29 loss: 0.35362661\n",
      "iteration:  30 loss: 0.36094645\n",
      "iteration:  31 loss: 0.21578705\n",
      "iteration:  32 loss: 0.27865517\n",
      "iteration:  33 loss: 0.27414039\n",
      "iteration:  34 loss: 1.83264613\n",
      "iteration:  35 loss: 1.16956639\n",
      "iteration:  36 loss: 0.08723543\n",
      "iteration:  37 loss: 0.40718970\n",
      "iteration:  38 loss: 1.16128933\n",
      "iteration:  39 loss: 3.30949521\n",
      "iteration:  40 loss: 1.92296743\n",
      "iteration:  41 loss: 0.54751354\n",
      "iteration:  42 loss: 0.75667679\n",
      "iteration:  43 loss: 2.92554092\n",
      "iteration:  44 loss: 0.69952369\n",
      "iteration:  45 loss: 0.14860147\n",
      "iteration:  46 loss: 1.15808749\n",
      "iteration:  47 loss: 0.75348413\n",
      "iteration:  48 loss: 0.74365962\n",
      "iteration:  49 loss: 0.53353924\n",
      "iteration:  50 loss: 2.37879157\n",
      "iteration:  51 loss: 0.34252235\n",
      "iteration:  52 loss: 0.69482988\n",
      "iteration:  53 loss: 0.59563488\n",
      "iteration:  54 loss: 0.43094799\n",
      "iteration:  55 loss: 3.94297266\n",
      "iteration:  56 loss: 0.81067365\n",
      "iteration:  57 loss: 2.93556380\n",
      "iteration:  58 loss: 2.65693998\n",
      "iteration:  59 loss: 1.86470377\n",
      "iteration:  60 loss: 0.46880707\n",
      "iteration:  61 loss: 0.24070086\n",
      "iteration:  62 loss: 0.38280454\n",
      "iteration:  63 loss: 0.79690564\n",
      "iteration:  64 loss: 0.32434678\n",
      "iteration:  65 loss: 0.63149250\n",
      "iteration:  66 loss: 0.41095293\n",
      "iteration:  67 loss: 0.16887927\n",
      "iteration:  68 loss: 0.21916606\n",
      "iteration:  69 loss: 0.33968341\n",
      "iteration:  70 loss: 0.41202474\n",
      "iteration:  71 loss: 0.73013246\n",
      "iteration:  72 loss: 0.70365191\n",
      "iteration:  73 loss: 2.75615287\n",
      "iteration:  74 loss: 0.41803578\n",
      "iteration:  75 loss: 0.72797704\n",
      "iteration:  76 loss: 0.67720515\n",
      "iteration:  77 loss: 1.15962481\n",
      "iteration:  78 loss: 1.21049190\n",
      "iteration:  79 loss: 0.45822936\n",
      "iteration:  80 loss: 1.82268894\n",
      "iteration:  81 loss: 0.16676047\n",
      "iteration:  82 loss: 0.38700253\n",
      "iteration:  83 loss: 0.78119975\n",
      "iteration:  84 loss: 3.17599225\n",
      "iteration:  85 loss: 0.15562554\n",
      "iteration:  86 loss: 0.53163218\n",
      "iteration:  87 loss: 3.14603400\n",
      "iteration:  88 loss: 0.33236045\n",
      "iteration:  89 loss: 2.29344201\n",
      "iteration:  90 loss: 1.68859065\n",
      "iteration:  91 loss: 0.42413127\n",
      "iteration:  92 loss: 0.12363029\n",
      "iteration:  93 loss: 2.34755492\n",
      "iteration:  94 loss: 1.20016611\n",
      "iteration:  95 loss: 0.70874810\n",
      "iteration:  96 loss: 2.83949804\n",
      "iteration:  97 loss: 0.61897004\n",
      "iteration:  98 loss: 0.57046044\n",
      "iteration:  99 loss: 0.27071926\n",
      "iteration: 100 loss: 0.32523122\n",
      "iteration: 101 loss: 0.19075692\n",
      "iteration: 102 loss: 0.99283314\n",
      "iteration: 103 loss: 2.85073566\n",
      "iteration: 104 loss: 0.34078008\n",
      "iteration: 105 loss: 0.83438694\n",
      "iteration: 106 loss: 0.35713065\n",
      "iteration: 107 loss: 0.33142680\n",
      "iteration: 108 loss: 1.99361980\n",
      "iteration: 109 loss: 0.56892425\n",
      "iteration: 110 loss: 2.06787038\n",
      "iteration: 111 loss: 3.11989856\n",
      "iteration: 112 loss: 2.77953124\n",
      "iteration: 113 loss: 0.40654561\n",
      "iteration: 114 loss: 0.60235107\n",
      "iteration: 115 loss: 1.79210162\n",
      "iteration: 116 loss: 2.01921082\n",
      "iteration: 117 loss: 0.33329493\n",
      "iteration: 118 loss: 0.26111406\n",
      "iteration: 119 loss: 0.50969517\n",
      "iteration: 120 loss: 0.13013758\n",
      "iteration: 121 loss: 0.73445636\n",
      "iteration: 122 loss: 0.20318629\n",
      "iteration: 123 loss: 2.61096382\n",
      "iteration: 124 loss: 1.38076556\n",
      "iteration: 125 loss: 2.01693749\n",
      "iteration: 126 loss: 3.40027833\n",
      "iteration: 127 loss: 0.33997676\n",
      "iteration: 128 loss: 2.02199578\n",
      "iteration: 129 loss: 0.24646750\n",
      "iteration: 130 loss: 2.65291214\n",
      "iteration: 131 loss: 2.30884504\n",
      "iteration: 132 loss: 1.20130229\n",
      "iteration: 133 loss: 0.61230373\n",
      "iteration: 134 loss: 0.36342520\n",
      "iteration: 135 loss: 2.09657311\n",
      "iteration: 136 loss: 0.96604347\n",
      "iteration: 137 loss: 0.85949838\n",
      "iteration: 138 loss: 1.16006923\n",
      "iteration: 139 loss: 1.12541282\n",
      "iteration: 140 loss: 0.98613012\n",
      "iteration: 141 loss: 1.12781799\n",
      "iteration: 142 loss: 0.86373425\n",
      "iteration: 143 loss: 0.71249050\n",
      "iteration: 144 loss: 3.60673904\n",
      "iteration: 145 loss: 0.99856740\n",
      "iteration: 146 loss: 0.23834242\n",
      "iteration: 147 loss: 0.97322482\n",
      "iteration: 148 loss: 0.76911122\n",
      "iteration: 149 loss: 0.83655488\n",
      "iteration: 150 loss: 1.99538851\n",
      "iteration: 151 loss: 2.13914585\n",
      "iteration: 152 loss: 3.23947477\n",
      "iteration: 153 loss: 3.27255487\n",
      "iteration: 154 loss: 1.38815475\n",
      "iteration: 155 loss: 2.61274338\n",
      "iteration: 156 loss: 0.17198552\n",
      "iteration: 157 loss: 0.76225406\n",
      "iteration: 158 loss: 0.78303522\n",
      "iteration: 159 loss: 0.22108023\n",
      "iteration: 160 loss: 1.05798495\n",
      "iteration: 161 loss: 1.08201611\n",
      "iteration: 162 loss: 0.23733522\n",
      "iteration: 163 loss: 2.72345543\n",
      "iteration: 164 loss: 3.11688185\n",
      "iteration: 165 loss: 1.26563799\n",
      "iteration: 166 loss: 0.44430158\n",
      "iteration: 167 loss: 0.17626865\n",
      "iteration: 168 loss: 0.76289421\n",
      "iteration: 169 loss: 0.36242539\n",
      "iteration: 170 loss: 3.08826923\n",
      "iteration: 171 loss: 0.50235516\n",
      "iteration: 172 loss: 0.53183800\n",
      "iteration: 173 loss: 0.13230377\n",
      "iteration: 174 loss: 3.34996700\n",
      "iteration: 175 loss: 2.46185780\n",
      "iteration: 176 loss: 1.27238643\n",
      "iteration: 177 loss: 0.77145320\n",
      "iteration: 178 loss: 0.26307636\n",
      "iteration: 179 loss: 0.63792711\n",
      "iteration: 180 loss: 0.81706774\n",
      "iteration: 181 loss: 1.39506495\n",
      "iteration: 182 loss: 0.36116889\n",
      "iteration: 183 loss: 1.49083900\n",
      "iteration: 184 loss: 0.58679116\n",
      "iteration: 185 loss: 1.97424698\n",
      "iteration: 186 loss: 2.02127028\n",
      "iteration: 187 loss: 0.70373309\n",
      "iteration: 188 loss: 2.99598193\n",
      "iteration: 189 loss: 1.79913044\n",
      "iteration: 190 loss: 2.03995299\n",
      "iteration: 191 loss: 0.75255227\n",
      "iteration: 192 loss: 1.48778462\n",
      "iteration: 193 loss: 1.07483625\n",
      "iteration: 194 loss: 0.71380532\n",
      "iteration: 195 loss: 2.98089552\n",
      "iteration: 196 loss: 3.39740467\n",
      "iteration: 197 loss: 0.77742475\n",
      "iteration: 198 loss: 0.75236058\n",
      "iteration: 199 loss: 0.98200786\n",
      "epoch:  72 mean loss training: 1.20983493\n",
      "epoch:  72 mean loss validation: 1.40468645\n",
      "iteration:   0 loss: 3.29034948\n",
      "iteration:   1 loss: 0.76089340\n",
      "iteration:   2 loss: 0.72932786\n",
      "iteration:   3 loss: 0.85761166\n",
      "iteration:   4 loss: 1.58602524\n",
      "iteration:   5 loss: 1.81024623\n",
      "iteration:   6 loss: 1.68775988\n",
      "iteration:   7 loss: 1.00108159\n",
      "iteration:   8 loss: 2.73572755\n",
      "iteration:   9 loss: 3.39648414\n",
      "iteration:  10 loss: 0.63749754\n",
      "iteration:  11 loss: 0.44735092\n",
      "iteration:  12 loss: 1.19837308\n",
      "iteration:  13 loss: 1.15458322\n",
      "iteration:  14 loss: 1.06728065\n",
      "iteration:  15 loss: 0.51726162\n",
      "iteration:  16 loss: 0.85905242\n",
      "iteration:  17 loss: 0.82485682\n",
      "iteration:  18 loss: 1.43727088\n",
      "iteration:  19 loss: 0.64751226\n",
      "iteration:  20 loss: 0.56438506\n",
      "iteration:  21 loss: 0.88205385\n",
      "iteration:  22 loss: 0.29094663\n",
      "iteration:  23 loss: 1.07962251\n",
      "iteration:  24 loss: 0.15690605\n",
      "iteration:  25 loss: 1.63626933\n",
      "iteration:  26 loss: 1.30023599\n",
      "iteration:  27 loss: 3.30269027\n",
      "iteration:  28 loss: 0.23962098\n",
      "iteration:  29 loss: 0.38603535\n",
      "iteration:  30 loss: 0.48867092\n",
      "iteration:  31 loss: 0.24936321\n",
      "iteration:  32 loss: 0.37323633\n",
      "iteration:  33 loss: 0.30710950\n",
      "iteration:  34 loss: 1.73744786\n",
      "iteration:  35 loss: 1.11948466\n",
      "iteration:  36 loss: 0.10160428\n",
      "iteration:  37 loss: 0.41064006\n",
      "iteration:  38 loss: 1.15811348\n",
      "iteration:  39 loss: 3.33223557\n",
      "iteration:  40 loss: 2.21283865\n",
      "iteration:  41 loss: 0.81868237\n",
      "iteration:  42 loss: 1.06079686\n",
      "iteration:  43 loss: 2.92765498\n",
      "iteration:  44 loss: 0.68342185\n",
      "iteration:  45 loss: 0.18328167\n",
      "iteration:  46 loss: 1.41496408\n",
      "iteration:  47 loss: 0.70173597\n",
      "iteration:  48 loss: 0.74601257\n",
      "iteration:  49 loss: 0.50449860\n",
      "iteration:  50 loss: 2.41671252\n",
      "iteration:  51 loss: 0.34321713\n",
      "iteration:  52 loss: 0.88610041\n",
      "iteration:  53 loss: 0.72631955\n",
      "iteration:  54 loss: 0.51369447\n",
      "iteration:  55 loss: 3.71868420\n",
      "iteration:  56 loss: 1.14703512\n",
      "iteration:  57 loss: 2.93843961\n",
      "iteration:  58 loss: 2.57876205\n",
      "iteration:  59 loss: 1.91886222\n",
      "iteration:  60 loss: 0.47825396\n",
      "iteration:  61 loss: 0.25620988\n",
      "iteration:  62 loss: 0.55587119\n",
      "iteration:  63 loss: 0.90090513\n",
      "iteration:  64 loss: 0.28830326\n",
      "iteration:  65 loss: 0.69217312\n",
      "iteration:  66 loss: 0.39973646\n",
      "iteration:  67 loss: 0.16958939\n",
      "iteration:  68 loss: 0.18176657\n",
      "iteration:  69 loss: 0.34261376\n",
      "iteration:  70 loss: 0.37336749\n",
      "iteration:  71 loss: 0.77141058\n",
      "iteration:  72 loss: 0.74492764\n",
      "iteration:  73 loss: 2.74543548\n",
      "iteration:  74 loss: 0.46313775\n",
      "iteration:  75 loss: 0.42213637\n",
      "iteration:  76 loss: 0.65337312\n",
      "iteration:  77 loss: 0.58866966\n",
      "iteration:  78 loss: 1.41730714\n",
      "iteration:  79 loss: 0.59928656\n",
      "iteration:  80 loss: 1.44701576\n",
      "iteration:  81 loss: 0.19705105\n",
      "iteration:  82 loss: 0.88481307\n",
      "iteration:  83 loss: 0.24781816\n",
      "iteration:  84 loss: 2.93405080\n",
      "iteration:  85 loss: 0.15518269\n",
      "iteration:  86 loss: 0.56257778\n",
      "iteration:  87 loss: 3.30153894\n",
      "iteration:  88 loss: 0.35088825\n",
      "iteration:  89 loss: 2.61835647\n",
      "iteration:  90 loss: 1.37949872\n",
      "iteration:  91 loss: 0.42694020\n",
      "iteration:  92 loss: 0.27579007\n",
      "iteration:  93 loss: 2.51886010\n",
      "iteration:  94 loss: 1.31930745\n",
      "iteration:  95 loss: 0.69543183\n",
      "iteration:  96 loss: 3.00426412\n",
      "iteration:  97 loss: 0.64871699\n",
      "iteration:  98 loss: 0.57564729\n",
      "iteration:  99 loss: 0.29931006\n",
      "iteration: 100 loss: 0.31842607\n",
      "iteration: 101 loss: 0.18905959\n",
      "iteration: 102 loss: 1.02020824\n",
      "iteration: 103 loss: 2.79125047\n",
      "iteration: 104 loss: 0.33221501\n",
      "iteration: 105 loss: 0.78740722\n",
      "iteration: 106 loss: 0.37838677\n",
      "iteration: 107 loss: 0.35103649\n",
      "iteration: 108 loss: 1.62014604\n",
      "iteration: 109 loss: 0.49481869\n",
      "iteration: 110 loss: 2.63455296\n",
      "iteration: 111 loss: 3.13027310\n",
      "iteration: 112 loss: 2.75787449\n",
      "iteration: 113 loss: 0.38324630\n",
      "iteration: 114 loss: 0.61099601\n",
      "iteration: 115 loss: 1.66292107\n",
      "iteration: 116 loss: 2.15925431\n",
      "iteration: 117 loss: 0.36305410\n",
      "iteration: 118 loss: 0.38094592\n",
      "iteration: 119 loss: 0.84536332\n",
      "iteration: 120 loss: 0.16493261\n",
      "iteration: 121 loss: 0.68885362\n",
      "iteration: 122 loss: 0.20555991\n",
      "iteration: 123 loss: 2.58268929\n",
      "iteration: 124 loss: 1.29455173\n",
      "iteration: 125 loss: 1.65589166\n",
      "iteration: 126 loss: 3.15976024\n",
      "iteration: 127 loss: 0.32005364\n",
      "iteration: 128 loss: 1.99959004\n",
      "iteration: 129 loss: 0.19676501\n",
      "iteration: 130 loss: 2.73377395\n",
      "iteration: 131 loss: 1.95940208\n",
      "iteration: 132 loss: 0.95192587\n",
      "iteration: 133 loss: 0.38916016\n",
      "iteration: 134 loss: 0.35375485\n",
      "iteration: 135 loss: 2.10109568\n",
      "iteration: 136 loss: 1.17821705\n",
      "iteration: 137 loss: 0.76970476\n",
      "iteration: 138 loss: 1.32088137\n",
      "iteration: 139 loss: 1.26270235\n",
      "iteration: 140 loss: 0.84168422\n",
      "iteration: 141 loss: 1.16143656\n",
      "iteration: 142 loss: 0.93342304\n",
      "iteration: 143 loss: 1.86588240\n",
      "iteration: 144 loss: 3.55184531\n",
      "iteration: 145 loss: 1.14027619\n",
      "iteration: 146 loss: 0.36833173\n",
      "iteration: 147 loss: 1.08107507\n",
      "iteration: 148 loss: 0.98281014\n",
      "iteration: 149 loss: 0.88176340\n",
      "iteration: 150 loss: 1.79893863\n",
      "iteration: 151 loss: 1.92691147\n",
      "iteration: 152 loss: 3.20382166\n",
      "iteration: 153 loss: 3.19624114\n",
      "iteration: 154 loss: 1.32436132\n",
      "iteration: 155 loss: 2.60648131\n",
      "iteration: 156 loss: 0.29610130\n",
      "iteration: 157 loss: 0.85120499\n",
      "iteration: 158 loss: 0.96827692\n",
      "iteration: 159 loss: 0.37192559\n",
      "iteration: 160 loss: 1.12378943\n",
      "iteration: 161 loss: 0.94346189\n",
      "iteration: 162 loss: 0.26400599\n",
      "iteration: 163 loss: 3.43733573\n",
      "iteration: 164 loss: 3.11163163\n",
      "iteration: 165 loss: 2.37237072\n",
      "iteration: 166 loss: 0.27152824\n",
      "iteration: 167 loss: 0.16655268\n",
      "iteration: 168 loss: 0.74107319\n",
      "iteration: 169 loss: 0.42293838\n",
      "iteration: 170 loss: 2.94423413\n",
      "iteration: 171 loss: 0.57312298\n",
      "iteration: 172 loss: 0.50883454\n",
      "iteration: 173 loss: 0.13657460\n",
      "iteration: 174 loss: 3.55965924\n",
      "iteration: 175 loss: 2.40200305\n",
      "iteration: 176 loss: 2.27171826\n",
      "iteration: 177 loss: 0.47188437\n",
      "iteration: 178 loss: 0.23429428\n",
      "iteration: 179 loss: 0.12786670\n",
      "iteration: 180 loss: 0.37568218\n",
      "iteration: 181 loss: 1.12215424\n",
      "iteration: 182 loss: 0.16417642\n",
      "iteration: 183 loss: 1.48581314\n",
      "iteration: 184 loss: 0.53446847\n",
      "iteration: 185 loss: 2.23733711\n",
      "iteration: 186 loss: 1.78865457\n",
      "iteration: 187 loss: 0.62332582\n",
      "iteration: 188 loss: 2.97592020\n",
      "iteration: 189 loss: 2.12961054\n",
      "iteration: 190 loss: 1.99665916\n",
      "iteration: 191 loss: 0.68083817\n",
      "iteration: 192 loss: 0.95637411\n",
      "iteration: 193 loss: 0.95505887\n",
      "iteration: 194 loss: 0.33764941\n",
      "iteration: 195 loss: 2.80969787\n",
      "iteration: 196 loss: 3.63233590\n",
      "iteration: 197 loss: 1.20364976\n",
      "iteration: 198 loss: 0.76332080\n",
      "iteration: 199 loss: 0.70502800\n",
      "epoch:  73 mean loss training: 1.21254313\n",
      "epoch:  73 mean loss validation: 1.40899074\n",
      "iteration:   0 loss: 3.22145557\n",
      "iteration:   1 loss: 0.74928480\n",
      "iteration:   2 loss: 0.89268386\n",
      "iteration:   3 loss: 0.87545741\n",
      "iteration:   4 loss: 1.41026843\n",
      "iteration:   5 loss: 1.41868997\n",
      "iteration:   6 loss: 1.58237839\n",
      "iteration:   7 loss: 0.93373692\n",
      "iteration:   8 loss: 2.87596107\n",
      "iteration:   9 loss: 4.34876394\n",
      "iteration:  10 loss: 0.52916712\n",
      "iteration:  11 loss: 0.39392668\n",
      "iteration:  12 loss: 1.32944298\n",
      "iteration:  13 loss: 0.96566510\n",
      "iteration:  14 loss: 1.05677247\n",
      "iteration:  15 loss: 0.57181126\n",
      "iteration:  16 loss: 0.80306983\n",
      "iteration:  17 loss: 0.68098414\n",
      "iteration:  18 loss: 1.47025728\n",
      "iteration:  19 loss: 1.12025499\n",
      "iteration:  20 loss: 0.55932766\n",
      "iteration:  21 loss: 0.80027199\n",
      "iteration:  22 loss: 0.18509644\n",
      "iteration:  23 loss: 1.10089064\n",
      "iteration:  24 loss: 0.30659246\n",
      "iteration:  25 loss: 2.55891180\n",
      "iteration:  26 loss: 1.05294490\n",
      "iteration:  27 loss: 3.34827471\n",
      "iteration:  28 loss: 0.18908678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  29 loss: 0.39863169\n",
      "iteration:  30 loss: 0.57619894\n",
      "iteration:  31 loss: 0.48419428\n",
      "iteration:  32 loss: 0.22215302\n",
      "iteration:  33 loss: 0.33827278\n",
      "iteration:  34 loss: 1.48653126\n",
      "iteration:  35 loss: 0.87973452\n",
      "iteration:  36 loss: 0.08257144\n",
      "iteration:  37 loss: 0.42545375\n",
      "iteration:  38 loss: 1.13946307\n",
      "iteration:  39 loss: 3.50400043\n",
      "iteration:  40 loss: 1.65222108\n",
      "iteration:  41 loss: 0.54811084\n",
      "iteration:  42 loss: 0.88039249\n",
      "iteration:  43 loss: 2.90712500\n",
      "iteration:  44 loss: 0.42618936\n",
      "iteration:  45 loss: 0.12236699\n",
      "iteration:  46 loss: 1.81212914\n",
      "iteration:  47 loss: 0.69985288\n",
      "iteration:  48 loss: 0.77325928\n",
      "iteration:  49 loss: 0.50010169\n",
      "iteration:  50 loss: 2.17594266\n",
      "iteration:  51 loss: 0.32278326\n",
      "iteration:  52 loss: 0.61115128\n",
      "iteration:  53 loss: 0.46511993\n",
      "iteration:  54 loss: 0.55877262\n",
      "iteration:  55 loss: 3.89875817\n",
      "iteration:  56 loss: 0.86902016\n",
      "iteration:  57 loss: 2.76019263\n",
      "iteration:  58 loss: 2.77127934\n",
      "iteration:  59 loss: 1.97077715\n",
      "iteration:  60 loss: 0.26816249\n",
      "iteration:  61 loss: 0.32453072\n",
      "iteration:  62 loss: 0.61034566\n",
      "iteration:  63 loss: 0.77723205\n",
      "iteration:  64 loss: 0.36381295\n",
      "iteration:  65 loss: 0.74017769\n",
      "iteration:  66 loss: 0.42308637\n",
      "iteration:  67 loss: 0.20069876\n",
      "iteration:  68 loss: 0.28256944\n",
      "iteration:  69 loss: 0.54036289\n",
      "iteration:  70 loss: 0.43524158\n",
      "iteration:  71 loss: 0.60614592\n",
      "iteration:  72 loss: 0.68755561\n",
      "iteration:  73 loss: 2.86874008\n",
      "iteration:  74 loss: 0.50077564\n",
      "iteration:  75 loss: 0.62547863\n",
      "iteration:  76 loss: 0.71051258\n",
      "iteration:  77 loss: 0.56178051\n",
      "iteration:  78 loss: 1.52760303\n",
      "iteration:  79 loss: 1.69707441\n",
      "iteration:  80 loss: 1.28377938\n",
      "iteration:  81 loss: 0.15985204\n",
      "iteration:  82 loss: 0.79748774\n",
      "iteration:  83 loss: 0.23922271\n",
      "iteration:  84 loss: 3.35779834\n",
      "iteration:  85 loss: 0.14013305\n",
      "iteration:  86 loss: 0.39178646\n",
      "iteration:  87 loss: 3.27751279\n",
      "iteration:  88 loss: 0.22284454\n",
      "iteration:  89 loss: 2.69163918\n",
      "iteration:  90 loss: 1.82798421\n",
      "iteration:  91 loss: 0.47803858\n",
      "iteration:  92 loss: 0.12332224\n",
      "iteration:  93 loss: 2.84718990\n",
      "iteration:  94 loss: 2.09252739\n",
      "iteration:  95 loss: 0.14346500\n",
      "iteration:  96 loss: 4.40621710\n",
      "iteration:  97 loss: 0.71983999\n",
      "iteration:  98 loss: 0.56060410\n",
      "iteration:  99 loss: 0.18960054\n",
      "iteration: 100 loss: 0.23827882\n",
      "iteration: 101 loss: 0.19416755\n",
      "iteration: 102 loss: 0.76263916\n",
      "iteration: 103 loss: 2.70942211\n",
      "iteration: 104 loss: 0.28199658\n",
      "iteration: 105 loss: 0.77050859\n",
      "iteration: 106 loss: 0.28162321\n",
      "iteration: 107 loss: 1.09383857\n",
      "iteration: 108 loss: 2.02085423\n",
      "iteration: 109 loss: 0.57977903\n",
      "iteration: 110 loss: 2.85175061\n",
      "iteration: 111 loss: 3.10563421\n",
      "iteration: 112 loss: 2.83287382\n",
      "iteration: 113 loss: 0.38353476\n",
      "iteration: 114 loss: 0.49426815\n",
      "iteration: 115 loss: 1.39750421\n",
      "iteration: 116 loss: 2.15549970\n",
      "iteration: 117 loss: 0.40415052\n",
      "iteration: 118 loss: 0.37138131\n",
      "iteration: 119 loss: 0.77967638\n",
      "iteration: 120 loss: 0.21291600\n",
      "iteration: 121 loss: 0.73710567\n",
      "iteration: 122 loss: 0.22766642\n",
      "iteration: 123 loss: 2.43019223\n",
      "iteration: 124 loss: 1.29329455\n",
      "iteration: 125 loss: 2.03224134\n",
      "iteration: 126 loss: 3.08576894\n",
      "iteration: 127 loss: 0.30091408\n",
      "iteration: 128 loss: 2.15949583\n",
      "iteration: 129 loss: 0.34202886\n",
      "iteration: 130 loss: 2.58421826\n",
      "iteration: 131 loss: 2.18164277\n",
      "iteration: 132 loss: 1.24575520\n",
      "iteration: 133 loss: 0.38846368\n",
      "iteration: 134 loss: 0.34137294\n",
      "iteration: 135 loss: 2.09297609\n",
      "iteration: 136 loss: 1.05631447\n",
      "iteration: 137 loss: 0.66354680\n",
      "iteration: 138 loss: 1.14174521\n",
      "iteration: 139 loss: 1.10525036\n",
      "iteration: 140 loss: 1.29572785\n",
      "iteration: 141 loss: 0.98674661\n",
      "iteration: 142 loss: 0.67253929\n",
      "iteration: 143 loss: 1.98413956\n",
      "iteration: 144 loss: 3.50671530\n",
      "iteration: 145 loss: 1.95880997\n",
      "iteration: 146 loss: 0.36650801\n",
      "iteration: 147 loss: 1.04337192\n",
      "iteration: 148 loss: 1.04145384\n",
      "iteration: 149 loss: 1.01027405\n",
      "iteration: 150 loss: 2.03348351\n",
      "iteration: 151 loss: 2.56835771\n",
      "iteration: 152 loss: 3.42892432\n",
      "iteration: 153 loss: 3.27082396\n",
      "iteration: 154 loss: 1.69695687\n",
      "iteration: 155 loss: 2.61421227\n",
      "iteration: 156 loss: 0.25846034\n",
      "iteration: 157 loss: 0.84226620\n",
      "iteration: 158 loss: 0.80242914\n",
      "iteration: 159 loss: 0.30289838\n",
      "iteration: 160 loss: 1.10094118\n",
      "iteration: 161 loss: 0.91328275\n",
      "iteration: 162 loss: 0.20894530\n",
      "iteration: 163 loss: 2.43867016\n",
      "iteration: 164 loss: 3.15149736\n",
      "iteration: 165 loss: 2.77417159\n",
      "iteration: 166 loss: 0.15120785\n",
      "iteration: 167 loss: 0.16691110\n",
      "iteration: 168 loss: 0.89382631\n",
      "iteration: 169 loss: 0.20958491\n",
      "iteration: 170 loss: 3.18515325\n",
      "iteration: 171 loss: 0.56320930\n",
      "iteration: 172 loss: 0.71479213\n",
      "iteration: 173 loss: 0.10096151\n",
      "iteration: 174 loss: 4.06292820\n",
      "iteration: 175 loss: 2.63577199\n",
      "iteration: 176 loss: 1.83620322\n",
      "iteration: 177 loss: 0.43253633\n",
      "iteration: 178 loss: 0.29536802\n",
      "iteration: 179 loss: 0.13862261\n",
      "iteration: 180 loss: 0.34574953\n",
      "iteration: 181 loss: 1.17104757\n",
      "iteration: 182 loss: 0.36477092\n",
      "iteration: 183 loss: 1.22034621\n",
      "iteration: 184 loss: 0.42300472\n",
      "iteration: 185 loss: 1.90645468\n",
      "iteration: 186 loss: 1.25545561\n",
      "iteration: 187 loss: 0.79010344\n",
      "iteration: 188 loss: 3.09175014\n",
      "iteration: 189 loss: 2.19721389\n",
      "iteration: 190 loss: 1.60082603\n",
      "iteration: 191 loss: 0.35976616\n",
      "iteration: 192 loss: 0.69554150\n",
      "iteration: 193 loss: 1.23309803\n",
      "iteration: 194 loss: 0.54674989\n",
      "iteration: 195 loss: 2.67884374\n",
      "iteration: 196 loss: 3.37185669\n",
      "iteration: 197 loss: 0.79382038\n",
      "iteration: 198 loss: 0.71814913\n",
      "iteration: 199 loss: 0.53309411\n",
      "epoch:  74 mean loss training: 1.23293269\n",
      "epoch:  74 mean loss validation: 1.35124838\n",
      "iteration:   0 loss: 2.97335076\n",
      "iteration:   1 loss: 0.77867854\n",
      "iteration:   2 loss: 0.80872881\n",
      "iteration:   3 loss: 0.75708389\n",
      "iteration:   4 loss: 2.46761990\n",
      "iteration:   5 loss: 1.80362189\n",
      "iteration:   6 loss: 1.42336833\n",
      "iteration:   7 loss: 0.95492291\n",
      "iteration:   8 loss: 2.58717752\n",
      "iteration:   9 loss: 3.84946322\n",
      "iteration:  10 loss: 0.62540424\n",
      "iteration:  11 loss: 0.41677868\n",
      "iteration:  12 loss: 0.90860307\n",
      "iteration:  13 loss: 1.23182118\n",
      "iteration:  14 loss: 1.05031979\n",
      "iteration:  15 loss: 0.55454332\n",
      "iteration:  16 loss: 0.72355855\n",
      "iteration:  17 loss: 1.03949797\n",
      "iteration:  18 loss: 1.53733873\n",
      "iteration:  19 loss: 0.34892237\n",
      "iteration:  20 loss: 0.66550571\n",
      "iteration:  21 loss: 0.42094448\n",
      "iteration:  22 loss: 0.20253731\n",
      "iteration:  23 loss: 0.94158351\n",
      "iteration:  24 loss: 0.23260503\n",
      "iteration:  25 loss: 2.75871062\n",
      "iteration:  26 loss: 1.29036903\n",
      "iteration:  27 loss: 2.96645331\n",
      "iteration:  28 loss: 0.15256505\n",
      "iteration:  29 loss: 0.33651307\n",
      "iteration:  30 loss: 0.39903075\n",
      "iteration:  31 loss: 0.20671602\n",
      "iteration:  32 loss: 0.18218109\n",
      "iteration:  33 loss: 0.27127239\n",
      "iteration:  34 loss: 1.91582227\n",
      "iteration:  35 loss: 1.07744896\n",
      "iteration:  36 loss: 0.08963433\n",
      "iteration:  37 loss: 0.37231106\n",
      "iteration:  38 loss: 1.13632596\n",
      "iteration:  39 loss: 3.30438042\n",
      "iteration:  40 loss: 1.85624814\n",
      "iteration:  41 loss: 0.61997789\n",
      "iteration:  42 loss: 0.81889683\n",
      "iteration:  43 loss: 2.89381909\n",
      "iteration:  44 loss: 0.50072116\n",
      "iteration:  45 loss: 0.14848034\n",
      "iteration:  46 loss: 1.35781729\n",
      "iteration:  47 loss: 0.70989591\n",
      "iteration:  48 loss: 0.71944273\n",
      "iteration:  49 loss: 0.50158370\n",
      "iteration:  50 loss: 2.13741517\n",
      "iteration:  51 loss: 0.31520265\n",
      "iteration:  52 loss: 0.44935757\n",
      "iteration:  53 loss: 0.36484528\n",
      "iteration:  54 loss: 0.41389900\n",
      "iteration:  55 loss: 4.22849894\n",
      "iteration:  56 loss: 0.71258110\n",
      "iteration:  57 loss: 2.79652739\n",
      "iteration:  58 loss: 2.65222764\n",
      "iteration:  59 loss: 1.85330200\n",
      "iteration:  60 loss: 0.30516899\n",
      "iteration:  61 loss: 0.31645757\n",
      "iteration:  62 loss: 0.45929319\n",
      "iteration:  63 loss: 0.60436118\n",
      "iteration:  64 loss: 0.29705018\n",
      "iteration:  65 loss: 0.49484679\n",
      "iteration:  66 loss: 0.39915344\n",
      "iteration:  67 loss: 0.13374414\n",
      "iteration:  68 loss: 0.12040180\n",
      "iteration:  69 loss: 0.52090019\n",
      "iteration:  70 loss: 0.28842589\n",
      "iteration:  71 loss: 0.69158822\n",
      "iteration:  72 loss: 0.84872204\n",
      "iteration:  73 loss: 2.52850938\n",
      "iteration:  74 loss: 0.37034899\n",
      "iteration:  75 loss: 0.60680836\n",
      "iteration:  76 loss: 0.70888305\n",
      "iteration:  77 loss: 1.11359465\n",
      "iteration:  78 loss: 1.25403917\n",
      "iteration:  79 loss: 0.85586506\n",
      "iteration:  80 loss: 1.67233479\n",
      "iteration:  81 loss: 0.13687001\n",
      "iteration:  82 loss: 0.28849629\n",
      "iteration:  83 loss: 0.73292518\n",
      "iteration:  84 loss: 3.57885647\n",
      "iteration:  85 loss: 0.14632791\n",
      "iteration:  86 loss: 0.53778690\n",
      "iteration:  87 loss: 3.23303747\n",
      "iteration:  88 loss: 0.29974788\n",
      "iteration:  89 loss: 2.29150271\n",
      "iteration:  90 loss: 1.37167394\n",
      "iteration:  91 loss: 0.33683309\n",
      "iteration:  92 loss: 0.15926771\n",
      "iteration:  93 loss: 0.73487091\n",
      "iteration:  94 loss: 0.98842019\n",
      "iteration:  95 loss: 0.70124972\n",
      "iteration:  96 loss: 3.07950640\n",
      "iteration:  97 loss: 0.57600766\n",
      "iteration:  98 loss: 0.59457952\n",
      "iteration:  99 loss: 0.32153636\n",
      "iteration: 100 loss: 0.37686208\n",
      "iteration: 101 loss: 0.16515797\n",
      "iteration: 102 loss: 1.05552793\n",
      "iteration: 103 loss: 2.77710128\n",
      "iteration: 104 loss: 0.34909016\n",
      "iteration: 105 loss: 0.98903185\n",
      "iteration: 106 loss: 0.55322915\n",
      "iteration: 107 loss: 0.40926209\n",
      "iteration: 108 loss: 1.32582426\n",
      "iteration: 109 loss: 0.48339894\n",
      "iteration: 110 loss: 2.05505848\n",
      "iteration: 111 loss: 3.15212679\n",
      "iteration: 112 loss: 2.87647939\n",
      "iteration: 113 loss: 0.48333821\n",
      "iteration: 114 loss: 0.95221049\n",
      "iteration: 115 loss: 2.04600120\n",
      "iteration: 116 loss: 2.30599618\n",
      "iteration: 117 loss: 0.25716326\n",
      "iteration: 118 loss: 0.32393962\n",
      "iteration: 119 loss: 0.60067427\n",
      "iteration: 120 loss: 0.28428236\n",
      "iteration: 121 loss: 0.65024972\n",
      "iteration: 122 loss: 0.27350298\n",
      "iteration: 123 loss: 2.41417933\n",
      "iteration: 124 loss: 1.42218983\n",
      "iteration: 125 loss: 2.17376089\n",
      "iteration: 126 loss: 2.92369008\n",
      "iteration: 127 loss: 0.29640159\n",
      "iteration: 128 loss: 2.03170419\n",
      "iteration: 129 loss: 0.17026237\n",
      "iteration: 130 loss: 2.73059559\n",
      "iteration: 131 loss: 2.13705373\n",
      "iteration: 132 loss: 0.66045636\n",
      "iteration: 133 loss: 0.39329857\n",
      "iteration: 134 loss: 0.43655905\n",
      "iteration: 135 loss: 2.13478065\n",
      "iteration: 136 loss: 1.17352903\n",
      "iteration: 137 loss: 0.89649725\n",
      "iteration: 138 loss: 1.29963756\n",
      "iteration: 139 loss: 1.30878270\n",
      "iteration: 140 loss: 0.83959830\n",
      "iteration: 141 loss: 1.21566308\n",
      "iteration: 142 loss: 1.09151173\n",
      "iteration: 143 loss: 0.82465255\n",
      "iteration: 144 loss: 3.78342056\n",
      "iteration: 145 loss: 1.12602484\n",
      "iteration: 146 loss: 0.33758122\n",
      "iteration: 147 loss: 1.14001226\n",
      "iteration: 148 loss: 0.89739197\n",
      "iteration: 149 loss: 0.95560908\n",
      "iteration: 150 loss: 2.40874600\n",
      "iteration: 151 loss: 1.43404627\n",
      "iteration: 152 loss: 3.17114830\n",
      "iteration: 153 loss: 3.27874565\n",
      "iteration: 154 loss: 1.61035979\n",
      "iteration: 155 loss: 2.91166759\n",
      "iteration: 156 loss: 0.19838589\n",
      "iteration: 157 loss: 0.80592072\n",
      "iteration: 158 loss: 0.80503738\n",
      "iteration: 159 loss: 0.23525064\n",
      "iteration: 160 loss: 1.02367079\n",
      "iteration: 161 loss: 0.99743533\n",
      "iteration: 162 loss: 0.21020545\n",
      "iteration: 163 loss: 3.22191548\n",
      "iteration: 164 loss: 3.43563795\n",
      "iteration: 165 loss: 2.44033051\n",
      "iteration: 166 loss: 0.17494781\n",
      "iteration: 167 loss: 0.21367475\n",
      "iteration: 168 loss: 0.79359150\n",
      "iteration: 169 loss: 0.29727909\n",
      "iteration: 170 loss: 2.78012729\n",
      "iteration: 171 loss: 0.51743412\n",
      "iteration: 172 loss: 0.45455754\n",
      "iteration: 173 loss: 0.12704307\n",
      "iteration: 174 loss: 3.50335407\n",
      "iteration: 175 loss: 2.01680517\n",
      "iteration: 176 loss: 0.82537293\n",
      "iteration: 177 loss: 0.33971891\n",
      "iteration: 178 loss: 0.25189739\n",
      "iteration: 179 loss: 0.12895019\n",
      "iteration: 180 loss: 0.19723159\n",
      "iteration: 181 loss: 1.42155898\n",
      "iteration: 182 loss: 0.12654804\n",
      "iteration: 183 loss: 0.94424343\n",
      "iteration: 184 loss: 0.47697535\n",
      "iteration: 185 loss: 2.58949137\n",
      "iteration: 186 loss: 1.97747290\n",
      "iteration: 187 loss: 0.57743484\n",
      "iteration: 188 loss: 3.29691148\n",
      "iteration: 189 loss: 2.11900091\n",
      "iteration: 190 loss: 2.45342445\n",
      "iteration: 191 loss: 0.46518424\n",
      "iteration: 192 loss: 0.76255810\n",
      "iteration: 193 loss: 1.27854347\n",
      "iteration: 194 loss: 0.57334501\n",
      "iteration: 195 loss: 2.86252356\n",
      "iteration: 196 loss: 3.26281500\n",
      "iteration: 197 loss: 0.77389240\n",
      "iteration: 198 loss: 0.77535963\n",
      "iteration: 199 loss: 0.61156297\n",
      "epoch:  75 mean loss training: 1.18117070\n",
      "epoch:  75 mean loss validation: 1.35373712\n",
      "iteration:   0 loss: 2.56553268\n",
      "iteration:   1 loss: 0.69361919\n",
      "iteration:   2 loss: 0.61721736\n",
      "iteration:   3 loss: 0.53640079\n",
      "iteration:   4 loss: 2.58344126\n",
      "iteration:   5 loss: 1.88778806\n",
      "iteration:   6 loss: 1.83470154\n",
      "iteration:   7 loss: 1.23608553\n",
      "iteration:   8 loss: 2.54414058\n",
      "iteration:   9 loss: 3.24013662\n",
      "iteration:  10 loss: 0.54017216\n",
      "iteration:  11 loss: 1.30724394\n",
      "iteration:  12 loss: 1.17339396\n",
      "iteration:  13 loss: 1.19189584\n",
      "iteration:  14 loss: 1.06396747\n",
      "iteration:  15 loss: 0.49032333\n",
      "iteration:  16 loss: 0.40130320\n",
      "iteration:  17 loss: 0.87343752\n",
      "iteration:  18 loss: 1.46873605\n",
      "iteration:  19 loss: 0.62886280\n",
      "iteration:  20 loss: 0.63120002\n",
      "iteration:  21 loss: 0.87871826\n",
      "iteration:  22 loss: 0.24008358\n",
      "iteration:  23 loss: 0.97905302\n",
      "iteration:  24 loss: 0.11501168\n",
      "iteration:  25 loss: 2.74590182\n",
      "iteration:  26 loss: 1.28283012\n",
      "iteration:  27 loss: 2.99907947\n",
      "iteration:  28 loss: 0.28688857\n",
      "iteration:  29 loss: 0.47711426\n",
      "iteration:  30 loss: 0.42199999\n",
      "iteration:  31 loss: 0.25203097\n",
      "iteration:  32 loss: 0.33441177\n",
      "iteration:  33 loss: 0.20888317\n",
      "iteration:  34 loss: 1.83936441\n",
      "iteration:  35 loss: 1.18397474\n",
      "iteration:  36 loss: 0.11161009\n",
      "iteration:  37 loss: 0.40174857\n",
      "iteration:  38 loss: 1.15245914\n",
      "iteration:  39 loss: 3.26735878\n",
      "iteration:  40 loss: 1.67935276\n",
      "iteration:  41 loss: 0.77297729\n",
      "iteration:  42 loss: 0.95151997\n",
      "iteration:  43 loss: 2.91534781\n",
      "iteration:  44 loss: 0.34373811\n",
      "iteration:  45 loss: 0.12131785\n",
      "iteration:  46 loss: 1.08442569\n",
      "iteration:  47 loss: 0.79441828\n",
      "iteration:  48 loss: 0.74298155\n",
      "iteration:  49 loss: 0.49986312\n",
      "iteration:  50 loss: 2.29888964\n",
      "iteration:  51 loss: 0.33815882\n",
      "iteration:  52 loss: 0.75756145\n",
      "iteration:  53 loss: 0.36382571\n",
      "iteration:  54 loss: 0.37338117\n",
      "iteration:  55 loss: 4.30850172\n",
      "iteration:  56 loss: 0.74412191\n",
      "iteration:  57 loss: 2.89436460\n",
      "iteration:  58 loss: 2.53272080\n",
      "iteration:  59 loss: 1.59498596\n",
      "iteration:  60 loss: 0.37346277\n",
      "iteration:  61 loss: 0.20921163\n",
      "iteration:  62 loss: 1.00330281\n",
      "iteration:  63 loss: 0.70385683\n",
      "iteration:  64 loss: 0.31036273\n",
      "iteration:  65 loss: 0.58386070\n",
      "iteration:  66 loss: 0.39963880\n",
      "iteration:  67 loss: 0.10478097\n",
      "iteration:  68 loss: 0.11223581\n",
      "iteration:  69 loss: 0.36393666\n",
      "iteration:  70 loss: 0.33348095\n",
      "iteration:  71 loss: 0.46585646\n",
      "iteration:  72 loss: 0.68559945\n",
      "iteration:  73 loss: 2.56541324\n",
      "iteration:  74 loss: 0.33121687\n",
      "iteration:  75 loss: 0.47776029\n",
      "iteration:  76 loss: 0.71467805\n",
      "iteration:  77 loss: 0.97907919\n",
      "iteration:  78 loss: 1.30015790\n",
      "iteration:  79 loss: 0.45526657\n",
      "iteration:  80 loss: 1.77459228\n",
      "iteration:  81 loss: 0.19036563\n",
      "iteration:  82 loss: 0.30812213\n",
      "iteration:  83 loss: 0.78572077\n",
      "iteration:  84 loss: 3.17515182\n",
      "iteration:  85 loss: 0.14490934\n",
      "iteration:  86 loss: 0.51971954\n",
      "iteration:  87 loss: 3.19766355\n",
      "iteration:  88 loss: 0.31720924\n",
      "iteration:  89 loss: 2.29039311\n",
      "iteration:  90 loss: 1.25165141\n",
      "iteration:  91 loss: 0.38909128\n",
      "iteration:  92 loss: 0.20089830\n",
      "iteration:  93 loss: 2.11670351\n",
      "iteration:  94 loss: 0.83370036\n",
      "iteration:  95 loss: 0.71503603\n",
      "iteration:  96 loss: 3.05969262\n",
      "iteration:  97 loss: 0.61865753\n",
      "iteration:  98 loss: 0.56010634\n",
      "iteration:  99 loss: 0.24996959\n",
      "iteration: 100 loss: 0.33481881\n",
      "iteration: 101 loss: 0.17565699\n",
      "iteration: 102 loss: 0.92883921\n",
      "iteration: 103 loss: 2.81894279\n",
      "iteration: 104 loss: 0.33045223\n",
      "iteration: 105 loss: 0.84106028\n",
      "iteration: 106 loss: 0.37953404\n",
      "iteration: 107 loss: 0.28887111\n",
      "iteration: 108 loss: 1.35872388\n",
      "iteration: 109 loss: 0.46198365\n",
      "iteration: 110 loss: 2.02595973\n",
      "iteration: 111 loss: 3.12326169\n",
      "iteration: 112 loss: 2.75241256\n",
      "iteration: 113 loss: 0.40411422\n",
      "iteration: 114 loss: 0.71577162\n",
      "iteration: 115 loss: 1.74184287\n",
      "iteration: 116 loss: 2.14429784\n",
      "iteration: 117 loss: 0.31503528\n",
      "iteration: 118 loss: 0.30116388\n",
      "iteration: 119 loss: 0.51062530\n",
      "iteration: 120 loss: 0.25649136\n",
      "iteration: 121 loss: 0.65578800\n",
      "iteration: 122 loss: 0.16495597\n",
      "iteration: 123 loss: 2.35717463\n",
      "iteration: 124 loss: 1.37011755\n",
      "iteration: 125 loss: 2.02562332\n",
      "iteration: 126 loss: 3.16077065\n",
      "iteration: 127 loss: 0.27787387\n",
      "iteration: 128 loss: 2.15982270\n",
      "iteration: 129 loss: 0.17338713\n",
      "iteration: 130 loss: 2.72064233\n",
      "iteration: 131 loss: 2.24417281\n",
      "iteration: 132 loss: 0.85999149\n",
      "iteration: 133 loss: 0.43167901\n",
      "iteration: 134 loss: 0.36543581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 135 loss: 2.10790849\n",
      "iteration: 136 loss: 1.02190173\n",
      "iteration: 137 loss: 0.91333115\n",
      "iteration: 138 loss: 1.17273045\n",
      "iteration: 139 loss: 1.18392229\n",
      "iteration: 140 loss: 0.89557791\n",
      "iteration: 141 loss: 1.20013893\n",
      "iteration: 142 loss: 0.97239804\n",
      "iteration: 143 loss: 0.72808743\n",
      "iteration: 144 loss: 3.70584011\n",
      "iteration: 145 loss: 1.57297993\n",
      "iteration: 146 loss: 0.28615785\n",
      "iteration: 147 loss: 0.97127098\n",
      "iteration: 148 loss: 0.93488735\n",
      "iteration: 149 loss: 0.64957690\n",
      "iteration: 150 loss: 1.83907557\n",
      "iteration: 151 loss: 2.03080869\n",
      "iteration: 152 loss: 3.31758833\n",
      "iteration: 153 loss: 3.25574446\n",
      "iteration: 154 loss: 1.52301526\n",
      "iteration: 155 loss: 2.60276318\n",
      "iteration: 156 loss: 0.23748021\n",
      "iteration: 157 loss: 0.78855115\n",
      "iteration: 158 loss: 0.85827345\n",
      "iteration: 159 loss: 0.24590893\n",
      "iteration: 160 loss: 1.07295740\n",
      "iteration: 161 loss: 0.96105802\n",
      "iteration: 162 loss: 0.24115986\n",
      "iteration: 163 loss: 2.52398634\n",
      "iteration: 164 loss: 3.11066580\n",
      "iteration: 165 loss: 0.84883791\n",
      "iteration: 166 loss: 0.17492676\n",
      "iteration: 167 loss: 0.17262004\n",
      "iteration: 168 loss: 0.82021379\n",
      "iteration: 169 loss: 0.30750474\n",
      "iteration: 170 loss: 2.89767981\n",
      "iteration: 171 loss: 0.53030282\n",
      "iteration: 172 loss: 0.42533988\n",
      "iteration: 173 loss: 0.13435772\n",
      "iteration: 174 loss: 3.53005934\n",
      "iteration: 175 loss: 2.36981440\n",
      "iteration: 176 loss: 0.95010549\n",
      "iteration: 177 loss: 0.51535177\n",
      "iteration: 178 loss: 0.22959115\n",
      "iteration: 179 loss: 0.13202555\n",
      "iteration: 180 loss: 0.26449570\n",
      "iteration: 181 loss: 1.43441689\n",
      "iteration: 182 loss: 0.15760085\n",
      "iteration: 183 loss: 1.01456046\n",
      "iteration: 184 loss: 0.60565901\n",
      "iteration: 185 loss: 1.84208333\n",
      "iteration: 186 loss: 2.00707388\n",
      "iteration: 187 loss: 0.53025478\n",
      "iteration: 188 loss: 3.14739466\n",
      "iteration: 189 loss: 2.11487675\n",
      "iteration: 190 loss: 2.42317605\n",
      "iteration: 191 loss: 0.54700601\n",
      "iteration: 192 loss: 0.86131418\n",
      "iteration: 193 loss: 1.31372523\n",
      "iteration: 194 loss: 0.29739466\n",
      "iteration: 195 loss: 2.72258139\n",
      "iteration: 196 loss: 3.44304228\n",
      "iteration: 197 loss: 1.08964837\n",
      "iteration: 198 loss: 0.82057446\n",
      "iteration: 199 loss: 0.69284910\n",
      "epoch:  76 mean loss training: 1.16722286\n",
      "epoch:  76 mean loss validation: 1.40894186\n",
      "iteration:   0 loss: 3.00844026\n",
      "iteration:   1 loss: 0.70668823\n",
      "iteration:   2 loss: 0.82654744\n",
      "iteration:   3 loss: 0.83201164\n",
      "iteration:   4 loss: 1.43036687\n",
      "iteration:   5 loss: 1.29404938\n",
      "iteration:   6 loss: 1.52035117\n",
      "iteration:   7 loss: 0.95643264\n",
      "iteration:   8 loss: 2.79202104\n",
      "iteration:   9 loss: 4.31978321\n",
      "iteration:  10 loss: 0.51692516\n",
      "iteration:  11 loss: 0.37727979\n",
      "iteration:  12 loss: 1.28316295\n",
      "iteration:  13 loss: 1.10970116\n",
      "iteration:  14 loss: 1.05455434\n",
      "iteration:  15 loss: 0.59974784\n",
      "iteration:  16 loss: 0.88282371\n",
      "iteration:  17 loss: 0.73281276\n",
      "iteration:  18 loss: 1.63886452\n",
      "iteration:  19 loss: 0.96470582\n",
      "iteration:  20 loss: 0.69873363\n",
      "iteration:  21 loss: 0.54929066\n",
      "iteration:  22 loss: 0.15949388\n",
      "iteration:  23 loss: 1.13489735\n",
      "iteration:  24 loss: 0.22798312\n",
      "iteration:  25 loss: 2.56096125\n",
      "iteration:  26 loss: 1.25647950\n",
      "iteration:  27 loss: 3.57992291\n",
      "iteration:  28 loss: 0.14815053\n",
      "iteration:  29 loss: 0.35915330\n",
      "iteration:  30 loss: 0.36549428\n",
      "iteration:  31 loss: 0.20258807\n",
      "iteration:  32 loss: 0.16763458\n",
      "iteration:  33 loss: 0.27372992\n",
      "iteration:  34 loss: 1.57149851\n",
      "iteration:  35 loss: 1.15999377\n",
      "iteration:  36 loss: 0.08908071\n",
      "iteration:  37 loss: 0.41775131\n",
      "iteration:  38 loss: 1.15040922\n",
      "iteration:  39 loss: 3.29113078\n",
      "iteration:  40 loss: 1.67884767\n",
      "iteration:  41 loss: 0.53837907\n",
      "iteration:  42 loss: 0.84210974\n",
      "iteration:  43 loss: 2.96304321\n",
      "iteration:  44 loss: 0.70209152\n",
      "iteration:  45 loss: 0.13717431\n",
      "iteration:  46 loss: 1.38862205\n",
      "iteration:  47 loss: 0.94275051\n",
      "iteration:  48 loss: 0.73107463\n",
      "iteration:  49 loss: 0.50157100\n",
      "iteration:  50 loss: 2.35872412\n",
      "iteration:  51 loss: 0.31372723\n",
      "iteration:  52 loss: 0.78166938\n",
      "iteration:  53 loss: 0.86807191\n",
      "iteration:  54 loss: 0.41164777\n",
      "iteration:  55 loss: 3.73466372\n",
      "iteration:  56 loss: 0.97278184\n",
      "iteration:  57 loss: 2.90713453\n",
      "iteration:  58 loss: 2.55853868\n",
      "iteration:  59 loss: 2.17787814\n",
      "iteration:  60 loss: 0.37858441\n",
      "iteration:  61 loss: 0.25424656\n",
      "iteration:  62 loss: 0.56347299\n",
      "iteration:  63 loss: 0.97543216\n",
      "iteration:  64 loss: 0.30626753\n",
      "iteration:  65 loss: 0.71677899\n",
      "iteration:  66 loss: 0.40875319\n",
      "iteration:  67 loss: 0.20397323\n",
      "iteration:  68 loss: 0.15395728\n",
      "iteration:  69 loss: 0.33148068\n",
      "iteration:  70 loss: 0.32635182\n",
      "iteration:  71 loss: 0.77440375\n",
      "iteration:  72 loss: 0.61471617\n",
      "iteration:  73 loss: 2.52662325\n",
      "iteration:  74 loss: 0.76358491\n",
      "iteration:  75 loss: 0.88917017\n",
      "iteration:  76 loss: 0.67166466\n",
      "iteration:  77 loss: 0.44548672\n",
      "iteration:  78 loss: 1.47488010\n",
      "iteration:  79 loss: 0.58354324\n",
      "iteration:  80 loss: 1.51596713\n",
      "iteration:  81 loss: 0.14787084\n",
      "iteration:  82 loss: 0.45833871\n",
      "iteration:  83 loss: 0.18375784\n",
      "iteration:  84 loss: 2.90660429\n",
      "iteration:  85 loss: 0.90771598\n",
      "iteration:  86 loss: 0.60857087\n",
      "iteration:  87 loss: 3.35543108\n",
      "iteration:  88 loss: 0.35570824\n",
      "iteration:  89 loss: 2.53685665\n",
      "iteration:  90 loss: 0.89582282\n",
      "iteration:  91 loss: 0.38794887\n",
      "iteration:  92 loss: 0.09797837\n",
      "iteration:  93 loss: 2.76650214\n",
      "iteration:  94 loss: 1.80087185\n",
      "iteration:  95 loss: 0.75205189\n",
      "iteration:  96 loss: 3.28969550\n",
      "iteration:  97 loss: 0.50055313\n",
      "iteration:  98 loss: 0.58827668\n",
      "iteration:  99 loss: 0.31366622\n",
      "iteration: 100 loss: 0.22515617\n",
      "iteration: 101 loss: 0.18006229\n",
      "iteration: 102 loss: 0.83597940\n",
      "iteration: 103 loss: 2.88448024\n",
      "iteration: 104 loss: 0.41404718\n",
      "iteration: 105 loss: 0.86270463\n",
      "iteration: 106 loss: 0.42610377\n",
      "iteration: 107 loss: 0.39521709\n",
      "iteration: 108 loss: 1.66862535\n",
      "iteration: 109 loss: 0.67618310\n",
      "iteration: 110 loss: 1.68659174\n",
      "iteration: 111 loss: 3.16136074\n",
      "iteration: 112 loss: 2.69917226\n",
      "iteration: 113 loss: 0.35332650\n",
      "iteration: 114 loss: 1.08594799\n",
      "iteration: 115 loss: 1.45792520\n",
      "iteration: 116 loss: 2.30533504\n",
      "iteration: 117 loss: 0.31610918\n",
      "iteration: 118 loss: 0.39708623\n",
      "iteration: 119 loss: 0.78294510\n",
      "iteration: 120 loss: 0.15129708\n",
      "iteration: 121 loss: 0.74654293\n",
      "iteration: 122 loss: 0.20762345\n",
      "iteration: 123 loss: 2.49240756\n",
      "iteration: 124 loss: 1.09407091\n",
      "iteration: 125 loss: 1.64129210\n",
      "iteration: 126 loss: 3.28400373\n",
      "iteration: 127 loss: 0.27614552\n",
      "iteration: 128 loss: 2.29015017\n",
      "iteration: 129 loss: 0.18194892\n",
      "iteration: 130 loss: 2.72976971\n",
      "iteration: 131 loss: 2.43203735\n",
      "iteration: 132 loss: 1.11527503\n",
      "iteration: 133 loss: 0.39612970\n",
      "iteration: 134 loss: 0.38007683\n",
      "iteration: 135 loss: 2.08947706\n",
      "iteration: 136 loss: 1.00497127\n",
      "iteration: 137 loss: 0.75455856\n",
      "iteration: 138 loss: 0.93806547\n",
      "iteration: 139 loss: 1.09059298\n",
      "iteration: 140 loss: 1.12168789\n",
      "iteration: 141 loss: 0.86241800\n",
      "iteration: 142 loss: 0.82370305\n",
      "iteration: 143 loss: 1.89604795\n",
      "iteration: 144 loss: 3.52039242\n",
      "iteration: 145 loss: 1.16658044\n",
      "iteration: 146 loss: 0.23316380\n",
      "iteration: 147 loss: 0.94410503\n",
      "iteration: 148 loss: 0.80133808\n",
      "iteration: 149 loss: 0.90685290\n",
      "iteration: 150 loss: 1.82236302\n",
      "iteration: 151 loss: 2.05226731\n",
      "iteration: 152 loss: 3.18005753\n",
      "iteration: 153 loss: 3.23098564\n",
      "iteration: 154 loss: 1.35477543\n",
      "iteration: 155 loss: 2.56703711\n",
      "iteration: 156 loss: 0.19882992\n",
      "iteration: 157 loss: 0.80321074\n",
      "iteration: 158 loss: 0.80480230\n",
      "iteration: 159 loss: 0.23276082\n",
      "iteration: 160 loss: 1.10640264\n",
      "iteration: 161 loss: 0.93227589\n",
      "iteration: 162 loss: 0.22655858\n",
      "iteration: 163 loss: 2.75232220\n",
      "iteration: 164 loss: 3.12804556\n",
      "iteration: 165 loss: 2.91056895\n",
      "iteration: 166 loss: 0.13859172\n",
      "iteration: 167 loss: 0.16256581\n",
      "iteration: 168 loss: 0.94253230\n",
      "iteration: 169 loss: 0.23181839\n",
      "iteration: 170 loss: 2.91599894\n",
      "iteration: 171 loss: 0.53363138\n",
      "iteration: 172 loss: 0.70333302\n",
      "iteration: 173 loss: 0.11650307\n",
      "iteration: 174 loss: 3.58264613\n",
      "iteration: 175 loss: 1.87405300\n",
      "iteration: 176 loss: 1.54908812\n",
      "iteration: 177 loss: 0.54159933\n",
      "iteration: 178 loss: 0.26230177\n",
      "iteration: 179 loss: 0.11345995\n",
      "iteration: 180 loss: 0.28439388\n",
      "iteration: 181 loss: 1.49548304\n",
      "iteration: 182 loss: 0.15498729\n",
      "iteration: 183 loss: 1.35133171\n",
      "iteration: 184 loss: 0.33472225\n",
      "iteration: 185 loss: 2.21701193\n",
      "iteration: 186 loss: 1.86219060\n",
      "iteration: 187 loss: 0.66658622\n",
      "iteration: 188 loss: 3.22412753\n",
      "iteration: 189 loss: 2.20863724\n",
      "iteration: 190 loss: 1.99736989\n",
      "iteration: 191 loss: 0.32878971\n",
      "iteration: 192 loss: 0.70622718\n",
      "iteration: 193 loss: 1.18946612\n",
      "iteration: 194 loss: 0.36974865\n",
      "iteration: 195 loss: 2.64359355\n",
      "iteration: 196 loss: 3.35003138\n",
      "iteration: 197 loss: 0.67235208\n",
      "iteration: 198 loss: 0.79937202\n",
      "iteration: 199 loss: 0.55053627\n",
      "epoch:  77 mean loss training: 1.19610572\n",
      "epoch:  77 mean loss validation: 1.35149682\n",
      "iteration:   0 loss: 2.62671351\n",
      "iteration:   1 loss: 0.74257177\n",
      "iteration:   2 loss: 0.79196692\n",
      "iteration:   3 loss: 0.80944419\n",
      "iteration:   4 loss: 2.13412452\n",
      "iteration:   5 loss: 1.65219712\n",
      "iteration:   6 loss: 1.31596255\n",
      "iteration:   7 loss: 0.88688725\n",
      "iteration:   8 loss: 3.08822107\n",
      "iteration:   9 loss: 4.42386627\n",
      "iteration:  10 loss: 0.72301507\n",
      "iteration:  11 loss: 0.43060005\n",
      "iteration:  12 loss: 0.84106672\n",
      "iteration:  13 loss: 1.50641263\n",
      "iteration:  14 loss: 1.07940948\n",
      "iteration:  15 loss: 0.65982699\n",
      "iteration:  16 loss: 1.63526070\n",
      "iteration:  17 loss: 0.91251558\n",
      "iteration:  18 loss: 1.66388011\n",
      "iteration:  19 loss: 1.04714966\n",
      "iteration:  20 loss: 0.81114751\n",
      "iteration:  21 loss: 0.57500821\n",
      "iteration:  22 loss: 0.26329800\n",
      "iteration:  23 loss: 0.98456395\n",
      "iteration:  24 loss: 0.20838939\n",
      "iteration:  25 loss: 2.03056192\n",
      "iteration:  26 loss: 1.25228870\n",
      "iteration:  27 loss: 2.57887220\n",
      "iteration:  28 loss: 0.29526615\n",
      "iteration:  29 loss: 0.39026043\n",
      "iteration:  30 loss: 0.37396026\n",
      "iteration:  31 loss: 0.24176328\n",
      "iteration:  32 loss: 0.46116424\n",
      "iteration:  33 loss: 0.19993255\n",
      "iteration:  34 loss: 1.44073069\n",
      "iteration:  35 loss: 1.14320135\n",
      "iteration:  36 loss: 0.09537151\n",
      "iteration:  37 loss: 0.42613900\n",
      "iteration:  38 loss: 1.14342153\n",
      "iteration:  39 loss: 3.31444931\n",
      "iteration:  40 loss: 2.01940703\n",
      "iteration:  41 loss: 0.58412391\n",
      "iteration:  42 loss: 0.75849664\n",
      "iteration:  43 loss: 2.90037608\n",
      "iteration:  44 loss: 0.66694391\n",
      "iteration:  45 loss: 0.13781023\n",
      "iteration:  46 loss: 1.15732741\n",
      "iteration:  47 loss: 0.98213506\n",
      "iteration:  48 loss: 0.73568547\n",
      "iteration:  49 loss: 0.49804440\n",
      "iteration:  50 loss: 2.46469617\n",
      "iteration:  51 loss: 0.43011102\n",
      "iteration:  52 loss: 0.79742545\n",
      "iteration:  53 loss: 0.76190621\n",
      "iteration:  54 loss: 0.36155847\n",
      "iteration:  55 loss: 4.05748034\n",
      "iteration:  56 loss: 0.91200447\n",
      "iteration:  57 loss: 2.91910267\n",
      "iteration:  58 loss: 2.52403951\n",
      "iteration:  59 loss: 2.18942094\n",
      "iteration:  60 loss: 0.42092669\n",
      "iteration:  61 loss: 0.17460650\n",
      "iteration:  62 loss: 0.45140028\n",
      "iteration:  63 loss: 0.96613759\n",
      "iteration:  64 loss: 0.30721447\n",
      "iteration:  65 loss: 0.73547518\n",
      "iteration:  66 loss: 0.40384758\n",
      "iteration:  67 loss: 0.23672135\n",
      "iteration:  68 loss: 0.15630117\n",
      "iteration:  69 loss: 0.48681554\n",
      "iteration:  70 loss: 0.40981749\n",
      "iteration:  71 loss: 0.74763155\n",
      "iteration:  72 loss: 0.56801933\n",
      "iteration:  73 loss: 2.90293813\n",
      "iteration:  74 loss: 0.36896923\n",
      "iteration:  75 loss: 0.94648838\n",
      "iteration:  76 loss: 0.70180851\n",
      "iteration:  77 loss: 0.51463473\n",
      "iteration:  78 loss: 1.62838912\n",
      "iteration:  79 loss: 0.59347981\n",
      "iteration:  80 loss: 1.89056766\n",
      "iteration:  81 loss: 0.13014396\n",
      "iteration:  82 loss: 0.33043429\n",
      "iteration:  83 loss: 0.32260931\n",
      "iteration:  84 loss: 3.17678690\n",
      "iteration:  85 loss: 0.16115978\n",
      "iteration:  86 loss: 0.48343179\n",
      "iteration:  87 loss: 3.32820702\n",
      "iteration:  88 loss: 0.20200184\n",
      "iteration:  89 loss: 2.12429094\n",
      "iteration:  90 loss: 1.13528645\n",
      "iteration:  91 loss: 0.38859460\n",
      "iteration:  92 loss: 0.15104108\n",
      "iteration:  93 loss: 1.59231484\n",
      "iteration:  94 loss: 0.93766528\n",
      "iteration:  95 loss: 0.71753776\n",
      "iteration:  96 loss: 3.95108771\n",
      "iteration:  97 loss: 0.47795171\n",
      "iteration:  98 loss: 0.59381896\n",
      "iteration:  99 loss: 0.33390287\n",
      "iteration: 100 loss: 0.25043437\n",
      "iteration: 101 loss: 0.15454942\n",
      "iteration: 102 loss: 1.06505668\n",
      "iteration: 103 loss: 2.76375175\n",
      "iteration: 104 loss: 0.45738074\n",
      "iteration: 105 loss: 0.72747016\n",
      "iteration: 106 loss: 0.31318232\n",
      "iteration: 107 loss: 0.32895088\n",
      "iteration: 108 loss: 1.46142220\n",
      "iteration: 109 loss: 0.49205408\n",
      "iteration: 110 loss: 2.01751900\n",
      "iteration: 111 loss: 3.13002729\n",
      "iteration: 112 loss: 2.80241919\n",
      "iteration: 113 loss: 0.36072463\n",
      "iteration: 114 loss: 0.62998247\n",
      "iteration: 115 loss: 1.62659907\n",
      "iteration: 116 loss: 2.48739982\n",
      "iteration: 117 loss: 0.37625384\n",
      "iteration: 118 loss: 0.28525746\n",
      "iteration: 119 loss: 0.51177901\n",
      "iteration: 120 loss: 0.14511918\n",
      "iteration: 121 loss: 0.66445279\n",
      "iteration: 122 loss: 0.15415378\n",
      "iteration: 123 loss: 2.51698732\n",
      "iteration: 124 loss: 1.16673625\n",
      "iteration: 125 loss: 1.64096510\n",
      "iteration: 126 loss: 3.24662352\n",
      "iteration: 127 loss: 0.29520956\n",
      "iteration: 128 loss: 1.97652102\n",
      "iteration: 129 loss: 0.17743012\n",
      "iteration: 130 loss: 2.76490998\n",
      "iteration: 131 loss: 2.49353004\n",
      "iteration: 132 loss: 1.11587238\n",
      "iteration: 133 loss: 0.97382063\n",
      "iteration: 134 loss: 0.35393694\n",
      "iteration: 135 loss: 2.09477592\n",
      "iteration: 136 loss: 0.99973559\n",
      "iteration: 137 loss: 0.70713371\n",
      "iteration: 138 loss: 1.03774023\n",
      "iteration: 139 loss: 1.14525783\n",
      "iteration: 140 loss: 1.06840050\n",
      "iteration: 141 loss: 1.06872869\n",
      "iteration: 142 loss: 0.95000935\n",
      "iteration: 143 loss: 0.71353406\n",
      "iteration: 144 loss: 3.62153220\n",
      "iteration: 145 loss: 1.04051197\n",
      "iteration: 146 loss: 0.26133928\n",
      "iteration: 147 loss: 1.01750517\n",
      "iteration: 148 loss: 0.92769706\n",
      "iteration: 149 loss: 0.63522261\n",
      "iteration: 150 loss: 1.81690896\n",
      "iteration: 151 loss: 2.06042743\n",
      "iteration: 152 loss: 3.23094034\n",
      "iteration: 153 loss: 3.29061079\n",
      "iteration: 154 loss: 1.41139150\n",
      "iteration: 155 loss: 2.63183832\n",
      "iteration: 156 loss: 0.19542880\n",
      "iteration: 157 loss: 0.78091359\n",
      "iteration: 158 loss: 0.86213571\n",
      "iteration: 159 loss: 0.31826204\n",
      "iteration: 160 loss: 1.09251249\n",
      "iteration: 161 loss: 0.89591026\n",
      "iteration: 162 loss: 0.23020957\n",
      "iteration: 163 loss: 2.78366089\n",
      "iteration: 164 loss: 3.22856236\n",
      "iteration: 165 loss: 1.31467223\n",
      "iteration: 166 loss: 0.15637736\n",
      "iteration: 167 loss: 0.17377989\n",
      "iteration: 168 loss: 1.14973938\n",
      "iteration: 169 loss: 0.26448071\n",
      "iteration: 170 loss: 2.90224266\n",
      "iteration: 171 loss: 0.51975238\n",
      "iteration: 172 loss: 0.74780059\n",
      "iteration: 173 loss: 0.13103153\n",
      "iteration: 174 loss: 3.53943157\n",
      "iteration: 175 loss: 2.04797029\n",
      "iteration: 176 loss: 0.88387936\n",
      "iteration: 177 loss: 0.53372890\n",
      "iteration: 178 loss: 0.25826925\n",
      "iteration: 179 loss: 0.12729745\n",
      "iteration: 180 loss: 0.29559481\n",
      "iteration: 181 loss: 1.39338207\n",
      "iteration: 182 loss: 0.25879005\n",
      "iteration: 183 loss: 1.13830125\n",
      "iteration: 184 loss: 0.59810877\n",
      "iteration: 185 loss: 1.88629019\n",
      "iteration: 186 loss: 1.38073957\n",
      "iteration: 187 loss: 0.65106648\n",
      "iteration: 188 loss: 2.81496525\n",
      "iteration: 189 loss: 2.19747257\n",
      "iteration: 190 loss: 1.70229363\n",
      "iteration: 191 loss: 0.41089830\n",
      "iteration: 192 loss: 0.95830995\n",
      "iteration: 193 loss: 1.53071070\n",
      "iteration: 194 loss: 0.31083009\n",
      "iteration: 195 loss: 2.76476502\n",
      "iteration: 196 loss: 3.35210824\n",
      "iteration: 197 loss: 0.74669623\n",
      "iteration: 198 loss: 0.67944974\n",
      "iteration: 199 loss: 0.64639771\n",
      "epoch:  78 mean loss training: 1.17862189\n",
      "epoch:  78 mean loss validation: 1.40126455\n",
      "iteration:   0 loss: 2.51533222\n",
      "iteration:   1 loss: 0.70308691\n",
      "iteration:   2 loss: 0.87783861\n",
      "iteration:   3 loss: 0.78683245\n",
      "iteration:   4 loss: 1.38637984\n",
      "iteration:   5 loss: 1.24448061\n",
      "iteration:   6 loss: 1.23951900\n",
      "iteration:   7 loss: 0.89723134\n",
      "iteration:   8 loss: 2.78331685\n",
      "iteration:   9 loss: 4.22577810\n",
      "iteration:  10 loss: 0.50915164\n",
      "iteration:  11 loss: 0.39847413\n",
      "iteration:  12 loss: 1.32397115\n",
      "iteration:  13 loss: 1.29848588\n",
      "iteration:  14 loss: 1.06000078\n",
      "iteration:  15 loss: 0.60207230\n",
      "iteration:  16 loss: 0.98921037\n",
      "iteration:  17 loss: 0.89134496\n",
      "iteration:  18 loss: 1.67508805\n",
      "iteration:  19 loss: 1.03555810\n",
      "iteration:  20 loss: 0.54457152\n",
      "iteration:  21 loss: 0.61622900\n",
      "iteration:  22 loss: 0.16192561\n",
      "iteration:  23 loss: 0.80460709\n",
      "iteration:  24 loss: 0.15408498\n",
      "iteration:  25 loss: 3.22491264\n",
      "iteration:  26 loss: 1.35224700\n",
      "iteration:  27 loss: 3.27607608\n",
      "iteration:  28 loss: 0.20250630\n",
      "iteration:  29 loss: 0.45040840\n",
      "iteration:  30 loss: 0.41006756\n",
      "iteration:  31 loss: 0.25879806\n",
      "iteration:  32 loss: 0.15625945\n",
      "iteration:  33 loss: 0.18532650\n",
      "iteration:  34 loss: 1.43543470\n",
      "iteration:  35 loss: 0.99437356\n",
      "iteration:  36 loss: 0.09649798\n",
      "iteration:  37 loss: 0.46737835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  38 loss: 1.12359035\n",
      "iteration:  39 loss: 3.28844285\n",
      "iteration:  40 loss: 1.63965619\n",
      "iteration:  41 loss: 0.59679681\n",
      "iteration:  42 loss: 0.75704414\n",
      "iteration:  43 loss: 2.89339328\n",
      "iteration:  44 loss: 0.24622381\n",
      "iteration:  45 loss: 0.13355635\n",
      "iteration:  46 loss: 1.01940393\n",
      "iteration:  47 loss: 0.61852700\n",
      "iteration:  48 loss: 0.71805656\n",
      "iteration:  49 loss: 0.50230038\n",
      "iteration:  50 loss: 2.10676241\n",
      "iteration:  51 loss: 0.27655444\n",
      "iteration:  52 loss: 0.69991571\n",
      "iteration:  53 loss: 0.35745353\n",
      "iteration:  54 loss: 0.50754803\n",
      "iteration:  55 loss: 4.02943277\n",
      "iteration:  56 loss: 0.76123106\n",
      "iteration:  57 loss: 2.88620996\n",
      "iteration:  58 loss: 2.39760971\n",
      "iteration:  59 loss: 1.75856578\n",
      "iteration:  60 loss: 0.26378605\n",
      "iteration:  61 loss: 0.23515312\n",
      "iteration:  62 loss: 0.62826908\n",
      "iteration:  63 loss: 0.63325059\n",
      "iteration:  64 loss: 0.30111563\n",
      "iteration:  65 loss: 0.63714832\n",
      "iteration:  66 loss: 0.39331472\n",
      "iteration:  67 loss: 0.16208792\n",
      "iteration:  68 loss: 0.17408663\n",
      "iteration:  69 loss: 0.38498417\n",
      "iteration:  70 loss: 0.22661822\n",
      "iteration:  71 loss: 0.65798950\n",
      "iteration:  72 loss: 0.58892155\n",
      "iteration:  73 loss: 2.29297757\n",
      "iteration:  74 loss: 0.50765544\n",
      "iteration:  75 loss: 0.57622617\n",
      "iteration:  76 loss: 0.68511438\n",
      "iteration:  77 loss: 0.42582846\n",
      "iteration:  78 loss: 1.65374529\n",
      "iteration:  79 loss: 0.59165609\n",
      "iteration:  80 loss: 1.64280200\n",
      "iteration:  81 loss: 0.12590373\n",
      "iteration:  82 loss: 0.73473394\n",
      "iteration:  83 loss: 0.42906135\n",
      "iteration:  84 loss: 3.28497791\n",
      "iteration:  85 loss: 0.15568981\n",
      "iteration:  86 loss: 0.48934278\n",
      "iteration:  87 loss: 3.32214046\n",
      "iteration:  88 loss: 0.16834468\n",
      "iteration:  89 loss: 2.58881593\n",
      "iteration:  90 loss: 1.25986087\n",
      "iteration:  91 loss: 0.40365988\n",
      "iteration:  92 loss: 0.09604983\n",
      "iteration:  93 loss: 2.80995417\n",
      "iteration:  94 loss: 2.13280773\n",
      "iteration:  95 loss: 0.68256623\n",
      "iteration:  96 loss: 3.69379950\n",
      "iteration:  97 loss: 0.58741158\n",
      "iteration:  98 loss: 0.57069552\n",
      "iteration:  99 loss: 0.17823502\n",
      "iteration: 100 loss: 0.21176670\n",
      "iteration: 101 loss: 0.14431697\n",
      "iteration: 102 loss: 0.92158175\n",
      "iteration: 103 loss: 2.62230134\n",
      "iteration: 104 loss: 0.69581681\n",
      "iteration: 105 loss: 0.74201107\n",
      "iteration: 106 loss: 0.28527445\n",
      "iteration: 107 loss: 0.28489596\n",
      "iteration: 108 loss: 1.57985067\n",
      "iteration: 109 loss: 0.50350744\n",
      "iteration: 110 loss: 2.25402141\n",
      "iteration: 111 loss: 3.11646032\n",
      "iteration: 112 loss: 2.68028498\n",
      "iteration: 113 loss: 0.32318196\n",
      "iteration: 114 loss: 0.55079901\n",
      "iteration: 115 loss: 1.33186018\n",
      "iteration: 116 loss: 2.20207667\n",
      "iteration: 117 loss: 0.44677034\n",
      "iteration: 118 loss: 0.31922120\n",
      "iteration: 119 loss: 0.47529086\n",
      "iteration: 120 loss: 0.14262311\n",
      "iteration: 121 loss: 0.68340665\n",
      "iteration: 122 loss: 0.17779279\n",
      "iteration: 123 loss: 2.34178686\n",
      "iteration: 124 loss: 1.44429231\n",
      "iteration: 125 loss: 2.24788666\n",
      "iteration: 126 loss: 3.31098175\n",
      "iteration: 127 loss: 0.31425568\n",
      "iteration: 128 loss: 2.59502482\n",
      "iteration: 129 loss: 0.17766592\n",
      "iteration: 130 loss: 2.64850545\n",
      "iteration: 131 loss: 2.30858874\n",
      "iteration: 132 loss: 1.49640131\n",
      "iteration: 133 loss: 0.42920876\n",
      "iteration: 134 loss: 0.31924641\n",
      "iteration: 135 loss: 2.08820271\n",
      "iteration: 136 loss: 0.98088372\n",
      "iteration: 137 loss: 0.70360374\n",
      "iteration: 138 loss: 0.90725380\n",
      "iteration: 139 loss: 1.08933485\n",
      "iteration: 140 loss: 1.00072777\n",
      "iteration: 141 loss: 0.77760410\n",
      "iteration: 142 loss: 0.76819849\n",
      "iteration: 143 loss: 1.79232073\n",
      "iteration: 144 loss: 3.49177265\n",
      "iteration: 145 loss: 1.02955389\n",
      "iteration: 146 loss: 0.26096609\n",
      "iteration: 147 loss: 0.91008890\n",
      "iteration: 148 loss: 0.73492527\n",
      "iteration: 149 loss: 0.99990875\n",
      "iteration: 150 loss: 1.88169432\n",
      "iteration: 151 loss: 2.04703426\n",
      "iteration: 152 loss: 3.23088169\n",
      "iteration: 153 loss: 3.39479113\n",
      "iteration: 154 loss: 1.14439738\n",
      "iteration: 155 loss: 2.55647063\n",
      "iteration: 156 loss: 0.18016836\n",
      "iteration: 157 loss: 0.77622038\n",
      "iteration: 158 loss: 0.77376425\n",
      "iteration: 159 loss: 0.27102396\n",
      "iteration: 160 loss: 1.16467714\n",
      "iteration: 161 loss: 0.95687091\n",
      "iteration: 162 loss: 0.26654449\n",
      "iteration: 163 loss: 1.96201289\n",
      "iteration: 164 loss: 3.11986136\n",
      "iteration: 165 loss: 2.57039738\n",
      "iteration: 166 loss: 0.16167286\n",
      "iteration: 167 loss: 0.16693383\n",
      "iteration: 168 loss: 0.87264472\n",
      "iteration: 169 loss: 0.22683996\n",
      "iteration: 170 loss: 2.92747068\n",
      "iteration: 171 loss: 0.60215968\n",
      "iteration: 172 loss: 0.50316679\n",
      "iteration: 173 loss: 0.11539482\n",
      "iteration: 174 loss: 3.55505824\n",
      "iteration: 175 loss: 2.91002226\n",
      "iteration: 176 loss: 1.82120717\n",
      "iteration: 177 loss: 0.41740900\n",
      "iteration: 178 loss: 0.24379982\n",
      "iteration: 179 loss: 0.11891519\n",
      "iteration: 180 loss: 0.31652287\n",
      "iteration: 181 loss: 1.17714667\n",
      "iteration: 182 loss: 0.17091864\n",
      "iteration: 183 loss: 1.33359599\n",
      "iteration: 184 loss: 0.44847089\n",
      "iteration: 185 loss: 1.75866425\n",
      "iteration: 186 loss: 2.05920362\n",
      "iteration: 187 loss: 0.59512901\n",
      "iteration: 188 loss: 2.88373160\n",
      "iteration: 189 loss: 2.13673639\n",
      "iteration: 190 loss: 1.77881455\n",
      "iteration: 191 loss: 0.42650726\n",
      "iteration: 192 loss: 0.99409777\n",
      "iteration: 193 loss: 1.34509432\n",
      "iteration: 194 loss: 0.35876337\n",
      "iteration: 195 loss: 2.70296168\n",
      "iteration: 196 loss: 3.32792306\n",
      "iteration: 197 loss: 0.67794436\n",
      "iteration: 198 loss: 0.73839086\n",
      "iteration: 199 loss: 0.62062311\n",
      "epoch:  79 mean loss training: 1.17292476\n",
      "epoch:  79 mean loss validation: 1.34175551\n",
      "iteration:   0 loss: 2.67252064\n",
      "iteration:   1 loss: 0.66194642\n",
      "iteration:   2 loss: 0.77154249\n",
      "iteration:   3 loss: 0.88935679\n",
      "iteration:   4 loss: 2.07529664\n",
      "iteration:   5 loss: 1.77307630\n",
      "iteration:   6 loss: 1.18481696\n",
      "iteration:   7 loss: 0.76272869\n",
      "iteration:   8 loss: 3.03916574\n",
      "iteration:   9 loss: 4.31668329\n",
      "iteration:  10 loss: 0.62716007\n",
      "iteration:  11 loss: 0.43546420\n",
      "iteration:  12 loss: 1.14794493\n",
      "iteration:  13 loss: 1.23595595\n",
      "iteration:  14 loss: 1.04425704\n",
      "iteration:  15 loss: 0.62110406\n",
      "iteration:  16 loss: 1.01406956\n",
      "iteration:  17 loss: 0.69855380\n",
      "iteration:  18 loss: 1.62238944\n",
      "iteration:  19 loss: 0.98370522\n",
      "iteration:  20 loss: 0.83483803\n",
      "iteration:  21 loss: 0.56050515\n",
      "iteration:  22 loss: 0.26889578\n",
      "iteration:  23 loss: 0.98346567\n",
      "iteration:  24 loss: 0.22209300\n",
      "iteration:  25 loss: 1.76945210\n",
      "iteration:  26 loss: 1.25322938\n",
      "iteration:  27 loss: 2.76212811\n",
      "iteration:  28 loss: 0.25839850\n",
      "iteration:  29 loss: 0.34258723\n",
      "iteration:  30 loss: 0.37343913\n",
      "iteration:  31 loss: 0.22213852\n",
      "iteration:  32 loss: 0.37405074\n",
      "iteration:  33 loss: 0.20406343\n",
      "iteration:  34 loss: 1.39209104\n",
      "iteration:  35 loss: 1.14612257\n",
      "iteration:  36 loss: 0.11179551\n",
      "iteration:  37 loss: 0.42558792\n",
      "iteration:  38 loss: 1.13346529\n",
      "iteration:  39 loss: 3.30061865\n",
      "iteration:  40 loss: 1.73811030\n",
      "iteration:  41 loss: 0.55045414\n",
      "iteration:  42 loss: 0.77133852\n",
      "iteration:  43 loss: 2.89212084\n",
      "iteration:  44 loss: 0.65896237\n",
      "iteration:  45 loss: 0.13362639\n",
      "iteration:  46 loss: 1.38042116\n",
      "iteration:  47 loss: 0.79300076\n",
      "iteration:  48 loss: 0.73404646\n",
      "iteration:  49 loss: 0.50338912\n",
      "iteration:  50 loss: 2.53750420\n",
      "iteration:  51 loss: 0.33969092\n",
      "iteration:  52 loss: 0.82781368\n",
      "iteration:  53 loss: 0.75510263\n",
      "iteration:  54 loss: 0.42721012\n",
      "iteration:  55 loss: 3.69765973\n",
      "iteration:  56 loss: 0.99256504\n",
      "iteration:  57 loss: 2.93317294\n",
      "iteration:  58 loss: 2.46768022\n",
      "iteration:  59 loss: 1.90300572\n",
      "iteration:  60 loss: 0.39487058\n",
      "iteration:  61 loss: 0.22937071\n",
      "iteration:  62 loss: 0.56805414\n",
      "iteration:  63 loss: 0.88255286\n",
      "iteration:  64 loss: 0.27932766\n",
      "iteration:  65 loss: 0.66879672\n",
      "iteration:  66 loss: 0.38730836\n",
      "iteration:  67 loss: 0.14134105\n",
      "iteration:  68 loss: 0.15023097\n",
      "iteration:  69 loss: 0.34006441\n",
      "iteration:  70 loss: 0.37551647\n",
      "iteration:  71 loss: 0.77713698\n",
      "iteration:  72 loss: 0.50341493\n",
      "iteration:  73 loss: 2.85519600\n",
      "iteration:  74 loss: 0.50953549\n",
      "iteration:  75 loss: 0.45782277\n",
      "iteration:  76 loss: 0.65996605\n",
      "iteration:  77 loss: 0.45050362\n",
      "iteration:  78 loss: 1.63449788\n",
      "iteration:  79 loss: 0.61238515\n",
      "iteration:  80 loss: 1.77051938\n",
      "iteration:  81 loss: 0.18593551\n",
      "iteration:  82 loss: 0.77963853\n",
      "iteration:  83 loss: 0.29097754\n",
      "iteration:  84 loss: 3.17578173\n",
      "iteration:  85 loss: 0.15752272\n",
      "iteration:  86 loss: 0.55871719\n",
      "iteration:  87 loss: 3.32344103\n",
      "iteration:  88 loss: 0.19937475\n",
      "iteration:  89 loss: 2.49746013\n",
      "iteration:  90 loss: 1.08934963\n",
      "iteration:  91 loss: 0.45792291\n",
      "iteration:  92 loss: 0.18124631\n",
      "iteration:  93 loss: 2.46989918\n",
      "iteration:  94 loss: 0.94513321\n",
      "iteration:  95 loss: 0.69045436\n",
      "iteration:  96 loss: 3.60783100\n",
      "iteration:  97 loss: 0.49003923\n",
      "iteration:  98 loss: 0.59726006\n",
      "iteration:  99 loss: 0.24343073\n",
      "iteration: 100 loss: 0.34308299\n",
      "iteration: 101 loss: 0.16407876\n",
      "iteration: 102 loss: 1.00608313\n",
      "iteration: 103 loss: 2.64798117\n",
      "iteration: 104 loss: 0.57494134\n",
      "iteration: 105 loss: 0.88325554\n",
      "iteration: 106 loss: 0.57847732\n",
      "iteration: 107 loss: 0.31864259\n",
      "iteration: 108 loss: 1.58928549\n",
      "iteration: 109 loss: 0.51101094\n",
      "iteration: 110 loss: 2.28882170\n",
      "iteration: 111 loss: 3.12130713\n",
      "iteration: 112 loss: 2.72383714\n",
      "iteration: 113 loss: 0.42915007\n",
      "iteration: 114 loss: 0.68600786\n",
      "iteration: 115 loss: 1.63607311\n",
      "iteration: 116 loss: 2.26025224\n",
      "iteration: 117 loss: 0.43241453\n",
      "iteration: 118 loss: 0.34547976\n",
      "iteration: 119 loss: 0.50168002\n",
      "iteration: 120 loss: 0.40373141\n",
      "iteration: 121 loss: 0.68644476\n",
      "iteration: 122 loss: 0.15973328\n",
      "iteration: 123 loss: 2.49297643\n",
      "iteration: 124 loss: 1.19686401\n",
      "iteration: 125 loss: 1.66759646\n",
      "iteration: 126 loss: 3.00387073\n",
      "iteration: 127 loss: 0.30393979\n",
      "iteration: 128 loss: 2.14331079\n",
      "iteration: 129 loss: 0.17426258\n",
      "iteration: 130 loss: 2.72778606\n",
      "iteration: 131 loss: 2.09217787\n",
      "iteration: 132 loss: 1.11072862\n",
      "iteration: 133 loss: 0.41310135\n",
      "iteration: 134 loss: 0.39989871\n",
      "iteration: 135 loss: 2.10255361\n",
      "iteration: 136 loss: 1.05201423\n",
      "iteration: 137 loss: 1.02916145\n",
      "iteration: 138 loss: 0.92987877\n",
      "iteration: 139 loss: 1.07031286\n",
      "iteration: 140 loss: 0.96439862\n",
      "iteration: 141 loss: 0.83326626\n",
      "iteration: 142 loss: 0.85970294\n",
      "iteration: 143 loss: 1.94686687\n",
      "iteration: 144 loss: 3.50604367\n",
      "iteration: 145 loss: 1.09350836\n",
      "iteration: 146 loss: 0.35762537\n",
      "iteration: 147 loss: 0.89706326\n",
      "iteration: 148 loss: 0.84459311\n",
      "iteration: 149 loss: 0.87971079\n",
      "iteration: 150 loss: 1.80418110\n",
      "iteration: 151 loss: 2.19051576\n",
      "iteration: 152 loss: 3.25226545\n",
      "iteration: 153 loss: 3.29228759\n",
      "iteration: 154 loss: 1.57145417\n",
      "iteration: 155 loss: 2.64427471\n",
      "iteration: 156 loss: 0.26577687\n",
      "iteration: 157 loss: 0.86510646\n",
      "iteration: 158 loss: 0.80504245\n",
      "iteration: 159 loss: 0.32653278\n",
      "iteration: 160 loss: 1.25024676\n",
      "iteration: 161 loss: 0.88251621\n",
      "iteration: 162 loss: 0.26560941\n",
      "iteration: 163 loss: 2.02079058\n",
      "iteration: 164 loss: 3.13738489\n",
      "iteration: 165 loss: 2.59404087\n",
      "iteration: 166 loss: 0.17779841\n",
      "iteration: 167 loss: 0.17163406\n",
      "iteration: 168 loss: 0.81076765\n",
      "iteration: 169 loss: 0.23963642\n",
      "iteration: 170 loss: 2.95078254\n",
      "iteration: 171 loss: 0.52378303\n",
      "iteration: 172 loss: 0.64905059\n",
      "iteration: 173 loss: 0.11861485\n",
      "iteration: 174 loss: 3.55461097\n",
      "iteration: 175 loss: 2.10193610\n",
      "iteration: 176 loss: 2.10258245\n",
      "iteration: 177 loss: 0.54876238\n",
      "iteration: 178 loss: 0.23783705\n",
      "iteration: 179 loss: 0.12083077\n",
      "iteration: 180 loss: 0.32907698\n",
      "iteration: 181 loss: 1.34705782\n",
      "iteration: 182 loss: 0.13372003\n",
      "iteration: 183 loss: 1.44796622\n",
      "iteration: 184 loss: 0.43510804\n",
      "iteration: 185 loss: 1.98988163\n",
      "iteration: 186 loss: 2.10233331\n",
      "iteration: 187 loss: 0.61359316\n",
      "iteration: 188 loss: 2.79554582\n",
      "iteration: 189 loss: 2.20915508\n",
      "iteration: 190 loss: 1.84018743\n",
      "iteration: 191 loss: 0.46100575\n",
      "iteration: 192 loss: 1.07487631\n",
      "iteration: 193 loss: 1.06197560\n",
      "iteration: 194 loss: 0.34413290\n",
      "iteration: 195 loss: 2.74058914\n",
      "iteration: 196 loss: 3.56532383\n",
      "iteration: 197 loss: 0.65957123\n",
      "iteration: 198 loss: 0.81357241\n",
      "iteration: 199 loss: 0.65615189\n",
      "epoch:  80 mean loss training: 1.18812931\n",
      "epoch:  80 mean loss validation: 1.38896537\n",
      "iteration:   0 loss: 2.47795415\n",
      "iteration:   1 loss: 0.64840925\n",
      "iteration:   2 loss: 0.80636132\n",
      "iteration:   3 loss: 0.76212031\n",
      "iteration:   4 loss: 1.35111201\n",
      "iteration:   5 loss: 1.65699553\n",
      "iteration:   6 loss: 1.28477669\n",
      "iteration:   7 loss: 0.95593596\n",
      "iteration:   8 loss: 2.84960938\n",
      "iteration:   9 loss: 4.11883450\n",
      "iteration:  10 loss: 0.46828219\n",
      "iteration:  11 loss: 0.38575664\n",
      "iteration:  12 loss: 1.14503837\n",
      "iteration:  13 loss: 1.11987817\n",
      "iteration:  14 loss: 1.01327944\n",
      "iteration:  15 loss: 0.51754755\n",
      "iteration:  16 loss: 0.78856015\n",
      "iteration:  17 loss: 0.64037502\n",
      "iteration:  18 loss: 1.43982029\n",
      "iteration:  19 loss: 1.36141336\n",
      "iteration:  20 loss: 0.76220262\n",
      "iteration:  21 loss: 0.84539121\n",
      "iteration:  22 loss: 0.18481568\n",
      "iteration:  23 loss: 0.90751833\n",
      "iteration:  24 loss: 0.25715491\n",
      "iteration:  25 loss: 2.71830821\n",
      "iteration:  26 loss: 1.10839427\n",
      "iteration:  27 loss: 3.15781283\n",
      "iteration:  28 loss: 0.28836253\n",
      "iteration:  29 loss: 0.45568231\n",
      "iteration:  30 loss: 0.57196736\n",
      "iteration:  31 loss: 0.44789723\n",
      "iteration:  32 loss: 0.21584681\n",
      "iteration:  33 loss: 0.17704242\n",
      "iteration:  34 loss: 1.13798463\n",
      "iteration:  35 loss: 0.97924274\n",
      "iteration:  36 loss: 0.10562236\n",
      "iteration:  37 loss: 0.47145629\n",
      "iteration:  38 loss: 1.12604725\n",
      "iteration:  39 loss: 3.48004341\n",
      "iteration:  40 loss: 1.34739721\n",
      "iteration:  41 loss: 0.53326511\n",
      "iteration:  42 loss: 0.85761654\n",
      "iteration:  43 loss: 2.88163638\n",
      "iteration:  44 loss: 0.33604974\n",
      "iteration:  45 loss: 0.12583706\n",
      "iteration:  46 loss: 1.04120123\n",
      "iteration:  47 loss: 0.71335399\n",
      "iteration:  48 loss: 0.74572170\n",
      "iteration:  49 loss: 0.50481856\n",
      "iteration:  50 loss: 2.19118452\n",
      "iteration:  51 loss: 0.30199486\n",
      "iteration:  52 loss: 0.76994431\n",
      "iteration:  53 loss: 0.43149054\n",
      "iteration:  54 loss: 0.43281659\n",
      "iteration:  55 loss: 4.60797310\n",
      "iteration:  56 loss: 0.81460410\n",
      "iteration:  57 loss: 2.89222598\n",
      "iteration:  58 loss: 2.65916085\n",
      "iteration:  59 loss: 1.34955800\n",
      "iteration:  60 loss: 0.37394109\n",
      "iteration:  61 loss: 0.21128446\n",
      "iteration:  62 loss: 1.22400224\n",
      "iteration:  63 loss: 0.80189335\n",
      "iteration:  64 loss: 0.11692407\n",
      "iteration:  65 loss: 0.66095889\n",
      "iteration:  66 loss: 0.13234563\n",
      "iteration:  67 loss: 0.13012950\n",
      "iteration:  68 loss: 0.12714662\n",
      "iteration:  69 loss: 0.33508658\n",
      "iteration:  70 loss: 0.44058073\n",
      "iteration:  71 loss: 0.47130996\n",
      "iteration:  72 loss: 0.57888550\n",
      "iteration:  73 loss: 2.40981555\n",
      "iteration:  74 loss: 0.29710683\n",
      "iteration:  75 loss: 0.49167979\n",
      "iteration:  76 loss: 0.61294568\n",
      "iteration:  77 loss: 0.51052254\n",
      "iteration:  78 loss: 1.49860573\n",
      "iteration:  79 loss: 0.60552603\n",
      "iteration:  80 loss: 1.33733559\n",
      "iteration:  81 loss: 0.14204924\n",
      "iteration:  82 loss: 0.73685008\n",
      "iteration:  83 loss: 0.24093591\n",
      "iteration:  84 loss: 3.01352549\n",
      "iteration:  85 loss: 0.15370992\n",
      "iteration:  86 loss: 0.48782897\n",
      "iteration:  87 loss: 3.03772020\n",
      "iteration:  88 loss: 0.19321615\n",
      "iteration:  89 loss: 2.55685878\n",
      "iteration:  90 loss: 2.63065767\n",
      "iteration:  91 loss: 2.01590824\n",
      "iteration:  92 loss: 0.15998879\n",
      "iteration:  93 loss: 2.54184580\n",
      "iteration:  94 loss: 1.13524449\n",
      "iteration:  95 loss: 0.66239715\n",
      "iteration:  96 loss: 3.24443173\n",
      "iteration:  97 loss: 0.92623407\n",
      "iteration:  98 loss: 0.39286116\n",
      "iteration:  99 loss: 0.19631031\n",
      "iteration: 100 loss: 0.28286040\n",
      "iteration: 101 loss: 0.20721658\n",
      "iteration: 102 loss: 0.66737205\n",
      "iteration: 103 loss: 2.81192613\n",
      "iteration: 104 loss: 0.34570649\n",
      "iteration: 105 loss: 0.49745587\n",
      "iteration: 106 loss: 0.25185677\n",
      "iteration: 107 loss: 0.24523681\n",
      "iteration: 108 loss: 0.79281980\n",
      "iteration: 109 loss: 0.13699484\n",
      "iteration: 110 loss: 2.58083606\n",
      "iteration: 111 loss: 3.11087084\n",
      "iteration: 112 loss: 2.69320965\n",
      "iteration: 113 loss: 0.32378814\n",
      "iteration: 114 loss: 0.56516570\n",
      "iteration: 115 loss: 1.07740116\n",
      "iteration: 116 loss: 2.22292829\n",
      "iteration: 117 loss: 0.46279255\n",
      "iteration: 118 loss: 0.54499716\n",
      "iteration: 119 loss: 0.47332877\n",
      "iteration: 120 loss: 0.31924015\n",
      "iteration: 121 loss: 0.58722103\n",
      "iteration: 122 loss: 0.16086334\n",
      "iteration: 123 loss: 2.34553742\n",
      "iteration: 124 loss: 1.11451650\n",
      "iteration: 125 loss: 1.58068275\n",
      "iteration: 126 loss: 3.18423009\n",
      "iteration: 127 loss: 0.27232894\n",
      "iteration: 128 loss: 2.19985843\n",
      "iteration: 129 loss: 0.19159023\n",
      "iteration: 130 loss: 2.72760224\n",
      "iteration: 131 loss: 2.24021459\n",
      "iteration: 132 loss: 0.99806279\n",
      "iteration: 133 loss: 0.50708246\n",
      "iteration: 134 loss: 0.38456959\n",
      "iteration: 135 loss: 1.21639729\n",
      "iteration: 136 loss: 1.02559137\n",
      "iteration: 137 loss: 0.89290565\n",
      "iteration: 138 loss: 1.08506083\n",
      "iteration: 139 loss: 1.40731537\n",
      "iteration: 140 loss: 0.84798455\n",
      "iteration: 141 loss: 0.86669850\n",
      "iteration: 142 loss: 0.94872516\n",
      "iteration: 143 loss: 1.73987889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 144 loss: 3.69529676\n",
      "iteration: 145 loss: 1.59447300\n",
      "iteration: 146 loss: 0.39900970\n",
      "iteration: 147 loss: 1.12750208\n",
      "iteration: 148 loss: 1.00996935\n",
      "iteration: 149 loss: 1.10791719\n",
      "iteration: 150 loss: 2.34502959\n",
      "iteration: 151 loss: 1.36722767\n",
      "iteration: 152 loss: 3.24157953\n",
      "iteration: 153 loss: 3.14664102\n",
      "iteration: 154 loss: 1.91413796\n",
      "iteration: 155 loss: 2.89951158\n",
      "iteration: 156 loss: 0.58591777\n",
      "iteration: 157 loss: 0.86658645\n",
      "iteration: 158 loss: 0.93361634\n",
      "iteration: 159 loss: 0.37497953\n",
      "iteration: 160 loss: 1.06422186\n",
      "iteration: 161 loss: 0.83632004\n",
      "iteration: 162 loss: 0.26574096\n",
      "iteration: 163 loss: 2.52037144\n",
      "iteration: 164 loss: 3.02758646\n",
      "iteration: 165 loss: 2.50086594\n",
      "iteration: 166 loss: 0.17034401\n",
      "iteration: 167 loss: 0.23831992\n",
      "iteration: 168 loss: 0.96913981\n",
      "iteration: 169 loss: 0.28484830\n",
      "iteration: 170 loss: 2.76824999\n",
      "iteration: 171 loss: 0.50572866\n",
      "iteration: 172 loss: 0.75168234\n",
      "iteration: 173 loss: 0.12000173\n",
      "iteration: 174 loss: 3.47780561\n",
      "iteration: 175 loss: 1.85234559\n",
      "iteration: 176 loss: 1.57643878\n",
      "iteration: 177 loss: 0.34992900\n",
      "iteration: 178 loss: 0.29799151\n",
      "iteration: 179 loss: 0.13830364\n",
      "iteration: 180 loss: 0.21227407\n",
      "iteration: 181 loss: 1.50062370\n",
      "iteration: 182 loss: 0.21373674\n",
      "iteration: 183 loss: 1.12310922\n",
      "iteration: 184 loss: 0.44190350\n",
      "iteration: 185 loss: 2.41558099\n",
      "iteration: 186 loss: 1.63008142\n",
      "iteration: 187 loss: 0.94708312\n",
      "iteration: 188 loss: 3.21344543\n",
      "iteration: 189 loss: 2.20301104\n",
      "iteration: 190 loss: 2.04933763\n",
      "iteration: 191 loss: 0.34145486\n",
      "iteration: 192 loss: 0.68191630\n",
      "iteration: 193 loss: 1.30994689\n",
      "iteration: 194 loss: 0.40889001\n",
      "iteration: 195 loss: 2.72451758\n",
      "iteration: 196 loss: 3.40915418\n",
      "iteration: 197 loss: 0.66504866\n",
      "iteration: 198 loss: 0.72325367\n",
      "iteration: 199 loss: 0.60577977\n",
      "epoch:  81 mean loss training: 1.17425001\n",
      "epoch:  81 mean loss validation: 1.38603592\n",
      "iteration:   0 loss: 2.70569348\n",
      "iteration:   1 loss: 0.66028833\n",
      "iteration:   2 loss: 0.83423108\n",
      "iteration:   3 loss: 0.72431159\n",
      "iteration:   4 loss: 2.15800238\n",
      "iteration:   5 loss: 1.39398468\n",
      "iteration:   6 loss: 1.29113197\n",
      "iteration:   7 loss: 1.10750473\n",
      "iteration:   8 loss: 2.62875175\n",
      "iteration:   9 loss: 3.58111382\n",
      "iteration:  10 loss: 0.57401383\n",
      "iteration:  11 loss: 1.04533052\n",
      "iteration:  12 loss: 1.10622203\n",
      "iteration:  13 loss: 1.40412545\n",
      "iteration:  14 loss: 1.04808426\n",
      "iteration:  15 loss: 0.36501673\n",
      "iteration:  16 loss: 1.07853723\n",
      "iteration:  17 loss: 0.88601655\n",
      "iteration:  18 loss: 1.33903325\n",
      "iteration:  19 loss: 1.21952069\n",
      "iteration:  20 loss: 0.49738920\n",
      "iteration:  21 loss: 1.61324024\n",
      "iteration:  22 loss: 0.29653326\n",
      "iteration:  23 loss: 0.97534961\n",
      "iteration:  24 loss: 0.38389695\n",
      "iteration:  25 loss: 1.74205363\n",
      "iteration:  26 loss: 0.97656494\n",
      "iteration:  27 loss: 2.68588591\n",
      "iteration:  28 loss: 0.41382176\n",
      "iteration:  29 loss: 0.54739928\n",
      "iteration:  30 loss: 0.63667214\n",
      "iteration:  31 loss: 0.43943116\n",
      "iteration:  32 loss: 0.38309985\n",
      "iteration:  33 loss: 0.17329827\n",
      "iteration:  34 loss: 1.26753843\n",
      "iteration:  35 loss: 1.06821525\n",
      "iteration:  36 loss: 0.10822792\n",
      "iteration:  37 loss: 0.44436488\n",
      "iteration:  38 loss: 1.13465273\n",
      "iteration:  39 loss: 3.44283271\n",
      "iteration:  40 loss: 1.60404277\n",
      "iteration:  41 loss: 0.59828347\n",
      "iteration:  42 loss: 0.97217721\n",
      "iteration:  43 loss: 2.87315035\n",
      "iteration:  44 loss: 0.29910663\n",
      "iteration:  45 loss: 0.41815260\n",
      "iteration:  46 loss: 1.04648948\n",
      "iteration:  47 loss: 0.86213887\n",
      "iteration:  48 loss: 0.76884007\n",
      "iteration:  49 loss: 0.50392252\n",
      "iteration:  50 loss: 2.22696114\n",
      "iteration:  51 loss: 0.31200567\n",
      "iteration:  52 loss: 0.70951015\n",
      "iteration:  53 loss: 0.59698439\n",
      "iteration:  54 loss: 0.35983533\n",
      "iteration:  55 loss: 4.54263353\n",
      "iteration:  56 loss: 0.83598942\n",
      "iteration:  57 loss: 2.88965082\n",
      "iteration:  58 loss: 2.66981912\n",
      "iteration:  59 loss: 1.46772826\n",
      "iteration:  60 loss: 0.39350867\n",
      "iteration:  61 loss: 0.19591002\n",
      "iteration:  62 loss: 1.29182863\n",
      "iteration:  63 loss: 0.93446970\n",
      "iteration:  64 loss: 0.13267347\n",
      "iteration:  65 loss: 0.61927891\n",
      "iteration:  66 loss: 0.14463086\n",
      "iteration:  67 loss: 0.21005009\n",
      "iteration:  68 loss: 0.12439749\n",
      "iteration:  69 loss: 0.30559638\n",
      "iteration:  70 loss: 0.38589525\n",
      "iteration:  71 loss: 0.42329776\n",
      "iteration:  72 loss: 0.61628693\n",
      "iteration:  73 loss: 2.55787873\n",
      "iteration:  74 loss: 0.45784548\n",
      "iteration:  75 loss: 0.78552747\n",
      "iteration:  76 loss: 0.64173615\n",
      "iteration:  77 loss: 0.51199120\n",
      "iteration:  78 loss: 2.54027987\n",
      "iteration:  79 loss: 0.59354585\n",
      "iteration:  80 loss: 1.48409510\n",
      "iteration:  81 loss: 0.13551901\n",
      "iteration:  82 loss: 0.72988248\n",
      "iteration:  83 loss: 0.50427985\n",
      "iteration:  84 loss: 2.83948445\n",
      "iteration:  85 loss: 0.18477602\n",
      "iteration:  86 loss: 0.47840098\n",
      "iteration:  87 loss: 2.84090948\n",
      "iteration:  88 loss: 0.21503569\n",
      "iteration:  89 loss: 2.19772077\n",
      "iteration:  90 loss: 1.65699363\n",
      "iteration:  91 loss: 0.39719787\n",
      "iteration:  92 loss: 0.28138545\n",
      "iteration:  93 loss: 2.99459624\n",
      "iteration:  94 loss: 1.66443169\n",
      "iteration:  95 loss: 0.71908116\n",
      "iteration:  96 loss: 3.44850945\n",
      "iteration:  97 loss: 0.61075491\n",
      "iteration:  98 loss: 0.52659619\n",
      "iteration:  99 loss: 0.45102611\n",
      "iteration: 100 loss: 0.53716499\n",
      "iteration: 101 loss: 0.55292791\n",
      "iteration: 102 loss: 1.17662776\n",
      "iteration: 103 loss: 2.57150984\n",
      "iteration: 104 loss: 0.51279134\n",
      "iteration: 105 loss: 0.67212975\n",
      "iteration: 106 loss: 0.26128247\n",
      "iteration: 107 loss: 0.95383495\n",
      "iteration: 108 loss: 2.78493929\n",
      "iteration: 109 loss: 0.86788487\n",
      "iteration: 110 loss: 2.17928672\n",
      "iteration: 111 loss: 3.13629961\n",
      "iteration: 112 loss: 2.55739665\n",
      "iteration: 113 loss: 1.23909438\n",
      "iteration: 114 loss: 0.66743195\n",
      "iteration: 115 loss: 1.21737432\n",
      "iteration: 116 loss: 2.17843413\n",
      "iteration: 117 loss: 0.94504333\n",
      "iteration: 118 loss: 0.45415613\n",
      "iteration: 119 loss: 0.75618893\n",
      "iteration: 120 loss: 0.18936543\n",
      "iteration: 121 loss: 0.72392637\n",
      "iteration: 122 loss: 0.13832004\n",
      "iteration: 123 loss: 2.39485145\n",
      "iteration: 124 loss: 1.20831823\n",
      "iteration: 125 loss: 2.01125264\n",
      "iteration: 126 loss: 3.40688896\n",
      "iteration: 127 loss: 0.29651278\n",
      "iteration: 128 loss: 2.34852672\n",
      "iteration: 129 loss: 0.71195483\n",
      "iteration: 130 loss: 2.74799991\n",
      "iteration: 131 loss: 2.57588744\n",
      "iteration: 132 loss: 1.04404104\n",
      "iteration: 133 loss: 0.37537071\n",
      "iteration: 134 loss: 0.53556913\n",
      "iteration: 135 loss: 0.81947243\n",
      "iteration: 136 loss: 1.18752289\n",
      "iteration: 137 loss: 0.79207611\n",
      "iteration: 138 loss: 0.96614146\n",
      "iteration: 139 loss: 1.57031751\n",
      "iteration: 140 loss: 0.95500320\n",
      "iteration: 141 loss: 0.85896909\n",
      "iteration: 142 loss: 0.93698597\n",
      "iteration: 143 loss: 1.81387115\n",
      "iteration: 144 loss: 3.41600227\n",
      "iteration: 145 loss: 0.90820831\n",
      "iteration: 146 loss: 0.26233250\n",
      "iteration: 147 loss: 0.99702162\n",
      "iteration: 148 loss: 0.92053324\n",
      "iteration: 149 loss: 0.93487000\n",
      "iteration: 150 loss: 1.87622273\n",
      "iteration: 151 loss: 1.25596142\n",
      "iteration: 152 loss: 3.19237685\n",
      "iteration: 153 loss: 3.18612266\n",
      "iteration: 154 loss: 1.36569190\n",
      "iteration: 155 loss: 2.56653523\n",
      "iteration: 156 loss: 0.27544740\n",
      "iteration: 157 loss: 0.83949828\n",
      "iteration: 158 loss: 0.97588527\n",
      "iteration: 159 loss: 0.32081729\n",
      "iteration: 160 loss: 0.94902718\n",
      "iteration: 161 loss: 1.07370198\n",
      "iteration: 162 loss: 0.24815851\n",
      "iteration: 163 loss: 1.93163896\n",
      "iteration: 164 loss: 3.01972604\n",
      "iteration: 165 loss: 2.48682308\n",
      "iteration: 166 loss: 0.15512286\n",
      "iteration: 167 loss: 0.26026681\n",
      "iteration: 168 loss: 0.92478746\n",
      "iteration: 169 loss: 0.38650560\n",
      "iteration: 170 loss: 2.76034975\n",
      "iteration: 171 loss: 0.48316166\n",
      "iteration: 172 loss: 0.63418525\n",
      "iteration: 173 loss: 0.15170099\n",
      "iteration: 174 loss: 3.44169021\n",
      "iteration: 175 loss: 1.97718787\n",
      "iteration: 176 loss: 1.61424410\n",
      "iteration: 177 loss: 0.38868240\n",
      "iteration: 178 loss: 0.23747581\n",
      "iteration: 179 loss: 0.12371476\n",
      "iteration: 180 loss: 0.21226902\n",
      "iteration: 181 loss: 1.44152462\n",
      "iteration: 182 loss: 0.16332895\n",
      "iteration: 183 loss: 1.12473679\n",
      "iteration: 184 loss: 0.38057682\n",
      "iteration: 185 loss: 2.25473881\n",
      "iteration: 186 loss: 1.38747799\n",
      "iteration: 187 loss: 0.68306708\n",
      "iteration: 188 loss: 3.27552438\n",
      "iteration: 189 loss: 2.22012281\n",
      "iteration: 190 loss: 2.03770351\n",
      "iteration: 191 loss: 0.33165047\n",
      "iteration: 192 loss: 0.71942699\n",
      "iteration: 193 loss: 1.24256909\n",
      "iteration: 194 loss: 0.25531992\n",
      "iteration: 195 loss: 2.62903261\n",
      "iteration: 196 loss: 3.37493825\n",
      "iteration: 197 loss: 0.67250687\n",
      "iteration: 198 loss: 0.70377100\n",
      "iteration: 199 loss: 0.58377409\n",
      "epoch:  82 mean loss training: 1.20116389\n",
      "epoch:  82 mean loss validation: 1.35362828\n",
      "iteration:   0 loss: 2.35881186\n",
      "iteration:   1 loss: 0.61366332\n",
      "iteration:   2 loss: 0.77079219\n",
      "iteration:   3 loss: 0.68227375\n",
      "iteration:   4 loss: 1.48293126\n",
      "iteration:   5 loss: 1.74902844\n",
      "iteration:   6 loss: 1.19879842\n",
      "iteration:   7 loss: 1.18196988\n",
      "iteration:   8 loss: 2.64621520\n",
      "iteration:   9 loss: 3.65163851\n",
      "iteration:  10 loss: 0.43468562\n",
      "iteration:  11 loss: 0.38385358\n",
      "iteration:  12 loss: 0.98744047\n",
      "iteration:  13 loss: 1.06684840\n",
      "iteration:  14 loss: 1.04606140\n",
      "iteration:  15 loss: 0.34301180\n",
      "iteration:  16 loss: 0.52456242\n",
      "iteration:  17 loss: 0.57963794\n",
      "iteration:  18 loss: 1.24310803\n",
      "iteration:  19 loss: 1.20973516\n",
      "iteration:  20 loss: 0.43509439\n",
      "iteration:  21 loss: 1.36913562\n",
      "iteration:  22 loss: 0.16358760\n",
      "iteration:  23 loss: 0.84136170\n",
      "iteration:  24 loss: 0.22928533\n",
      "iteration:  25 loss: 3.01282501\n",
      "iteration:  26 loss: 1.02783930\n",
      "iteration:  27 loss: 3.27413177\n",
      "iteration:  28 loss: 0.19569394\n",
      "iteration:  29 loss: 0.32259265\n",
      "iteration:  30 loss: 0.58520114\n",
      "iteration:  31 loss: 0.43083355\n",
      "iteration:  32 loss: 0.16217913\n",
      "iteration:  33 loss: 0.16751729\n",
      "iteration:  34 loss: 1.13452637\n",
      "iteration:  35 loss: 0.96927470\n",
      "iteration:  36 loss: 0.09608924\n",
      "iteration:  37 loss: 0.43610492\n",
      "iteration:  38 loss: 1.10916722\n",
      "iteration:  39 loss: 3.43621635\n",
      "iteration:  40 loss: 1.36489391\n",
      "iteration:  41 loss: 0.61364609\n",
      "iteration:  42 loss: 0.91364098\n",
      "iteration:  43 loss: 2.83917284\n",
      "iteration:  44 loss: 0.31030747\n",
      "iteration:  45 loss: 0.21184880\n",
      "iteration:  46 loss: 1.04224813\n",
      "iteration:  47 loss: 0.72267282\n",
      "iteration:  48 loss: 0.73192060\n",
      "iteration:  49 loss: 0.51067644\n",
      "iteration:  50 loss: 2.09633207\n",
      "iteration:  51 loss: 0.27737120\n",
      "iteration:  52 loss: 0.42613938\n",
      "iteration:  53 loss: 0.30316940\n",
      "iteration:  54 loss: 0.39786267\n",
      "iteration:  55 loss: 4.44640684\n",
      "iteration:  56 loss: 0.73886555\n",
      "iteration:  57 loss: 2.73654771\n",
      "iteration:  58 loss: 2.82723522\n",
      "iteration:  59 loss: 1.28698862\n",
      "iteration:  60 loss: 0.21421906\n",
      "iteration:  61 loss: 0.23803252\n",
      "iteration:  62 loss: 0.47574046\n",
      "iteration:  63 loss: 0.50640649\n",
      "iteration:  64 loss: 0.12077864\n",
      "iteration:  65 loss: 0.55815792\n",
      "iteration:  66 loss: 0.11228488\n",
      "iteration:  67 loss: 0.12609836\n",
      "iteration:  68 loss: 0.12689680\n",
      "iteration:  69 loss: 0.42139748\n",
      "iteration:  70 loss: 0.23820393\n",
      "iteration:  71 loss: 0.47956714\n",
      "iteration:  72 loss: 0.62042618\n",
      "iteration:  73 loss: 2.26481605\n",
      "iteration:  74 loss: 0.27291963\n",
      "iteration:  75 loss: 0.96061724\n",
      "iteration:  76 loss: 0.63916445\n",
      "iteration:  77 loss: 0.44961414\n",
      "iteration:  78 loss: 1.56061435\n",
      "iteration:  79 loss: 0.59447229\n",
      "iteration:  80 loss: 1.48395085\n",
      "iteration:  81 loss: 0.10879175\n",
      "iteration:  82 loss: 0.28574315\n",
      "iteration:  83 loss: 0.33181533\n",
      "iteration:  84 loss: 3.17637157\n",
      "iteration:  85 loss: 0.15672508\n",
      "iteration:  86 loss: 0.46616790\n",
      "iteration:  87 loss: 3.04017472\n",
      "iteration:  88 loss: 0.16612962\n",
      "iteration:  89 loss: 2.06145549\n",
      "iteration:  90 loss: 2.19438171\n",
      "iteration:  91 loss: 1.89945269\n",
      "iteration:  92 loss: 0.08619185\n",
      "iteration:  93 loss: 2.48616791\n",
      "iteration:  94 loss: 1.78769338\n",
      "iteration:  95 loss: 0.69067049\n",
      "iteration:  96 loss: 3.06831360\n",
      "iteration:  97 loss: 0.52237272\n",
      "iteration:  98 loss: 0.52330506\n",
      "iteration:  99 loss: 0.22958057\n",
      "iteration: 100 loss: 0.25501910\n",
      "iteration: 101 loss: 0.33482337\n",
      "iteration: 102 loss: 0.98931855\n",
      "iteration: 103 loss: 2.74994135\n",
      "iteration: 104 loss: 0.72438550\n",
      "iteration: 105 loss: 0.71907598\n",
      "iteration: 106 loss: 0.32024884\n",
      "iteration: 107 loss: 0.45597482\n",
      "iteration: 108 loss: 2.35492086\n",
      "iteration: 109 loss: 0.47158816\n",
      "iteration: 110 loss: 2.03979373\n",
      "iteration: 111 loss: 3.11584020\n",
      "iteration: 112 loss: 2.58842015\n",
      "iteration: 113 loss: 0.32015499\n",
      "iteration: 114 loss: 0.46524221\n",
      "iteration: 115 loss: 1.16483986\n",
      "iteration: 116 loss: 2.02271533\n",
      "iteration: 117 loss: 0.53003937\n",
      "iteration: 118 loss: 0.23365912\n",
      "iteration: 119 loss: 0.40853241\n",
      "iteration: 120 loss: 0.14978534\n",
      "iteration: 121 loss: 0.54389977\n",
      "iteration: 122 loss: 0.11975071\n",
      "iteration: 123 loss: 2.50704551\n",
      "iteration: 124 loss: 1.45184934\n",
      "iteration: 125 loss: 2.04810905\n",
      "iteration: 126 loss: 3.52463865\n",
      "iteration: 127 loss: 0.49176514\n",
      "iteration: 128 loss: 2.14111447\n",
      "iteration: 129 loss: 0.51067430\n",
      "iteration: 130 loss: 2.72434950\n",
      "iteration: 131 loss: 2.47466660\n",
      "iteration: 132 loss: 1.20120323\n",
      "iteration: 133 loss: 0.38683453\n",
      "iteration: 134 loss: 0.33397895\n",
      "iteration: 135 loss: 2.08825755\n",
      "iteration: 136 loss: 0.91647869\n",
      "iteration: 137 loss: 0.71331507\n",
      "iteration: 138 loss: 1.05459797\n",
      "iteration: 139 loss: 1.11591935\n",
      "iteration: 140 loss: 0.97779018\n",
      "iteration: 141 loss: 0.83203083\n",
      "iteration: 142 loss: 0.87331635\n",
      "iteration: 143 loss: 0.69074589\n",
      "iteration: 144 loss: 3.59964180\n",
      "iteration: 145 loss: 0.89137381\n",
      "iteration: 146 loss: 0.21140072\n",
      "iteration: 147 loss: 0.98265821\n",
      "iteration: 148 loss: 0.69440079\n",
      "iteration: 149 loss: 0.91340780\n",
      "iteration: 150 loss: 2.02414179\n",
      "iteration: 151 loss: 1.80977488\n",
      "iteration: 152 loss: 3.14078712\n",
      "iteration: 153 loss: 3.20209074\n",
      "iteration: 154 loss: 1.20104706\n",
      "iteration: 155 loss: 2.46700740\n",
      "iteration: 156 loss: 0.16181530\n",
      "iteration: 157 loss: 0.72310144\n",
      "iteration: 158 loss: 0.75241566\n",
      "iteration: 159 loss: 0.20583551\n",
      "iteration: 160 loss: 1.02979827\n",
      "iteration: 161 loss: 1.00811708\n",
      "iteration: 162 loss: 0.23052065\n",
      "iteration: 163 loss: 1.66953707\n",
      "iteration: 164 loss: 3.04380345\n",
      "iteration: 165 loss: 2.86480212\n",
      "iteration: 166 loss: 0.10528019\n",
      "iteration: 167 loss: 0.17372356\n",
      "iteration: 168 loss: 0.90658611\n",
      "iteration: 169 loss: 0.15452045\n",
      "iteration: 170 loss: 2.78794670\n",
      "iteration: 171 loss: 0.49335432\n",
      "iteration: 172 loss: 0.56681663\n",
      "iteration: 173 loss: 0.07878172\n",
      "iteration: 174 loss: 3.56439734\n",
      "iteration: 175 loss: 2.16358662\n",
      "iteration: 176 loss: 1.11964560\n",
      "iteration: 177 loss: 0.36539775\n",
      "iteration: 178 loss: 0.24444266\n",
      "iteration: 179 loss: 0.11571091\n",
      "iteration: 180 loss: 0.21495490\n",
      "iteration: 181 loss: 1.39894581\n",
      "iteration: 182 loss: 0.18757945\n",
      "iteration: 183 loss: 1.10597146\n",
      "iteration: 184 loss: 0.48193979\n",
      "iteration: 185 loss: 1.79533935\n",
      "iteration: 186 loss: 0.92309397\n",
      "iteration: 187 loss: 0.67822689\n",
      "iteration: 188 loss: 3.25363064\n",
      "iteration: 189 loss: 2.19427228\n",
      "iteration: 190 loss: 2.04598236\n",
      "iteration: 191 loss: 0.33037987\n",
      "iteration: 192 loss: 0.78496802\n",
      "iteration: 193 loss: 1.16477537\n",
      "iteration: 194 loss: 0.53187615\n",
      "iteration: 195 loss: 2.63398409\n",
      "iteration: 196 loss: 3.44141054\n",
      "iteration: 197 loss: 0.76893836\n",
      "iteration: 198 loss: 0.64922142\n",
      "iteration: 199 loss: 0.59561044\n",
      "epoch:  83 mean loss training: 1.13541484\n",
      "epoch:  83 mean loss validation: 1.37362862\n",
      "iteration:   0 loss: 3.10161304\n",
      "iteration:   1 loss: 0.60869372\n",
      "iteration:   2 loss: 0.83517146\n",
      "iteration:   3 loss: 0.75389135\n",
      "iteration:   4 loss: 1.92994905\n",
      "iteration:   5 loss: 1.54445302\n",
      "iteration:   6 loss: 1.29094303\n",
      "iteration:   7 loss: 0.89761484\n",
      "iteration:   8 loss: 2.45223308\n",
      "iteration:   9 loss: 3.33139586\n",
      "iteration:  10 loss: 0.51368862\n",
      "iteration:  11 loss: 0.43823776\n",
      "iteration:  12 loss: 0.79902858\n",
      "iteration:  13 loss: 0.93027687\n",
      "iteration:  14 loss: 0.97230661\n",
      "iteration:  15 loss: 0.49732450\n",
      "iteration:  16 loss: 0.56758898\n",
      "iteration:  17 loss: 0.82963479\n",
      "iteration:  18 loss: 1.29710388\n",
      "iteration:  19 loss: 0.45336398\n",
      "iteration:  20 loss: 0.22826211\n",
      "iteration:  21 loss: 0.46504959\n",
      "iteration:  22 loss: 0.19492733\n",
      "iteration:  23 loss: 0.79892355\n",
      "iteration:  24 loss: 0.17932746\n",
      "iteration:  25 loss: 2.66517472\n",
      "iteration:  26 loss: 1.25016272\n",
      "iteration:  27 loss: 2.82862496\n",
      "iteration:  28 loss: 0.16160984\n",
      "iteration:  29 loss: 0.33007395\n",
      "iteration:  30 loss: 0.35744384\n",
      "iteration:  31 loss: 0.20464535\n",
      "iteration:  32 loss: 0.17110968\n",
      "iteration:  33 loss: 0.17637359\n",
      "iteration:  34 loss: 1.78749394\n",
      "iteration:  35 loss: 0.96427035\n",
      "iteration:  36 loss: 0.09356789\n",
      "iteration:  37 loss: 0.38400435\n",
      "iteration:  38 loss: 1.10446835\n",
      "iteration:  39 loss: 3.26300836\n",
      "iteration:  40 loss: 1.43281400\n",
      "iteration:  41 loss: 0.54406470\n",
      "iteration:  42 loss: 0.71995962\n",
      "iteration:  43 loss: 2.82220173\n",
      "iteration:  44 loss: 0.26074946\n",
      "iteration:  45 loss: 0.11308500\n",
      "iteration:  46 loss: 1.19224274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  47 loss: 1.15016103\n",
      "iteration:  48 loss: 0.69919252\n",
      "iteration:  49 loss: 0.50897390\n",
      "iteration:  50 loss: 2.00565863\n",
      "iteration:  51 loss: 0.30157399\n",
      "iteration:  52 loss: 0.42981693\n",
      "iteration:  53 loss: 0.50857776\n",
      "iteration:  54 loss: 0.50296009\n",
      "iteration:  55 loss: 4.08836508\n",
      "iteration:  56 loss: 0.80341148\n",
      "iteration:  57 loss: 2.66789961\n",
      "iteration:  58 loss: 2.85369682\n",
      "iteration:  59 loss: 1.65502024\n",
      "iteration:  60 loss: 0.22216220\n",
      "iteration:  61 loss: 0.37903666\n",
      "iteration:  62 loss: 0.61856860\n",
      "iteration:  63 loss: 0.65086544\n",
      "iteration:  64 loss: 0.24622378\n",
      "iteration:  65 loss: 0.68478626\n",
      "iteration:  66 loss: 0.18281186\n",
      "iteration:  67 loss: 0.16796699\n",
      "iteration:  68 loss: 0.38528222\n",
      "iteration:  69 loss: 0.47017959\n",
      "iteration:  70 loss: 0.38067904\n",
      "iteration:  71 loss: 0.71698612\n",
      "iteration:  72 loss: 0.52428532\n",
      "iteration:  73 loss: 2.03747201\n",
      "iteration:  74 loss: 0.54098576\n",
      "iteration:  75 loss: 0.91849113\n",
      "iteration:  76 loss: 0.74625313\n",
      "iteration:  77 loss: 0.49785259\n",
      "iteration:  78 loss: 1.57895231\n",
      "iteration:  79 loss: 0.60811508\n",
      "iteration:  80 loss: 1.44037557\n",
      "iteration:  81 loss: 0.14762603\n",
      "iteration:  82 loss: 0.54750162\n",
      "iteration:  83 loss: 0.31859103\n",
      "iteration:  84 loss: 3.15994763\n",
      "iteration:  85 loss: 0.24621262\n",
      "iteration:  86 loss: 0.54424369\n",
      "iteration:  87 loss: 2.78175068\n",
      "iteration:  88 loss: 0.45735574\n",
      "iteration:  89 loss: 2.32146692\n",
      "iteration:  90 loss: 1.25717425\n",
      "iteration:  91 loss: 1.20108843\n",
      "iteration:  92 loss: 0.29987758\n",
      "iteration:  93 loss: 2.00491595\n",
      "iteration:  94 loss: 2.05648899\n",
      "iteration:  95 loss: 0.85140544\n",
      "iteration:  96 loss: 3.36421919\n",
      "iteration:  97 loss: 0.81616318\n",
      "iteration:  98 loss: 0.53332525\n",
      "iteration:  99 loss: 0.60847813\n",
      "iteration: 100 loss: 0.35097206\n",
      "iteration: 101 loss: 0.55297559\n",
      "iteration: 102 loss: 1.20930040\n",
      "iteration: 103 loss: 2.94127202\n",
      "iteration: 104 loss: 0.70514929\n",
      "iteration: 105 loss: 0.87948108\n",
      "iteration: 106 loss: 0.68362987\n",
      "iteration: 107 loss: 0.35207462\n",
      "iteration: 108 loss: 1.91788185\n",
      "iteration: 109 loss: 0.56499004\n",
      "iteration: 110 loss: 2.03756642\n",
      "iteration: 111 loss: 3.11996269\n",
      "iteration: 112 loss: 2.67130566\n",
      "iteration: 113 loss: 0.35356748\n",
      "iteration: 114 loss: 0.87311733\n",
      "iteration: 115 loss: 1.31051111\n",
      "iteration: 116 loss: 2.15121531\n",
      "iteration: 117 loss: 0.34219879\n",
      "iteration: 118 loss: 0.25830707\n",
      "iteration: 119 loss: 0.61594826\n",
      "iteration: 120 loss: 0.47997424\n",
      "iteration: 121 loss: 0.52045768\n",
      "iteration: 122 loss: 0.12245461\n",
      "iteration: 123 loss: 1.48590171\n",
      "iteration: 124 loss: 1.25776100\n",
      "iteration: 125 loss: 2.40592265\n",
      "iteration: 126 loss: 3.14672589\n",
      "iteration: 127 loss: 0.49899685\n",
      "iteration: 128 loss: 1.90087414\n",
      "iteration: 129 loss: 0.61130631\n",
      "iteration: 130 loss: 2.81454325\n",
      "iteration: 131 loss: 2.51578307\n",
      "iteration: 132 loss: 0.88677949\n",
      "iteration: 133 loss: 0.39694721\n",
      "iteration: 134 loss: 0.40397793\n",
      "iteration: 135 loss: 1.88254404\n",
      "iteration: 136 loss: 1.06135523\n",
      "iteration: 137 loss: 0.77339673\n",
      "iteration: 138 loss: 1.36250842\n",
      "iteration: 139 loss: 1.46289062\n",
      "iteration: 140 loss: 0.91684508\n",
      "iteration: 141 loss: 1.05478013\n",
      "iteration: 142 loss: 1.06513178\n",
      "iteration: 143 loss: 0.72316450\n",
      "iteration: 144 loss: 3.86938596\n",
      "iteration: 145 loss: 1.17018950\n",
      "iteration: 146 loss: 0.31219167\n",
      "iteration: 147 loss: 1.26802135\n",
      "iteration: 148 loss: 1.02034330\n",
      "iteration: 149 loss: 0.96821058\n",
      "iteration: 150 loss: 2.48704600\n",
      "iteration: 151 loss: 1.29917109\n",
      "iteration: 152 loss: 3.24023938\n",
      "iteration: 153 loss: 3.23962784\n",
      "iteration: 154 loss: 1.66715813\n",
      "iteration: 155 loss: 3.10665488\n",
      "iteration: 156 loss: 0.34090018\n",
      "iteration: 157 loss: 0.81221378\n",
      "iteration: 158 loss: 0.74248809\n",
      "iteration: 159 loss: 0.39864922\n",
      "iteration: 160 loss: 1.03123879\n",
      "iteration: 161 loss: 0.83407319\n",
      "iteration: 162 loss: 0.22962163\n",
      "iteration: 163 loss: 1.81709254\n",
      "iteration: 164 loss: 2.99146533\n",
      "iteration: 165 loss: 0.37507576\n",
      "iteration: 166 loss: 0.29145727\n",
      "iteration: 167 loss: 0.23972166\n",
      "iteration: 168 loss: 0.89920491\n",
      "iteration: 169 loss: 0.19322869\n",
      "iteration: 170 loss: 2.75584006\n",
      "iteration: 171 loss: 0.46551067\n",
      "iteration: 172 loss: 0.29091573\n",
      "iteration: 173 loss: 0.09189331\n",
      "iteration: 174 loss: 3.51083064\n",
      "iteration: 175 loss: 2.60257316\n",
      "iteration: 176 loss: 0.68066722\n",
      "iteration: 177 loss: 0.26570091\n",
      "iteration: 178 loss: 0.32164076\n",
      "iteration: 179 loss: 0.11390205\n",
      "iteration: 180 loss: 0.21407470\n",
      "iteration: 181 loss: 1.19517446\n",
      "iteration: 182 loss: 0.13786685\n",
      "iteration: 183 loss: 0.86615282\n",
      "iteration: 184 loss: 0.59628451\n",
      "iteration: 185 loss: 1.97223163\n",
      "iteration: 186 loss: 1.95991254\n",
      "iteration: 187 loss: 0.50334704\n",
      "iteration: 188 loss: 2.79171395\n",
      "iteration: 189 loss: 2.16565871\n",
      "iteration: 190 loss: 2.36966014\n",
      "iteration: 191 loss: 0.57165217\n",
      "iteration: 192 loss: 1.13675773\n",
      "iteration: 193 loss: 0.85788280\n",
      "iteration: 194 loss: 0.58519161\n",
      "iteration: 195 loss: 2.71930552\n",
      "iteration: 196 loss: 3.63285542\n",
      "iteration: 197 loss: 0.85610920\n",
      "iteration: 198 loss: 0.90994853\n",
      "iteration: 199 loss: 0.74230731\n",
      "epoch:  84 mean loss training: 1.15400159\n",
      "epoch:  84 mean loss validation: 1.36481094\n",
      "iteration:   0 loss: 3.32610846\n",
      "iteration:   1 loss: 0.59107047\n",
      "iteration:   2 loss: 0.81797451\n",
      "iteration:   3 loss: 0.85921681\n",
      "iteration:   4 loss: 1.34353685\n",
      "iteration:   5 loss: 1.31223774\n",
      "iteration:   6 loss: 1.60655177\n",
      "iteration:   7 loss: 1.12356055\n",
      "iteration:   8 loss: 2.46792030\n",
      "iteration:   9 loss: 3.16505599\n",
      "iteration:  10 loss: 0.46271503\n",
      "iteration:  11 loss: 0.38712773\n",
      "iteration:  12 loss: 1.39415824\n",
      "iteration:  13 loss: 0.78632379\n",
      "iteration:  14 loss: 1.00540984\n",
      "iteration:  15 loss: 0.41817483\n",
      "iteration:  16 loss: 0.44869113\n",
      "iteration:  17 loss: 0.70712173\n",
      "iteration:  18 loss: 1.26232755\n",
      "iteration:  19 loss: 0.74491525\n",
      "iteration:  20 loss: 0.50880909\n",
      "iteration:  21 loss: 1.13189662\n",
      "iteration:  22 loss: 0.17825142\n",
      "iteration:  23 loss: 1.11873388\n",
      "iteration:  24 loss: 0.17696406\n",
      "iteration:  25 loss: 2.01450348\n",
      "iteration:  26 loss: 1.28057814\n",
      "iteration:  27 loss: 3.63567710\n",
      "iteration:  28 loss: 0.15591674\n",
      "iteration:  29 loss: 0.18536733\n",
      "iteration:  30 loss: 0.47450101\n",
      "iteration:  31 loss: 0.20444052\n",
      "iteration:  32 loss: 0.18945614\n",
      "iteration:  33 loss: 0.21436062\n",
      "iteration:  34 loss: 1.23600507\n",
      "iteration:  35 loss: 1.13905394\n",
      "iteration:  36 loss: 0.10364568\n",
      "iteration:  37 loss: 0.41861194\n",
      "iteration:  38 loss: 1.13513434\n",
      "iteration:  39 loss: 3.23261690\n",
      "iteration:  40 loss: 1.44546330\n",
      "iteration:  41 loss: 1.05313337\n",
      "iteration:  42 loss: 1.23448288\n",
      "iteration:  43 loss: 2.83388233\n",
      "iteration:  44 loss: 0.69220072\n",
      "iteration:  45 loss: 0.09342672\n",
      "iteration:  46 loss: 1.79077840\n",
      "iteration:  47 loss: 0.89773822\n",
      "iteration:  48 loss: 0.76086485\n",
      "iteration:  49 loss: 0.54455483\n",
      "iteration:  50 loss: 2.39161634\n",
      "iteration:  51 loss: 0.34614420\n",
      "iteration:  52 loss: 0.82784450\n",
      "iteration:  53 loss: 0.73167741\n",
      "iteration:  54 loss: 0.41688603\n",
      "iteration:  55 loss: 3.35755134\n",
      "iteration:  56 loss: 0.85867661\n",
      "iteration:  57 loss: 2.93667817\n",
      "iteration:  58 loss: 2.40362477\n",
      "iteration:  59 loss: 1.75392163\n",
      "iteration:  60 loss: 0.34487715\n",
      "iteration:  61 loss: 0.24717160\n",
      "iteration:  62 loss: 0.41553512\n",
      "iteration:  63 loss: 0.80633867\n",
      "iteration:  64 loss: 0.26994824\n",
      "iteration:  65 loss: 0.71304601\n",
      "iteration:  66 loss: 0.39269158\n",
      "iteration:  67 loss: 0.12714952\n",
      "iteration:  68 loss: 0.14015797\n",
      "iteration:  69 loss: 0.32427025\n",
      "iteration:  70 loss: 0.38436583\n",
      "iteration:  71 loss: 0.74954283\n",
      "iteration:  72 loss: 0.49651167\n",
      "iteration:  73 loss: 2.69814515\n",
      "iteration:  74 loss: 0.29667428\n",
      "iteration:  75 loss: 0.47322297\n",
      "iteration:  76 loss: 0.59607977\n",
      "iteration:  77 loss: 0.42166743\n",
      "iteration:  78 loss: 1.65590978\n",
      "iteration:  79 loss: 0.60721499\n",
      "iteration:  80 loss: 1.51488709\n",
      "iteration:  81 loss: 0.15941322\n",
      "iteration:  82 loss: 0.35287136\n",
      "iteration:  83 loss: 0.38185257\n",
      "iteration:  84 loss: 3.15972137\n",
      "iteration:  85 loss: 0.15829405\n",
      "iteration:  86 loss: 0.52122766\n",
      "iteration:  87 loss: 3.28608441\n",
      "iteration:  88 loss: 0.18312205\n",
      "iteration:  89 loss: 2.12514448\n",
      "iteration:  90 loss: 1.61263216\n",
      "iteration:  91 loss: 0.43661729\n",
      "iteration:  92 loss: 0.25241113\n",
      "iteration:  93 loss: 2.27972651\n",
      "iteration:  94 loss: 0.96275896\n",
      "iteration:  95 loss: 0.68475783\n",
      "iteration:  96 loss: 3.62337756\n",
      "iteration:  97 loss: 0.46243683\n",
      "iteration:  98 loss: 0.56425339\n",
      "iteration:  99 loss: 0.21361354\n",
      "iteration: 100 loss: 0.31827292\n",
      "iteration: 101 loss: 0.29117751\n",
      "iteration: 102 loss: 0.98131132\n",
      "iteration: 103 loss: 2.74351549\n",
      "iteration: 104 loss: 0.56520647\n",
      "iteration: 105 loss: 0.87475669\n",
      "iteration: 106 loss: 0.59196186\n",
      "iteration: 107 loss: 0.26051557\n",
      "iteration: 108 loss: 1.96862555\n",
      "iteration: 109 loss: 0.63487810\n",
      "iteration: 110 loss: 1.97589159\n",
      "iteration: 111 loss: 3.12287498\n",
      "iteration: 112 loss: 2.69459748\n",
      "iteration: 113 loss: 0.41217780\n",
      "iteration: 114 loss: 0.62976372\n",
      "iteration: 115 loss: 1.44071674\n",
      "iteration: 116 loss: 2.27327800\n",
      "iteration: 117 loss: 0.43135124\n",
      "iteration: 118 loss: 0.28074482\n",
      "iteration: 119 loss: 0.42876664\n",
      "iteration: 120 loss: 0.33715788\n",
      "iteration: 121 loss: 0.70046389\n",
      "iteration: 122 loss: 0.11364423\n",
      "iteration: 123 loss: 2.51079631\n",
      "iteration: 124 loss: 1.27474451\n",
      "iteration: 125 loss: 1.71889937\n",
      "iteration: 126 loss: 3.17701983\n",
      "iteration: 127 loss: 0.26967469\n",
      "iteration: 128 loss: 2.07864022\n",
      "iteration: 129 loss: 0.21626665\n",
      "iteration: 130 loss: 2.68567729\n",
      "iteration: 131 loss: 2.50264597\n",
      "iteration: 132 loss: 0.98028529\n",
      "iteration: 133 loss: 0.41304663\n",
      "iteration: 134 loss: 0.37002093\n",
      "iteration: 135 loss: 2.10651159\n",
      "iteration: 136 loss: 0.97293115\n",
      "iteration: 137 loss: 0.97327942\n",
      "iteration: 138 loss: 1.03527677\n",
      "iteration: 139 loss: 1.19368112\n",
      "iteration: 140 loss: 0.98572695\n",
      "iteration: 141 loss: 0.94409865\n",
      "iteration: 142 loss: 0.96653557\n",
      "iteration: 143 loss: 0.70508969\n",
      "iteration: 144 loss: 3.65998769\n",
      "iteration: 145 loss: 1.01761174\n",
      "iteration: 146 loss: 0.21600430\n",
      "iteration: 147 loss: 0.88506687\n",
      "iteration: 148 loss: 0.83332270\n",
      "iteration: 149 loss: 0.67416143\n",
      "iteration: 150 loss: 1.83752120\n",
      "iteration: 151 loss: 1.99480069\n",
      "iteration: 152 loss: 3.10214019\n",
      "iteration: 153 loss: 3.23407912\n",
      "iteration: 154 loss: 1.11672354\n",
      "iteration: 155 loss: 2.52975869\n",
      "iteration: 156 loss: 0.18163963\n",
      "iteration: 157 loss: 0.72344136\n",
      "iteration: 158 loss: 0.71045929\n",
      "iteration: 159 loss: 0.20655346\n",
      "iteration: 160 loss: 1.06385016\n",
      "iteration: 161 loss: 0.88322502\n",
      "iteration: 162 loss: 0.18841924\n",
      "iteration: 163 loss: 2.07230210\n",
      "iteration: 164 loss: 3.55632734\n",
      "iteration: 165 loss: 2.58667636\n",
      "iteration: 166 loss: 0.26422167\n",
      "iteration: 167 loss: 0.17149958\n",
      "iteration: 168 loss: 0.83763772\n",
      "iteration: 169 loss: 0.27483422\n",
      "iteration: 170 loss: 2.94057584\n",
      "iteration: 171 loss: 0.48045164\n",
      "iteration: 172 loss: 0.57474762\n",
      "iteration: 173 loss: 0.13078262\n",
      "iteration: 174 loss: 3.52732182\n",
      "iteration: 175 loss: 2.04190874\n",
      "iteration: 176 loss: 0.95276695\n",
      "iteration: 177 loss: 0.59970218\n",
      "iteration: 178 loss: 0.24234216\n",
      "iteration: 179 loss: 0.26635712\n",
      "iteration: 180 loss: 0.56758201\n",
      "iteration: 181 loss: 1.49042201\n",
      "iteration: 182 loss: 0.13827261\n",
      "iteration: 183 loss: 1.42164195\n",
      "iteration: 184 loss: 0.31876728\n",
      "iteration: 185 loss: 1.94653869\n",
      "iteration: 186 loss: 1.68073344\n",
      "iteration: 187 loss: 0.73442429\n",
      "iteration: 188 loss: 3.15325308\n",
      "iteration: 189 loss: 2.24832249\n",
      "iteration: 190 loss: 1.99401093\n",
      "iteration: 191 loss: 0.25961944\n",
      "iteration: 192 loss: 0.87157130\n",
      "iteration: 193 loss: 1.11472261\n",
      "iteration: 194 loss: 0.16039690\n",
      "iteration: 195 loss: 2.71926260\n",
      "iteration: 196 loss: 3.43140793\n",
      "iteration: 197 loss: 0.67098242\n",
      "iteration: 198 loss: 1.00906086\n",
      "iteration: 199 loss: 0.59759980\n",
      "epoch:  85 mean loss training: 1.15345955\n",
      "epoch:  85 mean loss validation: 1.38282025\n",
      "iteration:   0 loss: 2.51659822\n",
      "iteration:   1 loss: 0.63716114\n",
      "iteration:   2 loss: 0.67966825\n",
      "iteration:   3 loss: 0.70394576\n",
      "iteration:   4 loss: 2.42835045\n",
      "iteration:   5 loss: 1.34012985\n",
      "iteration:   6 loss: 1.25121415\n",
      "iteration:   7 loss: 0.99067843\n",
      "iteration:   8 loss: 2.64511108\n",
      "iteration:   9 loss: 3.97019720\n",
      "iteration:  10 loss: 0.43707126\n",
      "iteration:  11 loss: 1.07971728\n",
      "iteration:  12 loss: 0.88501716\n",
      "iteration:  13 loss: 1.39445662\n",
      "iteration:  14 loss: 1.00152147\n",
      "iteration:  15 loss: 0.35361177\n",
      "iteration:  16 loss: 0.76469278\n",
      "iteration:  17 loss: 0.88735634\n",
      "iteration:  18 loss: 1.40723050\n",
      "iteration:  19 loss: 1.35852981\n",
      "iteration:  20 loss: 0.52640331\n",
      "iteration:  21 loss: 0.91471630\n",
      "iteration:  22 loss: 0.24639936\n",
      "iteration:  23 loss: 0.82935405\n",
      "iteration:  24 loss: 0.21200810\n",
      "iteration:  25 loss: 2.05913448\n",
      "iteration:  26 loss: 1.17808521\n",
      "iteration:  27 loss: 2.70055556\n",
      "iteration:  28 loss: 0.28226930\n",
      "iteration:  29 loss: 0.36662227\n",
      "iteration:  30 loss: 0.59750181\n",
      "iteration:  31 loss: 0.45725122\n",
      "iteration:  32 loss: 0.24806768\n",
      "iteration:  33 loss: 0.14483620\n",
      "iteration:  34 loss: 1.25678480\n",
      "iteration:  35 loss: 1.00927937\n",
      "iteration:  36 loss: 0.08586578\n",
      "iteration:  37 loss: 0.42134941\n",
      "iteration:  38 loss: 1.10065758\n",
      "iteration:  39 loss: 3.45769882\n",
      "iteration:  40 loss: 1.37227237\n",
      "iteration:  41 loss: 0.60009837\n",
      "iteration:  42 loss: 0.90591407\n",
      "iteration:  43 loss: 2.83516026\n",
      "iteration:  44 loss: 0.48399910\n",
      "iteration:  45 loss: 0.13389735\n",
      "iteration:  46 loss: 0.90268338\n",
      "iteration:  47 loss: 0.63643438\n",
      "iteration:  48 loss: 0.79015452\n",
      "iteration:  49 loss: 0.50801188\n",
      "iteration:  50 loss: 2.08480287\n",
      "iteration:  51 loss: 0.28169432\n",
      "iteration:  52 loss: 0.36653608\n",
      "iteration:  53 loss: 0.24008562\n",
      "iteration:  54 loss: 0.41454658\n",
      "iteration:  55 loss: 4.54909277\n",
      "iteration:  56 loss: 0.61224544\n",
      "iteration:  57 loss: 2.68799162\n",
      "iteration:  58 loss: 2.79377532\n",
      "iteration:  59 loss: 1.29219937\n",
      "iteration:  60 loss: 0.17938930\n",
      "iteration:  61 loss: 0.22324508\n",
      "iteration:  62 loss: 0.38582060\n",
      "iteration:  63 loss: 0.47328791\n",
      "iteration:  64 loss: 0.12887557\n",
      "iteration:  65 loss: 0.45675981\n",
      "iteration:  66 loss: 0.11385446\n",
      "iteration:  67 loss: 0.10334303\n",
      "iteration:  68 loss: 0.11385638\n",
      "iteration:  69 loss: 0.46940216\n",
      "iteration:  70 loss: 0.19774318\n",
      "iteration:  71 loss: 0.66822523\n",
      "iteration:  72 loss: 0.63908690\n",
      "iteration:  73 loss: 2.40876842\n",
      "iteration:  74 loss: 0.31412846\n",
      "iteration:  75 loss: 0.73819339\n",
      "iteration:  76 loss: 0.71454030\n",
      "iteration:  77 loss: 0.68471336\n",
      "iteration:  78 loss: 1.34038055\n",
      "iteration:  79 loss: 0.51632869\n",
      "iteration:  80 loss: 1.06211126\n",
      "iteration:  81 loss: 0.10485982\n",
      "iteration:  82 loss: 0.22098146\n",
      "iteration:  83 loss: 0.62865967\n",
      "iteration:  84 loss: 3.69045734\n",
      "iteration:  85 loss: 0.13595001\n",
      "iteration:  86 loss: 0.45312345\n",
      "iteration:  87 loss: 2.81895208\n",
      "iteration:  88 loss: 0.21642837\n",
      "iteration:  89 loss: 2.42579579\n",
      "iteration:  90 loss: 2.38284683\n",
      "iteration:  91 loss: 1.98715663\n",
      "iteration:  92 loss: 0.08062290\n",
      "iteration:  93 loss: 2.41084409\n",
      "iteration:  94 loss: 1.94016314\n",
      "iteration:  95 loss: 0.13501810\n",
      "iteration:  96 loss: 4.18112659\n",
      "iteration:  97 loss: 0.93426085\n",
      "iteration:  98 loss: 0.37658897\n",
      "iteration:  99 loss: 0.14565454\n",
      "iteration: 100 loss: 0.22861730\n",
      "iteration: 101 loss: 0.14930245\n",
      "iteration: 102 loss: 1.21667504\n",
      "iteration: 103 loss: 2.55537343\n",
      "iteration: 104 loss: 0.24391411\n",
      "iteration: 105 loss: 0.42868543\n",
      "iteration: 106 loss: 0.18697196\n",
      "iteration: 107 loss: 0.80901647\n",
      "iteration: 108 loss: 0.77954710\n",
      "iteration: 109 loss: 0.13079138\n",
      "iteration: 110 loss: 1.98032236\n",
      "iteration: 111 loss: 3.10890722\n",
      "iteration: 112 loss: 2.59246206\n",
      "iteration: 113 loss: 0.31587881\n",
      "iteration: 114 loss: 0.44975403\n",
      "iteration: 115 loss: 1.04118752\n",
      "iteration: 116 loss: 2.00342107\n",
      "iteration: 117 loss: 0.46673167\n",
      "iteration: 118 loss: 0.37257168\n",
      "iteration: 119 loss: 0.44729203\n",
      "iteration: 120 loss: 0.19529106\n",
      "iteration: 121 loss: 0.56501454\n",
      "iteration: 122 loss: 0.14197063\n",
      "iteration: 123 loss: 2.36039209\n",
      "iteration: 124 loss: 1.25641608\n",
      "iteration: 125 loss: 2.03972244\n",
      "iteration: 126 loss: 3.29317880\n",
      "iteration: 127 loss: 0.28189039\n",
      "iteration: 128 loss: 2.28427815\n",
      "iteration: 129 loss: 0.17739636\n",
      "iteration: 130 loss: 2.58952904\n",
      "iteration: 131 loss: 2.74987411\n",
      "iteration: 132 loss: 1.25217009\n",
      "iteration: 133 loss: 0.69487458\n",
      "iteration: 134 loss: 0.29050216\n",
      "iteration: 135 loss: 2.08543253\n",
      "iteration: 136 loss: 1.05393827\n",
      "iteration: 137 loss: 0.46253467\n",
      "iteration: 138 loss: 1.23927224\n",
      "iteration: 139 loss: 1.28037655\n",
      "iteration: 140 loss: 0.91930133\n",
      "iteration: 141 loss: 1.02826345\n",
      "iteration: 142 loss: 0.69656903\n",
      "iteration: 143 loss: 0.69572437\n",
      "iteration: 144 loss: 3.53795075\n",
      "iteration: 145 loss: 2.07834125\n",
      "iteration: 146 loss: 0.30303067\n",
      "iteration: 147 loss: 0.98344755\n",
      "iteration: 148 loss: 0.91088510\n",
      "iteration: 149 loss: 0.56300181\n",
      "iteration: 150 loss: 1.82673180\n",
      "iteration: 151 loss: 2.16689038\n",
      "iteration: 152 loss: 3.28188276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 153 loss: 3.24316025\n",
      "iteration: 154 loss: 1.55865300\n",
      "iteration: 155 loss: 2.69391537\n",
      "iteration: 156 loss: 0.35515231\n",
      "iteration: 157 loss: 0.76501757\n",
      "iteration: 158 loss: 0.73913014\n",
      "iteration: 159 loss: 0.28683838\n",
      "iteration: 160 loss: 1.06255448\n",
      "iteration: 161 loss: 0.73319846\n",
      "iteration: 162 loss: 0.21631636\n",
      "iteration: 163 loss: 2.26775956\n",
      "iteration: 164 loss: 3.10133362\n",
      "iteration: 165 loss: 1.60558653\n",
      "iteration: 166 loss: 0.13449280\n",
      "iteration: 167 loss: 0.17383833\n",
      "iteration: 168 loss: 0.92201138\n",
      "iteration: 169 loss: 0.21727003\n",
      "iteration: 170 loss: 2.95411205\n",
      "iteration: 171 loss: 0.47467348\n",
      "iteration: 172 loss: 0.51912969\n",
      "iteration: 173 loss: 0.12017187\n",
      "iteration: 174 loss: 4.04733372\n",
      "iteration: 175 loss: 3.21832824\n",
      "iteration: 176 loss: 0.64524567\n",
      "iteration: 177 loss: 0.39745516\n",
      "iteration: 178 loss: 0.22809066\n",
      "iteration: 179 loss: 0.10538686\n",
      "iteration: 180 loss: 0.32689047\n",
      "iteration: 181 loss: 1.16865087\n",
      "iteration: 182 loss: 0.15497820\n",
      "iteration: 183 loss: 2.29265666\n",
      "iteration: 184 loss: 0.58550727\n",
      "iteration: 185 loss: 2.10773730\n",
      "iteration: 186 loss: 1.47525442\n",
      "iteration: 187 loss: 0.58279049\n",
      "iteration: 188 loss: 2.84275270\n",
      "iteration: 189 loss: 2.23507452\n",
      "iteration: 190 loss: 1.89712334\n",
      "iteration: 191 loss: 0.44998267\n",
      "iteration: 192 loss: 1.33081937\n",
      "iteration: 193 loss: 0.72634625\n",
      "iteration: 194 loss: 0.16279940\n",
      "iteration: 195 loss: 2.80247808\n",
      "iteration: 196 loss: 3.55657578\n",
      "iteration: 197 loss: 0.66741610\n",
      "iteration: 198 loss: 0.85699970\n",
      "iteration: 199 loss: 0.64787054\n",
      "epoch:  86 mean loss training: 1.15485311\n",
      "epoch:  86 mean loss validation: 1.38980687\n",
      "iteration:   0 loss: 2.87430429\n",
      "iteration:   1 loss: 0.59725845\n",
      "iteration:   2 loss: 1.00435293\n",
      "iteration:   3 loss: 0.78090739\n",
      "iteration:   4 loss: 1.40406394\n",
      "iteration:   5 loss: 1.54581618\n",
      "iteration:   6 loss: 1.20015335\n",
      "iteration:   7 loss: 0.84187412\n",
      "iteration:   8 loss: 2.66806197\n",
      "iteration:   9 loss: 4.01455116\n",
      "iteration:  10 loss: 0.47893283\n",
      "iteration:  11 loss: 0.39579597\n",
      "iteration:  12 loss: 0.76663458\n",
      "iteration:  13 loss: 1.12755191\n",
      "iteration:  14 loss: 1.02546489\n",
      "iteration:  15 loss: 0.45545626\n",
      "iteration:  16 loss: 0.55417699\n",
      "iteration:  17 loss: 0.68307406\n",
      "iteration:  18 loss: 1.38591850\n",
      "iteration:  19 loss: 1.13286924\n",
      "iteration:  20 loss: 0.82870781\n",
      "iteration:  21 loss: 0.75981575\n",
      "iteration:  22 loss: 0.20592709\n",
      "iteration:  23 loss: 0.95584613\n",
      "iteration:  24 loss: 0.19509712\n",
      "iteration:  25 loss: 1.99581778\n",
      "iteration:  26 loss: 1.06624484\n",
      "iteration:  27 loss: 2.91126680\n",
      "iteration:  28 loss: 0.29774714\n",
      "iteration:  29 loss: 0.23443054\n",
      "iteration:  30 loss: 0.51519269\n",
      "iteration:  31 loss: 0.41672331\n",
      "iteration:  32 loss: 0.27045956\n",
      "iteration:  33 loss: 0.15500365\n",
      "iteration:  34 loss: 1.27288675\n",
      "iteration:  35 loss: 1.09621406\n",
      "iteration:  36 loss: 0.09385595\n",
      "iteration:  37 loss: 0.40591851\n",
      "iteration:  38 loss: 1.12181818\n",
      "iteration:  39 loss: 3.40368032\n",
      "iteration:  40 loss: 1.24130309\n",
      "iteration:  41 loss: 0.52867025\n",
      "iteration:  42 loss: 0.75381356\n",
      "iteration:  43 loss: 2.81554055\n",
      "iteration:  44 loss: 0.70973188\n",
      "iteration:  45 loss: 0.10510737\n",
      "iteration:  46 loss: 1.25366724\n",
      "iteration:  47 loss: 0.84858203\n",
      "iteration:  48 loss: 0.76398695\n",
      "iteration:  49 loss: 0.51724428\n",
      "iteration:  50 loss: 2.33894086\n",
      "iteration:  51 loss: 0.32683396\n",
      "iteration:  52 loss: 0.59621352\n",
      "iteration:  53 loss: 0.58186752\n",
      "iteration:  54 loss: 0.34305072\n",
      "iteration:  55 loss: 3.69331622\n",
      "iteration:  56 loss: 0.64921117\n",
      "iteration:  57 loss: 2.90857863\n",
      "iteration:  58 loss: 2.28319430\n",
      "iteration:  59 loss: 1.80994499\n",
      "iteration:  60 loss: 0.31835851\n",
      "iteration:  61 loss: 0.19298241\n",
      "iteration:  62 loss: 0.29442784\n",
      "iteration:  63 loss: 0.76301134\n",
      "iteration:  64 loss: 0.40586969\n",
      "iteration:  65 loss: 0.82187784\n",
      "iteration:  66 loss: 0.43903014\n",
      "iteration:  67 loss: 0.10128056\n",
      "iteration:  68 loss: 0.14335488\n",
      "iteration:  69 loss: 0.30865157\n",
      "iteration:  70 loss: 0.38456887\n",
      "iteration:  71 loss: 0.73942906\n",
      "iteration:  72 loss: 0.58921087\n",
      "iteration:  73 loss: 2.56030154\n",
      "iteration:  74 loss: 0.34964111\n",
      "iteration:  75 loss: 0.42743480\n",
      "iteration:  76 loss: 0.78749382\n",
      "iteration:  77 loss: 0.54681069\n",
      "iteration:  78 loss: 1.26006365\n",
      "iteration:  79 loss: 0.49957031\n",
      "iteration:  80 loss: 1.53584957\n",
      "iteration:  81 loss: 0.11035880\n",
      "iteration:  82 loss: 0.23014963\n",
      "iteration:  83 loss: 1.07536566\n",
      "iteration:  84 loss: 3.06495905\n",
      "iteration:  85 loss: 0.14838132\n",
      "iteration:  86 loss: 0.44905841\n",
      "iteration:  87 loss: 3.03251767\n",
      "iteration:  88 loss: 0.22368975\n",
      "iteration:  89 loss: 2.57224274\n",
      "iteration:  90 loss: 1.73787451\n",
      "iteration:  91 loss: 0.28655937\n",
      "iteration:  92 loss: 0.31512886\n",
      "iteration:  93 loss: 2.52059603\n",
      "iteration:  94 loss: 1.09752572\n",
      "iteration:  95 loss: 0.67075193\n",
      "iteration:  96 loss: 3.50394559\n",
      "iteration:  97 loss: 0.76471829\n",
      "iteration:  98 loss: 0.51030165\n",
      "iteration:  99 loss: 0.17868564\n",
      "iteration: 100 loss: 0.24547631\n",
      "iteration: 101 loss: 0.17486624\n",
      "iteration: 102 loss: 0.89570856\n",
      "iteration: 103 loss: 2.84364295\n",
      "iteration: 104 loss: 0.32651955\n",
      "iteration: 105 loss: 0.79878700\n",
      "iteration: 106 loss: 0.35395050\n",
      "iteration: 107 loss: 0.20118456\n",
      "iteration: 108 loss: 1.39883816\n",
      "iteration: 109 loss: 0.45907009\n",
      "iteration: 110 loss: 1.94979799\n",
      "iteration: 111 loss: 3.12728453\n",
      "iteration: 112 loss: 2.63514066\n",
      "iteration: 113 loss: 0.33918476\n",
      "iteration: 114 loss: 0.51409447\n",
      "iteration: 115 loss: 0.97412878\n",
      "iteration: 116 loss: 2.13887858\n",
      "iteration: 117 loss: 0.45805418\n",
      "iteration: 118 loss: 0.27791595\n",
      "iteration: 119 loss: 0.49866039\n",
      "iteration: 120 loss: 0.15734896\n",
      "iteration: 121 loss: 0.66448551\n",
      "iteration: 122 loss: 0.11875658\n",
      "iteration: 123 loss: 2.55456591\n",
      "iteration: 124 loss: 1.13742566\n",
      "iteration: 125 loss: 1.28737986\n",
      "iteration: 126 loss: 3.19145846\n",
      "iteration: 127 loss: 0.24160230\n",
      "iteration: 128 loss: 1.72220480\n",
      "iteration: 129 loss: 0.17277165\n",
      "iteration: 130 loss: 2.70623469\n",
      "iteration: 131 loss: 2.71732187\n",
      "iteration: 132 loss: 1.10408008\n",
      "iteration: 133 loss: 0.42229217\n",
      "iteration: 134 loss: 0.36585617\n",
      "iteration: 135 loss: 2.08921599\n",
      "iteration: 136 loss: 0.94350976\n",
      "iteration: 137 loss: 0.75238580\n",
      "iteration: 138 loss: 0.97876596\n",
      "iteration: 139 loss: 1.12909484\n",
      "iteration: 140 loss: 1.03449011\n",
      "iteration: 141 loss: 0.76348364\n",
      "iteration: 142 loss: 0.78833026\n",
      "iteration: 143 loss: 0.69142079\n",
      "iteration: 144 loss: 3.52200675\n",
      "iteration: 145 loss: 1.12653017\n",
      "iteration: 146 loss: 0.30583078\n",
      "iteration: 147 loss: 0.82889825\n",
      "iteration: 148 loss: 0.78498852\n",
      "iteration: 149 loss: 0.63739371\n",
      "iteration: 150 loss: 1.82928038\n",
      "iteration: 151 loss: 2.07870555\n",
      "iteration: 152 loss: 3.14272165\n",
      "iteration: 153 loss: 3.16820669\n",
      "iteration: 154 loss: 1.31352592\n",
      "iteration: 155 loss: 2.62168336\n",
      "iteration: 156 loss: 0.17134178\n",
      "iteration: 157 loss: 0.73689497\n",
      "iteration: 158 loss: 0.69472480\n",
      "iteration: 159 loss: 0.23891373\n",
      "iteration: 160 loss: 1.04716027\n",
      "iteration: 161 loss: 0.86312145\n",
      "iteration: 162 loss: 0.20325184\n",
      "iteration: 163 loss: 1.79521668\n",
      "iteration: 164 loss: 3.56820083\n",
      "iteration: 165 loss: 2.29338050\n",
      "iteration: 166 loss: 0.24458425\n",
      "iteration: 167 loss: 0.16844359\n",
      "iteration: 168 loss: 0.84349120\n",
      "iteration: 169 loss: 0.27472293\n",
      "iteration: 170 loss: 2.99512601\n",
      "iteration: 171 loss: 0.48710987\n",
      "iteration: 172 loss: 0.54565960\n",
      "iteration: 173 loss: 0.13263915\n",
      "iteration: 174 loss: 3.50798392\n",
      "iteration: 175 loss: 2.26256537\n",
      "iteration: 176 loss: 0.93615544\n",
      "iteration: 177 loss: 0.56984150\n",
      "iteration: 178 loss: 0.22816782\n",
      "iteration: 179 loss: 0.28956646\n",
      "iteration: 180 loss: 0.61331093\n",
      "iteration: 181 loss: 1.16764820\n",
      "iteration: 182 loss: 0.15035106\n",
      "iteration: 183 loss: 1.47958302\n",
      "iteration: 184 loss: 0.41329995\n",
      "iteration: 185 loss: 1.89438188\n",
      "iteration: 186 loss: 1.64715862\n",
      "iteration: 187 loss: 0.64384615\n",
      "iteration: 188 loss: 3.07523704\n",
      "iteration: 189 loss: 2.23998213\n",
      "iteration: 190 loss: 1.94030428\n",
      "iteration: 191 loss: 0.37834591\n",
      "iteration: 192 loss: 1.05636728\n",
      "iteration: 193 loss: 0.93996185\n",
      "iteration: 194 loss: 0.14665814\n",
      "iteration: 195 loss: 2.68747640\n",
      "iteration: 196 loss: 3.54173446\n",
      "iteration: 197 loss: 0.66131657\n",
      "iteration: 198 loss: 0.74183422\n",
      "iteration: 199 loss: 0.58934665\n",
      "epoch:  87 mean loss training: 1.12491262\n",
      "epoch:  87 mean loss validation: 1.37105227\n",
      "iteration:   0 loss: 2.50942326\n",
      "iteration:   1 loss: 0.55500990\n",
      "iteration:   2 loss: 0.80333066\n",
      "iteration:   3 loss: 0.67613631\n",
      "iteration:   4 loss: 1.76891744\n",
      "iteration:   5 loss: 1.80435097\n",
      "iteration:   6 loss: 1.27965808\n",
      "iteration:   7 loss: 0.80278462\n",
      "iteration:   8 loss: 2.73507118\n",
      "iteration:   9 loss: 3.61191344\n",
      "iteration:  10 loss: 0.58835310\n",
      "iteration:  11 loss: 0.45401844\n",
      "iteration:  12 loss: 0.63377106\n",
      "iteration:  13 loss: 1.12532914\n",
      "iteration:  14 loss: 1.03378570\n",
      "iteration:  15 loss: 0.26331547\n",
      "iteration:  16 loss: 0.67316920\n",
      "iteration:  17 loss: 0.60562974\n",
      "iteration:  18 loss: 1.11598682\n",
      "iteration:  19 loss: 1.21835923\n",
      "iteration:  20 loss: 0.43541995\n",
      "iteration:  21 loss: 1.19331622\n",
      "iteration:  22 loss: 0.41632524\n",
      "iteration:  23 loss: 0.99106240\n",
      "iteration:  24 loss: 0.27328172\n",
      "iteration:  25 loss: 1.33210588\n",
      "iteration:  26 loss: 0.95288348\n",
      "iteration:  27 loss: 2.85501122\n",
      "iteration:  28 loss: 0.22542693\n",
      "iteration:  29 loss: 0.44712320\n",
      "iteration:  30 loss: 0.55170399\n",
      "iteration:  31 loss: 0.39223894\n",
      "iteration:  32 loss: 0.41413179\n",
      "iteration:  33 loss: 0.16371854\n",
      "iteration:  34 loss: 1.29387856\n",
      "iteration:  35 loss: 1.00660074\n",
      "iteration:  36 loss: 0.11841536\n",
      "iteration:  37 loss: 0.40375367\n",
      "iteration:  38 loss: 1.12077320\n",
      "iteration:  39 loss: 3.38658237\n",
      "iteration:  40 loss: 1.51788890\n",
      "iteration:  41 loss: 0.67526692\n",
      "iteration:  42 loss: 1.01601422\n",
      "iteration:  43 loss: 2.80157447\n",
      "iteration:  44 loss: 0.29124334\n",
      "iteration:  45 loss: 0.19172263\n",
      "iteration:  46 loss: 1.38550639\n",
      "iteration:  47 loss: 0.84471226\n",
      "iteration:  48 loss: 0.85602766\n",
      "iteration:  49 loss: 0.50174415\n",
      "iteration:  50 loss: 2.23245788\n",
      "iteration:  51 loss: 0.31898248\n",
      "iteration:  52 loss: 0.68751389\n",
      "iteration:  53 loss: 0.64704353\n",
      "iteration:  54 loss: 0.36659518\n",
      "iteration:  55 loss: 4.31940269\n",
      "iteration:  56 loss: 0.93825477\n",
      "iteration:  57 loss: 2.89485025\n",
      "iteration:  58 loss: 2.52874637\n",
      "iteration:  59 loss: 1.44124949\n",
      "iteration:  60 loss: 0.35281438\n",
      "iteration:  61 loss: 0.20593292\n",
      "iteration:  62 loss: 1.26602328\n",
      "iteration:  63 loss: 0.87679601\n",
      "iteration:  64 loss: 0.12460000\n",
      "iteration:  65 loss: 0.65526247\n",
      "iteration:  66 loss: 0.15675610\n",
      "iteration:  67 loss: 0.12352239\n",
      "iteration:  68 loss: 0.13493611\n",
      "iteration:  69 loss: 0.34477007\n",
      "iteration:  70 loss: 0.37925318\n",
      "iteration:  71 loss: 0.41321433\n",
      "iteration:  72 loss: 0.52994782\n",
      "iteration:  73 loss: 2.47117209\n",
      "iteration:  74 loss: 0.28799912\n",
      "iteration:  75 loss: 0.78521943\n",
      "iteration:  76 loss: 0.79977763\n",
      "iteration:  77 loss: 0.46975940\n",
      "iteration:  78 loss: 1.66728961\n",
      "iteration:  79 loss: 0.83787102\n",
      "iteration:  80 loss: 1.28910971\n",
      "iteration:  81 loss: 0.12937881\n",
      "iteration:  82 loss: 0.34615746\n",
      "iteration:  83 loss: 0.64197350\n",
      "iteration:  84 loss: 3.02635431\n",
      "iteration:  85 loss: 0.15591316\n",
      "iteration:  86 loss: 0.47171533\n",
      "iteration:  87 loss: 2.99604392\n",
      "iteration:  88 loss: 0.18886551\n",
      "iteration:  89 loss: 2.04287791\n",
      "iteration:  90 loss: 2.08242488\n",
      "iteration:  91 loss: 2.01816130\n",
      "iteration:  92 loss: 0.17868151\n",
      "iteration:  93 loss: 1.99698055\n",
      "iteration:  94 loss: 0.85421258\n",
      "iteration:  95 loss: 0.70423561\n",
      "iteration:  96 loss: 3.56380463\n",
      "iteration:  97 loss: 0.79299515\n",
      "iteration:  98 loss: 0.33477342\n",
      "iteration:  99 loss: 0.18272401\n",
      "iteration: 100 loss: 0.28857121\n",
      "iteration: 101 loss: 0.18353681\n",
      "iteration: 102 loss: 0.65357423\n",
      "iteration: 103 loss: 2.76671076\n",
      "iteration: 104 loss: 0.51253235\n",
      "iteration: 105 loss: 0.56624740\n",
      "iteration: 106 loss: 0.34632605\n",
      "iteration: 107 loss: 0.21191503\n",
      "iteration: 108 loss: 0.81974030\n",
      "iteration: 109 loss: 0.15125483\n",
      "iteration: 110 loss: 1.96627247\n",
      "iteration: 111 loss: 3.11521840\n",
      "iteration: 112 loss: 2.58912206\n",
      "iteration: 113 loss: 0.33970597\n",
      "iteration: 114 loss: 0.51953739\n",
      "iteration: 115 loss: 1.02869141\n",
      "iteration: 116 loss: 1.95111370\n",
      "iteration: 117 loss: 0.49281666\n",
      "iteration: 118 loss: 0.36493537\n",
      "iteration: 119 loss: 0.39313436\n",
      "iteration: 120 loss: 0.25746393\n",
      "iteration: 121 loss: 0.51891941\n",
      "iteration: 122 loss: 0.10629319\n",
      "iteration: 123 loss: 2.25756478\n",
      "iteration: 124 loss: 1.37666309\n",
      "iteration: 125 loss: 1.91356194\n",
      "iteration: 126 loss: 3.45479965\n",
      "iteration: 127 loss: 0.36608285\n",
      "iteration: 128 loss: 2.10213470\n",
      "iteration: 129 loss: 0.29423478\n",
      "iteration: 130 loss: 2.57210732\n",
      "iteration: 131 loss: 2.62617826\n",
      "iteration: 132 loss: 1.39730465\n",
      "iteration: 133 loss: 0.72264177\n",
      "iteration: 134 loss: 0.35487425\n",
      "iteration: 135 loss: 1.92135870\n",
      "iteration: 136 loss: 0.83392274\n",
      "iteration: 137 loss: 0.72053760\n",
      "iteration: 138 loss: 0.91377765\n",
      "iteration: 139 loss: 1.32992887\n",
      "iteration: 140 loss: 0.97652793\n",
      "iteration: 141 loss: 0.77593899\n",
      "iteration: 142 loss: 0.81266260\n",
      "iteration: 143 loss: 0.68231928\n",
      "iteration: 144 loss: 3.63071179\n",
      "iteration: 145 loss: 1.04386127\n",
      "iteration: 146 loss: 0.21751200\n",
      "iteration: 147 loss: 0.99948859\n",
      "iteration: 148 loss: 0.74526322\n",
      "iteration: 149 loss: 1.08740830\n",
      "iteration: 150 loss: 2.42275095\n",
      "iteration: 151 loss: 1.27082670\n",
      "iteration: 152 loss: 3.24632812\n",
      "iteration: 153 loss: 3.16650009\n",
      "iteration: 154 loss: 1.58115554\n",
      "iteration: 155 loss: 2.75277424\n",
      "iteration: 156 loss: 0.24189878\n",
      "iteration: 157 loss: 0.75788838\n",
      "iteration: 158 loss: 0.74609250\n",
      "iteration: 159 loss: 0.25198776\n",
      "iteration: 160 loss: 0.99529916\n",
      "iteration: 161 loss: 0.98465431\n",
      "iteration: 162 loss: 0.25436544\n",
      "iteration: 163 loss: 0.93482351\n",
      "iteration: 164 loss: 3.00037432\n",
      "iteration: 165 loss: 1.77461255\n",
      "iteration: 166 loss: 0.12872104\n",
      "iteration: 167 loss: 0.21699773\n",
      "iteration: 168 loss: 0.86633909\n",
      "iteration: 169 loss: 0.20537594\n",
      "iteration: 170 loss: 2.70723557\n",
      "iteration: 171 loss: 0.50405329\n",
      "iteration: 172 loss: 0.60492957\n",
      "iteration: 173 loss: 0.09819697\n",
      "iteration: 174 loss: 3.41514587\n",
      "iteration: 175 loss: 2.08773899\n",
      "iteration: 176 loss: 0.93133324\n",
      "iteration: 177 loss: 0.30702433\n",
      "iteration: 178 loss: 0.24385582\n",
      "iteration: 179 loss: 0.12068772\n",
      "iteration: 180 loss: 0.21254863\n",
      "iteration: 181 loss: 1.45810807\n",
      "iteration: 182 loss: 0.15336132\n",
      "iteration: 183 loss: 1.20453465\n",
      "iteration: 184 loss: 0.57078946\n",
      "iteration: 185 loss: 2.01019716\n",
      "iteration: 186 loss: 1.00250661\n",
      "iteration: 187 loss: 0.77080262\n",
      "iteration: 188 loss: 2.89821386\n",
      "iteration: 189 loss: 2.22244692\n",
      "iteration: 190 loss: 1.83801579\n",
      "iteration: 191 loss: 0.44239524\n",
      "iteration: 192 loss: 1.10332668\n",
      "iteration: 193 loss: 0.98279840\n",
      "iteration: 194 loss: 0.25028393\n",
      "iteration: 195 loss: 2.65967965\n",
      "iteration: 196 loss: 3.62673783\n",
      "iteration: 197 loss: 0.67656326\n",
      "iteration: 198 loss: 0.70496070\n",
      "iteration: 199 loss: 0.60379392\n",
      "epoch:  88 mean loss training: 1.12027097\n",
      "epoch:  88 mean loss validation: 1.41494799\n",
      "iteration:   0 loss: 2.67798948\n",
      "iteration:   1 loss: 0.75130469\n",
      "iteration:   2 loss: 0.75489610\n",
      "iteration:   3 loss: 0.53888816\n",
      "iteration:   4 loss: 1.94712269\n",
      "iteration:   5 loss: 1.78948712\n",
      "iteration:   6 loss: 1.73004067\n",
      "iteration:   7 loss: 1.13021028\n",
      "iteration:   8 loss: 2.55888009\n",
      "iteration:   9 loss: 3.51650953\n",
      "iteration:  10 loss: 0.61082584\n",
      "iteration:  11 loss: 1.33027816\n",
      "iteration:  12 loss: 1.36025226\n",
      "iteration:  13 loss: 1.17189038\n",
      "iteration:  14 loss: 1.10998571\n",
      "iteration:  15 loss: 0.31572351\n",
      "iteration:  16 loss: 0.79898530\n",
      "iteration:  17 loss: 0.67294770\n",
      "iteration:  18 loss: 1.26837564\n",
      "iteration:  19 loss: 1.04672337\n",
      "iteration:  20 loss: 0.53983730\n",
      "iteration:  21 loss: 1.26243174\n",
      "iteration:  22 loss: 0.23611648\n",
      "iteration:  23 loss: 0.87734026\n",
      "iteration:  24 loss: 0.26498035\n",
      "iteration:  25 loss: 2.31440353\n",
      "iteration:  26 loss: 0.86384559\n",
      "iteration:  27 loss: 3.04117799\n",
      "iteration:  28 loss: 0.19619678\n",
      "iteration:  29 loss: 0.44392806\n",
      "iteration:  30 loss: 0.54599053\n",
      "iteration:  31 loss: 0.38816187\n",
      "iteration:  32 loss: 0.18373916\n",
      "iteration:  33 loss: 0.15685484\n",
      "iteration:  34 loss: 1.22885549\n",
      "iteration:  35 loss: 0.92647833\n",
      "iteration:  36 loss: 0.09559989\n",
      "iteration:  37 loss: 0.40186670\n",
      "iteration:  38 loss: 1.10980725\n",
      "iteration:  39 loss: 3.38126731\n",
      "iteration:  40 loss: 1.37153769\n",
      "iteration:  41 loss: 0.59591705\n",
      "iteration:  42 loss: 1.02256382\n",
      "iteration:  43 loss: 2.80436611\n",
      "iteration:  44 loss: 0.33929324\n",
      "iteration:  45 loss: 0.32175708\n",
      "iteration:  46 loss: 1.37438166\n",
      "iteration:  47 loss: 0.81794631\n",
      "iteration:  48 loss: 0.76475316\n",
      "iteration:  49 loss: 0.51199478\n",
      "iteration:  50 loss: 2.12490106\n",
      "iteration:  51 loss: 0.28995487\n",
      "iteration:  52 loss: 0.59344888\n",
      "iteration:  53 loss: 0.81945229\n",
      "iteration:  54 loss: 0.43825680\n",
      "iteration:  55 loss: 4.26046276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  56 loss: 0.78940952\n",
      "iteration:  57 loss: 2.87167048\n",
      "iteration:  58 loss: 2.55749559\n",
      "iteration:  59 loss: 1.51332402\n",
      "iteration:  60 loss: 0.23002422\n",
      "iteration:  61 loss: 0.18787463\n",
      "iteration:  62 loss: 0.47703376\n",
      "iteration:  63 loss: 0.64956415\n",
      "iteration:  64 loss: 0.13245979\n",
      "iteration:  65 loss: 0.68206435\n",
      "iteration:  66 loss: 0.10550122\n",
      "iteration:  67 loss: 0.16103171\n",
      "iteration:  68 loss: 0.13079663\n",
      "iteration:  69 loss: 0.20762828\n",
      "iteration:  70 loss: 0.18737972\n",
      "iteration:  71 loss: 0.45922729\n",
      "iteration:  72 loss: 0.61227673\n",
      "iteration:  73 loss: 2.48801661\n",
      "iteration:  74 loss: 0.50981849\n",
      "iteration:  75 loss: 0.73380506\n",
      "iteration:  76 loss: 0.69643670\n",
      "iteration:  77 loss: 0.43379939\n",
      "iteration:  78 loss: 2.12787342\n",
      "iteration:  79 loss: 0.59776223\n",
      "iteration:  80 loss: 1.55910015\n",
      "iteration:  81 loss: 0.08995381\n",
      "iteration:  82 loss: 0.69806939\n",
      "iteration:  83 loss: 0.11127321\n",
      "iteration:  84 loss: 3.22040582\n",
      "iteration:  85 loss: 0.15194322\n",
      "iteration:  86 loss: 0.48860118\n",
      "iteration:  87 loss: 3.14597130\n",
      "iteration:  88 loss: 0.15311249\n",
      "iteration:  89 loss: 2.56935358\n",
      "iteration:  90 loss: 1.64414287\n",
      "iteration:  91 loss: 0.36194530\n",
      "iteration:  92 loss: 0.27155459\n",
      "iteration:  93 loss: 2.92785573\n",
      "iteration:  94 loss: 1.90805089\n",
      "iteration:  95 loss: 0.70075947\n",
      "iteration:  96 loss: 4.13946199\n",
      "iteration:  97 loss: 0.37524509\n",
      "iteration:  98 loss: 0.66478187\n",
      "iteration:  99 loss: 0.51711524\n",
      "iteration: 100 loss: 0.44519618\n",
      "iteration: 101 loss: 0.43422973\n",
      "iteration: 102 loss: 1.45571506\n",
      "iteration: 103 loss: 2.72818732\n",
      "iteration: 104 loss: 0.42420554\n",
      "iteration: 105 loss: 0.89050633\n",
      "iteration: 106 loss: 0.34667197\n",
      "iteration: 107 loss: 0.78583568\n",
      "iteration: 108 loss: 3.03853512\n",
      "iteration: 109 loss: 0.93797952\n",
      "iteration: 110 loss: 2.18907785\n",
      "iteration: 111 loss: 3.12271166\n",
      "iteration: 112 loss: 2.44951320\n",
      "iteration: 113 loss: 0.34275675\n",
      "iteration: 114 loss: 0.80392748\n",
      "iteration: 115 loss: 1.04163539\n",
      "iteration: 116 loss: 2.34464502\n",
      "iteration: 117 loss: 0.97092313\n",
      "iteration: 118 loss: 0.39058483\n",
      "iteration: 119 loss: 0.70650423\n",
      "iteration: 120 loss: 0.21686520\n",
      "iteration: 121 loss: 0.87082821\n",
      "iteration: 122 loss: 0.13072553\n",
      "iteration: 123 loss: 2.31896734\n",
      "iteration: 124 loss: 0.94919938\n",
      "iteration: 125 loss: 1.17059004\n",
      "iteration: 126 loss: 3.23265076\n",
      "iteration: 127 loss: 0.23524798\n",
      "iteration: 128 loss: 1.93208373\n",
      "iteration: 129 loss: 0.65223068\n",
      "iteration: 130 loss: 2.60694122\n",
      "iteration: 131 loss: 2.75597930\n",
      "iteration: 132 loss: 1.34886312\n",
      "iteration: 133 loss: 0.47516620\n",
      "iteration: 134 loss: 0.40176558\n",
      "iteration: 135 loss: 0.71318978\n",
      "iteration: 136 loss: 0.92614073\n",
      "iteration: 137 loss: 0.74282408\n",
      "iteration: 138 loss: 0.80349219\n",
      "iteration: 139 loss: 1.16213381\n",
      "iteration: 140 loss: 1.10604978\n",
      "iteration: 141 loss: 0.74417818\n",
      "iteration: 142 loss: 0.74760026\n",
      "iteration: 143 loss: 2.05159879\n",
      "iteration: 144 loss: 3.42261648\n",
      "iteration: 145 loss: 1.44719648\n",
      "iteration: 146 loss: 0.30656150\n",
      "iteration: 147 loss: 0.77424937\n",
      "iteration: 148 loss: 0.78730118\n",
      "iteration: 149 loss: 1.08724809\n",
      "iteration: 150 loss: 1.66314435\n",
      "iteration: 151 loss: 1.38044190\n",
      "iteration: 152 loss: 3.08119678\n",
      "iteration: 153 loss: 3.14600706\n",
      "iteration: 154 loss: 1.25509429\n",
      "iteration: 155 loss: 2.49243855\n",
      "iteration: 156 loss: 0.30829719\n",
      "iteration: 157 loss: 0.80215394\n",
      "iteration: 158 loss: 0.87825471\n",
      "iteration: 159 loss: 0.30495217\n",
      "iteration: 160 loss: 0.99814099\n",
      "iteration: 161 loss: 0.80508333\n",
      "iteration: 162 loss: 0.24402286\n",
      "iteration: 163 loss: 1.50440884\n",
      "iteration: 164 loss: 3.00678897\n",
      "iteration: 165 loss: 2.65007687\n",
      "iteration: 166 loss: 0.12388042\n",
      "iteration: 167 loss: 0.19356714\n",
      "iteration: 168 loss: 0.81814218\n",
      "iteration: 169 loss: 0.22465345\n",
      "iteration: 170 loss: 2.71275330\n",
      "iteration: 171 loss: 0.51794249\n",
      "iteration: 172 loss: 0.59561157\n",
      "iteration: 173 loss: 0.10819038\n",
      "iteration: 174 loss: 3.45340395\n",
      "iteration: 175 loss: 2.11927152\n",
      "iteration: 176 loss: 1.69643080\n",
      "iteration: 177 loss: 0.31082261\n",
      "iteration: 178 loss: 0.22998704\n",
      "iteration: 179 loss: 0.12080436\n",
      "iteration: 180 loss: 0.21313521\n",
      "iteration: 181 loss: 1.29623687\n",
      "iteration: 182 loss: 0.13489729\n",
      "iteration: 183 loss: 2.09074497\n",
      "iteration: 184 loss: 0.37869149\n",
      "iteration: 185 loss: 2.27758574\n",
      "iteration: 186 loss: 1.86205959\n",
      "iteration: 187 loss: 0.57051593\n",
      "iteration: 188 loss: 3.45842004\n",
      "iteration: 189 loss: 2.22718430\n",
      "iteration: 190 loss: 2.03444290\n",
      "iteration: 191 loss: 0.26613414\n",
      "iteration: 192 loss: 0.82764649\n",
      "iteration: 193 loss: 1.11191654\n",
      "iteration: 194 loss: 0.14969578\n",
      "iteration: 195 loss: 2.62821841\n",
      "iteration: 196 loss: 3.60424042\n",
      "iteration: 197 loss: 0.68493170\n",
      "iteration: 198 loss: 0.90317494\n",
      "iteration: 199 loss: 0.64029223\n",
      "epoch:  89 mean loss training: 1.18154049\n",
      "epoch:  89 mean loss validation: 1.39522123\n",
      "iteration:   0 loss: 2.62604237\n",
      "iteration:   1 loss: 0.55128932\n",
      "iteration:   2 loss: 0.95487851\n",
      "iteration:   3 loss: 0.71642369\n",
      "iteration:   4 loss: 1.27800703\n",
      "iteration:   5 loss: 1.40458643\n",
      "iteration:   6 loss: 1.18781364\n",
      "iteration:   7 loss: 1.03323042\n",
      "iteration:   8 loss: 2.57974195\n",
      "iteration:   9 loss: 3.61622381\n",
      "iteration:  10 loss: 0.45258081\n",
      "iteration:  11 loss: 0.38792375\n",
      "iteration:  12 loss: 0.61838514\n",
      "iteration:  13 loss: 1.23687685\n",
      "iteration:  14 loss: 1.01710343\n",
      "iteration:  15 loss: 0.21340640\n",
      "iteration:  16 loss: 0.65913373\n",
      "iteration:  17 loss: 0.61127752\n",
      "iteration:  18 loss: 1.00068927\n",
      "iteration:  19 loss: 1.20957327\n",
      "iteration:  20 loss: 0.25247386\n",
      "iteration:  21 loss: 1.00498676\n",
      "iteration:  22 loss: 0.17563511\n",
      "iteration:  23 loss: 0.82099247\n",
      "iteration:  24 loss: 0.25600502\n",
      "iteration:  25 loss: 2.70300579\n",
      "iteration:  26 loss: 0.82229900\n",
      "iteration:  27 loss: 3.20176196\n",
      "iteration:  28 loss: 0.18111417\n",
      "iteration:  29 loss: 0.25914407\n",
      "iteration:  30 loss: 0.54325992\n",
      "iteration:  31 loss: 0.46989521\n",
      "iteration:  32 loss: 0.17399962\n",
      "iteration:  33 loss: 0.15377598\n",
      "iteration:  34 loss: 1.16094410\n",
      "iteration:  35 loss: 0.95162702\n",
      "iteration:  36 loss: 0.08661083\n",
      "iteration:  37 loss: 0.40582106\n",
      "iteration:  38 loss: 1.10295475\n",
      "iteration:  39 loss: 3.40450978\n",
      "iteration:  40 loss: 1.18474078\n",
      "iteration:  41 loss: 1.16247964\n",
      "iteration:  42 loss: 0.91307706\n",
      "iteration:  43 loss: 2.78510022\n",
      "iteration:  44 loss: 0.38446599\n",
      "iteration:  45 loss: 0.30316079\n",
      "iteration:  46 loss: 1.41601825\n",
      "iteration:  47 loss: 0.86125422\n",
      "iteration:  48 loss: 0.80915916\n",
      "iteration:  49 loss: 0.50535303\n",
      "iteration:  50 loss: 1.98743451\n",
      "iteration:  51 loss: 0.28069791\n",
      "iteration:  52 loss: 0.37842205\n",
      "iteration:  53 loss: 0.39114803\n",
      "iteration:  54 loss: 0.38131216\n",
      "iteration:  55 loss: 4.31030512\n",
      "iteration:  56 loss: 0.72780085\n",
      "iteration:  57 loss: 2.86307311\n",
      "iteration:  58 loss: 2.48894286\n",
      "iteration:  59 loss: 1.43836653\n",
      "iteration:  60 loss: 0.20169453\n",
      "iteration:  61 loss: 0.21801233\n",
      "iteration:  62 loss: 0.46046382\n",
      "iteration:  63 loss: 0.56730562\n",
      "iteration:  64 loss: 0.13367830\n",
      "iteration:  65 loss: 0.62779111\n",
      "iteration:  66 loss: 0.11311372\n",
      "iteration:  67 loss: 0.14928423\n",
      "iteration:  68 loss: 0.15095034\n",
      "iteration:  69 loss: 0.33125526\n",
      "iteration:  70 loss: 0.20311961\n",
      "iteration:  71 loss: 0.46472120\n",
      "iteration:  72 loss: 0.58040363\n",
      "iteration:  73 loss: 2.83307910\n",
      "iteration:  74 loss: 0.30231383\n",
      "iteration:  75 loss: 0.76164198\n",
      "iteration:  76 loss: 0.72685391\n",
      "iteration:  77 loss: 0.38567021\n",
      "iteration:  78 loss: 2.60309434\n",
      "iteration:  79 loss: 0.62307441\n",
      "iteration:  80 loss: 1.32068825\n",
      "iteration:  81 loss: 0.08849724\n",
      "iteration:  82 loss: 0.32352346\n",
      "iteration:  83 loss: 0.18718676\n",
      "iteration:  84 loss: 3.54118776\n",
      "iteration:  85 loss: 0.14378829\n",
      "iteration:  86 loss: 0.44132173\n",
      "iteration:  87 loss: 3.22650933\n",
      "iteration:  88 loss: 0.18398882\n",
      "iteration:  89 loss: 2.28915405\n",
      "iteration:  90 loss: 1.42400587\n",
      "iteration:  91 loss: 0.29253528\n",
      "iteration:  92 loss: 0.17724010\n",
      "iteration:  93 loss: 1.29115534\n",
      "iteration:  94 loss: 1.65965128\n",
      "iteration:  95 loss: 0.69869351\n",
      "iteration:  96 loss: 3.45130157\n",
      "iteration:  97 loss: 0.39941677\n",
      "iteration:  98 loss: 0.68670112\n",
      "iteration:  99 loss: 0.25064877\n",
      "iteration: 100 loss: 0.38433146\n",
      "iteration: 101 loss: 0.42083567\n",
      "iteration: 102 loss: 1.14782679\n",
      "iteration: 103 loss: 2.78863835\n",
      "iteration: 104 loss: 0.59098530\n",
      "iteration: 105 loss: 1.73461354\n",
      "iteration: 106 loss: 0.74612749\n",
      "iteration: 107 loss: 0.41234088\n",
      "iteration: 108 loss: 2.95055914\n",
      "iteration: 109 loss: 1.13172328\n",
      "iteration: 110 loss: 1.93977225\n",
      "iteration: 111 loss: 3.13426542\n",
      "iteration: 112 loss: 2.68741512\n",
      "iteration: 113 loss: 0.67072672\n",
      "iteration: 114 loss: 0.96366596\n",
      "iteration: 115 loss: 1.23608041\n",
      "iteration: 116 loss: 2.54347515\n",
      "iteration: 117 loss: 0.46705380\n",
      "iteration: 118 loss: 0.19988452\n",
      "iteration: 119 loss: 0.51651824\n",
      "iteration: 120 loss: 0.54261565\n",
      "iteration: 121 loss: 0.69704986\n",
      "iteration: 122 loss: 0.12809412\n",
      "iteration: 123 loss: 2.23044753\n",
      "iteration: 124 loss: 0.88329953\n",
      "iteration: 125 loss: 1.35890746\n",
      "iteration: 126 loss: 2.97531700\n",
      "iteration: 127 loss: 0.29157031\n",
      "iteration: 128 loss: 1.69605350\n",
      "iteration: 129 loss: 0.17071994\n",
      "iteration: 130 loss: 2.69644570\n",
      "iteration: 131 loss: 2.52078819\n",
      "iteration: 132 loss: 0.78452623\n",
      "iteration: 133 loss: 0.95279467\n",
      "iteration: 134 loss: 0.36530891\n",
      "iteration: 135 loss: 2.09355855\n",
      "iteration: 136 loss: 1.02470684\n",
      "iteration: 137 loss: 1.35935473\n",
      "iteration: 138 loss: 1.31694365\n",
      "iteration: 139 loss: 1.25360858\n",
      "iteration: 140 loss: 0.78976840\n",
      "iteration: 141 loss: 0.79054558\n",
      "iteration: 142 loss: 0.85508007\n",
      "iteration: 143 loss: 0.87748253\n",
      "iteration: 144 loss: 3.63726568\n",
      "iteration: 145 loss: 1.28718317\n",
      "iteration: 146 loss: 0.50899726\n",
      "iteration: 147 loss: 0.90625226\n",
      "iteration: 148 loss: 1.02298534\n",
      "iteration: 149 loss: 0.62481874\n",
      "iteration: 150 loss: 1.69471073\n",
      "iteration: 151 loss: 2.19248605\n",
      "iteration: 152 loss: 2.84856415\n",
      "iteration: 153 loss: 2.91474438\n",
      "iteration: 154 loss: 1.30779207\n",
      "iteration: 155 loss: 2.65173483\n",
      "iteration: 156 loss: 0.42093104\n",
      "iteration: 157 loss: 0.77499127\n",
      "iteration: 158 loss: 0.71754766\n",
      "iteration: 159 loss: 0.32966098\n",
      "iteration: 160 loss: 1.00375259\n",
      "iteration: 161 loss: 0.97668463\n",
      "iteration: 162 loss: 0.26359233\n",
      "iteration: 163 loss: 2.95329452\n",
      "iteration: 164 loss: 3.12835979\n",
      "iteration: 165 loss: 1.25137913\n",
      "iteration: 166 loss: 0.21079816\n",
      "iteration: 167 loss: 0.17267647\n",
      "iteration: 168 loss: 1.34763193\n",
      "iteration: 169 loss: 0.21825157\n",
      "iteration: 170 loss: 2.94749880\n",
      "iteration: 171 loss: 0.50394326\n",
      "iteration: 172 loss: 0.58550769\n",
      "iteration: 173 loss: 0.11389232\n",
      "iteration: 174 loss: 3.66554475\n",
      "iteration: 175 loss: 2.30945134\n",
      "iteration: 176 loss: 0.61074919\n",
      "iteration: 177 loss: 0.70355958\n",
      "iteration: 178 loss: 0.26518869\n",
      "iteration: 179 loss: 0.30240643\n",
      "iteration: 180 loss: 0.58634597\n",
      "iteration: 181 loss: 1.56722832\n",
      "iteration: 182 loss: 0.15204147\n",
      "iteration: 183 loss: 2.09884858\n",
      "iteration: 184 loss: 0.56230837\n",
      "iteration: 185 loss: 1.71986532\n",
      "iteration: 186 loss: 1.66536641\n",
      "iteration: 187 loss: 0.92732298\n",
      "iteration: 188 loss: 3.81801057\n",
      "iteration: 189 loss: 2.15083265\n",
      "iteration: 190 loss: 2.28271890\n",
      "iteration: 191 loss: 0.29127872\n",
      "iteration: 192 loss: 0.87795144\n",
      "iteration: 193 loss: 1.04018784\n",
      "iteration: 194 loss: 0.14083341\n",
      "iteration: 195 loss: 2.59581161\n",
      "iteration: 196 loss: 3.64220405\n",
      "iteration: 197 loss: 0.66024411\n",
      "iteration: 198 loss: 0.89778674\n",
      "iteration: 199 loss: 0.66664243\n",
      "epoch:  90 mean loss training: 1.16306758\n",
      "epoch:  90 mean loss validation: 1.43560529\n",
      "iteration:   0 loss: 2.58032084\n",
      "iteration:   1 loss: 0.56142223\n",
      "iteration:   2 loss: 0.84205049\n",
      "iteration:   3 loss: 1.02313983\n",
      "iteration:   4 loss: 1.67575061\n",
      "iteration:   5 loss: 1.43410814\n",
      "iteration:   6 loss: 1.43836093\n",
      "iteration:   7 loss: 0.77600056\n",
      "iteration:   8 loss: 2.65144420\n",
      "iteration:   9 loss: 3.56267357\n",
      "iteration:  10 loss: 0.70512223\n",
      "iteration:  11 loss: 0.59270924\n",
      "iteration:  12 loss: 0.61961675\n",
      "iteration:  13 loss: 1.33556986\n",
      "iteration:  14 loss: 1.02893412\n",
      "iteration:  15 loss: 0.20441623\n",
      "iteration:  16 loss: 0.55224979\n",
      "iteration:  17 loss: 0.60776502\n",
      "iteration:  18 loss: 0.96421295\n",
      "iteration:  19 loss: 1.24054432\n",
      "iteration:  20 loss: 0.48000512\n",
      "iteration:  21 loss: 1.04968250\n",
      "iteration:  22 loss: 0.59797561\n",
      "iteration:  23 loss: 1.00113750\n",
      "iteration:  24 loss: 0.32087776\n",
      "iteration:  25 loss: 0.94752902\n",
      "iteration:  26 loss: 0.93083185\n",
      "iteration:  27 loss: 2.37105346\n",
      "iteration:  28 loss: 0.18312578\n",
      "iteration:  29 loss: 0.27597842\n",
      "iteration:  30 loss: 0.54238796\n",
      "iteration:  31 loss: 0.37847632\n",
      "iteration:  32 loss: 0.45125666\n",
      "iteration:  33 loss: 0.16868810\n",
      "iteration:  34 loss: 1.05715847\n",
      "iteration:  35 loss: 0.99956185\n",
      "iteration:  36 loss: 0.10424476\n",
      "iteration:  37 loss: 0.41189542\n",
      "iteration:  38 loss: 1.11922586\n",
      "iteration:  39 loss: 3.38525438\n",
      "iteration:  40 loss: 1.39498603\n",
      "iteration:  41 loss: 0.65903276\n",
      "iteration:  42 loss: 0.96176881\n",
      "iteration:  43 loss: 2.77440071\n",
      "iteration:  44 loss: 0.44220993\n",
      "iteration:  45 loss: 0.18115729\n",
      "iteration:  46 loss: 1.84621084\n",
      "iteration:  47 loss: 0.93947190\n",
      "iteration:  48 loss: 0.81631649\n",
      "iteration:  49 loss: 0.50093347\n",
      "iteration:  50 loss: 2.28311253\n",
      "iteration:  51 loss: 0.33968619\n",
      "iteration:  52 loss: 0.68584365\n",
      "iteration:  53 loss: 0.84322697\n",
      "iteration:  54 loss: 0.43643644\n",
      "iteration:  55 loss: 4.31496239\n",
      "iteration:  56 loss: 0.90182567\n",
      "iteration:  57 loss: 2.91957927\n",
      "iteration:  58 loss: 2.52592087\n",
      "iteration:  59 loss: 1.66719890\n",
      "iteration:  60 loss: 0.33874419\n",
      "iteration:  61 loss: 0.23648584\n",
      "iteration:  62 loss: 0.41447362\n",
      "iteration:  63 loss: 0.87574404\n",
      "iteration:  64 loss: 0.12824506\n",
      "iteration:  65 loss: 0.68605590\n",
      "iteration:  66 loss: 0.13432522\n",
      "iteration:  67 loss: 0.13756789\n",
      "iteration:  68 loss: 0.15953416\n",
      "iteration:  69 loss: 0.52758664\n",
      "iteration:  70 loss: 0.41443425\n",
      "iteration:  71 loss: 0.49873471\n",
      "iteration:  72 loss: 0.61030489\n",
      "iteration:  73 loss: 2.47196627\n",
      "iteration:  74 loss: 0.26923457\n",
      "iteration:  75 loss: 0.74801314\n",
      "iteration:  76 loss: 0.76786143\n",
      "iteration:  77 loss: 0.40226033\n",
      "iteration:  78 loss: 1.55110121\n",
      "iteration:  79 loss: 0.61253452\n",
      "iteration:  80 loss: 1.26666534\n",
      "iteration:  81 loss: 0.14296846\n",
      "iteration:  82 loss: 0.34540823\n",
      "iteration:  83 loss: 0.35797113\n",
      "iteration:  84 loss: 3.00317001\n",
      "iteration:  85 loss: 0.16885743\n",
      "iteration:  86 loss: 0.32764047\n",
      "iteration:  87 loss: 2.99315953\n",
      "iteration:  88 loss: 0.21654493\n",
      "iteration:  89 loss: 2.37597990\n",
      "iteration:  90 loss: 2.11848116\n",
      "iteration:  91 loss: 2.08050513\n",
      "iteration:  92 loss: 0.26765662\n",
      "iteration:  93 loss: 1.87760365\n",
      "iteration:  94 loss: 1.41228426\n",
      "iteration:  95 loss: 0.78904468\n",
      "iteration:  96 loss: 3.97271633\n",
      "iteration:  97 loss: 0.92093062\n",
      "iteration:  98 loss: 0.35041487\n",
      "iteration:  99 loss: 0.22713561\n",
      "iteration: 100 loss: 0.33075449\n",
      "iteration: 101 loss: 0.18893740\n",
      "iteration: 102 loss: 0.70153606\n",
      "iteration: 103 loss: 2.78264332\n",
      "iteration: 104 loss: 0.46895063\n",
      "iteration: 105 loss: 0.64360034\n",
      "iteration: 106 loss: 0.28975454\n",
      "iteration: 107 loss: 0.67554474\n",
      "iteration: 108 loss: 0.83375943\n",
      "iteration: 109 loss: 0.22656120\n",
      "iteration: 110 loss: 2.22430587\n",
      "iteration: 111 loss: 3.14103103\n",
      "iteration: 112 loss: 2.53488588\n",
      "iteration: 113 loss: 0.37198958\n",
      "iteration: 114 loss: 0.68044597\n",
      "iteration: 115 loss: 1.09217882\n",
      "iteration: 116 loss: 2.20137835\n",
      "iteration: 117 loss: 0.39404982\n",
      "iteration: 118 loss: 0.27701497\n",
      "iteration: 119 loss: 0.42637250\n",
      "iteration: 120 loss: 0.27805844\n",
      "iteration: 121 loss: 0.63765424\n",
      "iteration: 122 loss: 0.18092027\n",
      "iteration: 123 loss: 2.21964669\n",
      "iteration: 124 loss: 1.37550068\n",
      "iteration: 125 loss: 2.18346262\n",
      "iteration: 126 loss: 3.49394512\n",
      "iteration: 127 loss: 0.24299130\n",
      "iteration: 128 loss: 1.70792711\n",
      "iteration: 129 loss: 0.16654870\n",
      "iteration: 130 loss: 2.53067851\n",
      "iteration: 131 loss: 2.80784178\n",
      "iteration: 132 loss: 1.18937159\n",
      "iteration: 133 loss: 0.59033138\n",
      "iteration: 134 loss: 0.37588283\n",
      "iteration: 135 loss: 2.11680412\n",
      "iteration: 136 loss: 0.96152890\n",
      "iteration: 137 loss: 0.89408231\n",
      "iteration: 138 loss: 1.15162575\n",
      "iteration: 139 loss: 1.06988955\n",
      "iteration: 140 loss: 0.90817237\n",
      "iteration: 141 loss: 0.74722648\n",
      "iteration: 142 loss: 0.84246290\n",
      "iteration: 143 loss: 1.30018032\n",
      "iteration: 144 loss: 3.52398896\n",
      "iteration: 145 loss: 1.28349268\n",
      "iteration: 146 loss: 0.43718076\n",
      "iteration: 147 loss: 0.99150723\n",
      "iteration: 148 loss: 0.91309375\n",
      "iteration: 149 loss: 0.68996799\n",
      "iteration: 150 loss: 2.07888436\n",
      "iteration: 151 loss: 2.20539165\n",
      "iteration: 152 loss: 3.16472578\n",
      "iteration: 153 loss: 3.01238322\n",
      "iteration: 154 loss: 1.61490107\n",
      "iteration: 155 loss: 2.74869704\n",
      "iteration: 156 loss: 0.56241506\n",
      "iteration: 157 loss: 0.80021083\n",
      "iteration: 158 loss: 0.61233079\n",
      "iteration: 159 loss: 0.44057482\n",
      "iteration: 160 loss: 0.97261667\n",
      "iteration: 161 loss: 0.80725288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 162 loss: 0.26929632\n",
      "iteration: 163 loss: 1.57259047\n",
      "iteration: 164 loss: 3.13194370\n",
      "iteration: 165 loss: 1.16890728\n",
      "iteration: 166 loss: 0.18211128\n",
      "iteration: 167 loss: 0.18433209\n",
      "iteration: 168 loss: 1.26861691\n",
      "iteration: 169 loss: 0.14261039\n",
      "iteration: 170 loss: 2.92241836\n",
      "iteration: 171 loss: 0.45885924\n",
      "iteration: 172 loss: 0.49258769\n",
      "iteration: 173 loss: 0.07577060\n",
      "iteration: 174 loss: 3.63301134\n",
      "iteration: 175 loss: 2.13803673\n",
      "iteration: 176 loss: 0.87303323\n",
      "iteration: 177 loss: 0.50187820\n",
      "iteration: 178 loss: 0.28104115\n",
      "iteration: 179 loss: 0.28049478\n",
      "iteration: 180 loss: 0.88853806\n",
      "iteration: 181 loss: 1.28556049\n",
      "iteration: 182 loss: 0.17138757\n",
      "iteration: 183 loss: 2.67311263\n",
      "iteration: 184 loss: 0.66142946\n",
      "iteration: 185 loss: 1.57743120\n",
      "iteration: 186 loss: 1.44787800\n",
      "iteration: 187 loss: 0.65738237\n",
      "iteration: 188 loss: 3.42364049\n",
      "iteration: 189 loss: 2.21962523\n",
      "iteration: 190 loss: 1.90735734\n",
      "iteration: 191 loss: 0.32134849\n",
      "iteration: 192 loss: 1.21575785\n",
      "iteration: 193 loss: 0.73911273\n",
      "iteration: 194 loss: 0.38194305\n",
      "iteration: 195 loss: 2.76100469\n",
      "iteration: 196 loss: 3.52252913\n",
      "iteration: 197 loss: 0.65773302\n",
      "iteration: 198 loss: 0.97862285\n",
      "iteration: 199 loss: 0.65323973\n",
      "epoch:  91 mean loss training: 1.15423787\n",
      "epoch:  91 mean loss validation: 1.45190561\n",
      "iteration:   0 loss: 3.18188977\n",
      "iteration:   1 loss: 0.69738603\n",
      "iteration:   2 loss: 1.21020472\n",
      "iteration:   3 loss: 0.96833098\n",
      "iteration:   4 loss: 1.41718495\n",
      "iteration:   5 loss: 1.39489412\n",
      "iteration:   6 loss: 1.36832416\n",
      "iteration:   7 loss: 0.69729936\n",
      "iteration:   8 loss: 2.61233592\n",
      "iteration:   9 loss: 3.73706913\n",
      "iteration:  10 loss: 0.57866538\n",
      "iteration:  11 loss: 0.50787371\n",
      "iteration:  12 loss: 0.67505676\n",
      "iteration:  13 loss: 1.34230435\n",
      "iteration:  14 loss: 1.04053819\n",
      "iteration:  15 loss: 0.19746427\n",
      "iteration:  16 loss: 0.57602412\n",
      "iteration:  17 loss: 0.63584721\n",
      "iteration:  18 loss: 1.06571484\n",
      "iteration:  19 loss: 1.04398286\n",
      "iteration:  20 loss: 0.51497233\n",
      "iteration:  21 loss: 0.83115345\n",
      "iteration:  22 loss: 0.55616045\n",
      "iteration:  23 loss: 0.94432425\n",
      "iteration:  24 loss: 0.31022131\n",
      "iteration:  25 loss: 0.71609002\n",
      "iteration:  26 loss: 0.86485589\n",
      "iteration:  27 loss: 2.11487532\n",
      "iteration:  28 loss: 0.25871500\n",
      "iteration:  29 loss: 0.23958975\n",
      "iteration:  30 loss: 0.54589635\n",
      "iteration:  31 loss: 0.40522727\n",
      "iteration:  32 loss: 0.46157622\n",
      "iteration:  33 loss: 0.15821095\n",
      "iteration:  34 loss: 1.16001999\n",
      "iteration:  35 loss: 1.03832340\n",
      "iteration:  36 loss: 0.10084154\n",
      "iteration:  37 loss: 0.38568035\n",
      "iteration:  38 loss: 1.11669743\n",
      "iteration:  39 loss: 3.39674377\n",
      "iteration:  40 loss: 1.59085000\n",
      "iteration:  41 loss: 0.63638747\n",
      "iteration:  42 loss: 0.93789214\n",
      "iteration:  43 loss: 2.76572061\n",
      "iteration:  44 loss: 0.67563349\n",
      "iteration:  45 loss: 0.15318279\n",
      "iteration:  46 loss: 1.64207506\n",
      "iteration:  47 loss: 0.74613547\n",
      "iteration:  48 loss: 0.85808337\n",
      "iteration:  49 loss: 0.49723491\n",
      "iteration:  50 loss: 2.33921504\n",
      "iteration:  51 loss: 0.32083836\n",
      "iteration:  52 loss: 0.67235142\n",
      "iteration:  53 loss: 0.79891354\n",
      "iteration:  54 loss: 0.44820419\n",
      "iteration:  55 loss: 4.36388254\n",
      "iteration:  56 loss: 0.90265572\n",
      "iteration:  57 loss: 2.92867565\n",
      "iteration:  58 loss: 2.37566543\n",
      "iteration:  59 loss: 1.53516543\n",
      "iteration:  60 loss: 0.25974548\n",
      "iteration:  61 loss: 0.21274947\n",
      "iteration:  62 loss: 0.34145576\n",
      "iteration:  63 loss: 0.88764566\n",
      "iteration:  64 loss: 0.12007982\n",
      "iteration:  65 loss: 0.64795566\n",
      "iteration:  66 loss: 0.14029354\n",
      "iteration:  67 loss: 0.10560890\n",
      "iteration:  68 loss: 0.14560716\n",
      "iteration:  69 loss: 0.51406366\n",
      "iteration:  70 loss: 0.59262234\n",
      "iteration:  71 loss: 0.75929803\n",
      "iteration:  72 loss: 0.56762147\n",
      "iteration:  73 loss: 2.38789535\n",
      "iteration:  74 loss: 0.19427860\n",
      "iteration:  75 loss: 0.48986694\n",
      "iteration:  76 loss: 0.72160685\n",
      "iteration:  77 loss: 0.59213597\n",
      "iteration:  78 loss: 1.45268321\n",
      "iteration:  79 loss: 0.61827743\n",
      "iteration:  80 loss: 1.12104273\n",
      "iteration:  81 loss: 0.12904443\n",
      "iteration:  82 loss: 0.45219120\n",
      "iteration:  83 loss: 0.24519181\n",
      "iteration:  84 loss: 2.91618538\n",
      "iteration:  85 loss: 0.14412047\n",
      "iteration:  86 loss: 0.47842550\n",
      "iteration:  87 loss: 2.90999770\n",
      "iteration:  88 loss: 0.20100623\n",
      "iteration:  89 loss: 2.59285259\n",
      "iteration:  90 loss: 2.75172877\n",
      "iteration:  91 loss: 2.02863860\n",
      "iteration:  92 loss: 0.20505889\n",
      "iteration:  93 loss: 1.92796421\n",
      "iteration:  94 loss: 1.54133201\n",
      "iteration:  95 loss: 0.69960010\n",
      "iteration:  96 loss: 3.74275517\n",
      "iteration:  97 loss: 1.27526700\n",
      "iteration:  98 loss: 0.26805130\n",
      "iteration:  99 loss: 0.21597090\n",
      "iteration: 100 loss: 0.26847836\n",
      "iteration: 101 loss: 0.18206415\n",
      "iteration: 102 loss: 0.82226187\n",
      "iteration: 103 loss: 2.98518538\n",
      "iteration: 104 loss: 0.27044842\n",
      "iteration: 105 loss: 0.67955977\n",
      "iteration: 106 loss: 0.45329735\n",
      "iteration: 107 loss: 0.19515017\n",
      "iteration: 108 loss: 0.79724550\n",
      "iteration: 109 loss: 0.23820008\n",
      "iteration: 110 loss: 2.56641722\n",
      "iteration: 111 loss: 3.11891866\n",
      "iteration: 112 loss: 2.49620152\n",
      "iteration: 113 loss: 0.41200793\n",
      "iteration: 114 loss: 0.66328049\n",
      "iteration: 115 loss: 0.89796007\n",
      "iteration: 116 loss: 1.90437353\n",
      "iteration: 117 loss: 0.60272300\n",
      "iteration: 118 loss: 0.27848777\n",
      "iteration: 119 loss: 0.62049472\n",
      "iteration: 120 loss: 0.32131082\n",
      "iteration: 121 loss: 0.58199668\n",
      "iteration: 122 loss: 0.14073293\n",
      "iteration: 123 loss: 2.30957103\n",
      "iteration: 124 loss: 1.19757199\n",
      "iteration: 125 loss: 1.07849121\n",
      "iteration: 126 loss: 3.34803414\n",
      "iteration: 127 loss: 0.28427839\n",
      "iteration: 128 loss: 1.90476477\n",
      "iteration: 129 loss: 0.22352813\n",
      "iteration: 130 loss: 2.61990213\n",
      "iteration: 131 loss: 2.67417789\n",
      "iteration: 132 loss: 1.28122222\n",
      "iteration: 133 loss: 0.35468799\n",
      "iteration: 134 loss: 0.39946386\n",
      "iteration: 135 loss: 2.08942413\n",
      "iteration: 136 loss: 0.89919180\n",
      "iteration: 137 loss: 0.91165155\n",
      "iteration: 138 loss: 0.86712730\n",
      "iteration: 139 loss: 1.08419919\n",
      "iteration: 140 loss: 1.03294194\n",
      "iteration: 141 loss: 0.68710649\n",
      "iteration: 142 loss: 0.87867516\n",
      "iteration: 143 loss: 1.67802167\n",
      "iteration: 144 loss: 3.41502953\n",
      "iteration: 145 loss: 1.03844345\n",
      "iteration: 146 loss: 0.31199482\n",
      "iteration: 147 loss: 0.80457491\n",
      "iteration: 148 loss: 0.86139876\n",
      "iteration: 149 loss: 0.68234324\n",
      "iteration: 150 loss: 1.73830843\n",
      "iteration: 151 loss: 1.96726465\n",
      "iteration: 152 loss: 3.24003744\n",
      "iteration: 153 loss: 3.06772947\n",
      "iteration: 154 loss: 1.44124007\n",
      "iteration: 155 loss: 2.59719110\n",
      "iteration: 156 loss: 0.50520611\n",
      "iteration: 157 loss: 0.81938529\n",
      "iteration: 158 loss: 0.87283885\n",
      "iteration: 159 loss: 0.48114407\n",
      "iteration: 160 loss: 1.05675995\n",
      "iteration: 161 loss: 0.80289257\n",
      "iteration: 162 loss: 0.26775098\n",
      "iteration: 163 loss: 1.15660620\n",
      "iteration: 164 loss: 3.13167143\n",
      "iteration: 165 loss: 1.29348207\n",
      "iteration: 166 loss: 0.18666305\n",
      "iteration: 167 loss: 0.17147520\n",
      "iteration: 168 loss: 1.20132315\n",
      "iteration: 169 loss: 0.17174895\n",
      "iteration: 170 loss: 2.95004201\n",
      "iteration: 171 loss: 0.46480379\n",
      "iteration: 172 loss: 0.55384153\n",
      "iteration: 173 loss: 0.08879033\n",
      "iteration: 174 loss: 3.52672052\n",
      "iteration: 175 loss: 2.19406104\n",
      "iteration: 176 loss: 1.05745327\n",
      "iteration: 177 loss: 0.57452399\n",
      "iteration: 178 loss: 0.25022322\n",
      "iteration: 179 loss: 0.30505627\n",
      "iteration: 180 loss: 0.60123485\n",
      "iteration: 181 loss: 1.44448757\n",
      "iteration: 182 loss: 0.18274680\n",
      "iteration: 183 loss: 1.47155035\n",
      "iteration: 184 loss: 0.67615384\n",
      "iteration: 185 loss: 1.75548649\n",
      "iteration: 186 loss: 1.30811203\n",
      "iteration: 187 loss: 0.81191826\n",
      "iteration: 188 loss: 3.81625938\n",
      "iteration: 189 loss: 2.20223188\n",
      "iteration: 190 loss: 2.05849957\n",
      "iteration: 191 loss: 0.26976827\n",
      "iteration: 192 loss: 0.91477388\n",
      "iteration: 193 loss: 1.55487251\n",
      "iteration: 194 loss: 0.16412887\n",
      "iteration: 195 loss: 2.67287779\n",
      "iteration: 196 loss: 3.45436168\n",
      "iteration: 197 loss: 0.64355808\n",
      "iteration: 198 loss: 0.83845985\n",
      "iteration: 199 loss: 0.64282459\n",
      "epoch:  92 mean loss training: 1.13945198\n",
      "epoch:  92 mean loss validation: 1.42305994\n",
      "iteration:   0 loss: 2.67320585\n",
      "iteration:   1 loss: 0.58297575\n",
      "iteration:   2 loss: 0.98303229\n",
      "iteration:   3 loss: 0.74162966\n",
      "iteration:   4 loss: 1.31035352\n",
      "iteration:   5 loss: 1.13750625\n",
      "iteration:   6 loss: 1.18933856\n",
      "iteration:   7 loss: 1.02692628\n",
      "iteration:   8 loss: 2.45687079\n",
      "iteration:   9 loss: 3.47798514\n",
      "iteration:  10 loss: 0.42093545\n",
      "iteration:  11 loss: 0.39383113\n",
      "iteration:  12 loss: 0.66960382\n",
      "iteration:  13 loss: 1.25871491\n",
      "iteration:  14 loss: 0.99653667\n",
      "iteration:  15 loss: 0.13966595\n",
      "iteration:  16 loss: 0.49040031\n",
      "iteration:  17 loss: 0.62189668\n",
      "iteration:  18 loss: 0.93402058\n",
      "iteration:  19 loss: 1.40016651\n",
      "iteration:  20 loss: 0.23124780\n",
      "iteration:  21 loss: 1.00304496\n",
      "iteration:  22 loss: 0.17917061\n",
      "iteration:  23 loss: 0.86881745\n",
      "iteration:  24 loss: 0.29890701\n",
      "iteration:  25 loss: 2.44073296\n",
      "iteration:  26 loss: 0.81934464\n",
      "iteration:  27 loss: 3.23949552\n",
      "iteration:  28 loss: 0.20361067\n",
      "iteration:  29 loss: 0.30286387\n",
      "iteration:  30 loss: 0.53562558\n",
      "iteration:  31 loss: 0.37745553\n",
      "iteration:  32 loss: 0.16130091\n",
      "iteration:  33 loss: 0.15028627\n",
      "iteration:  34 loss: 0.97567832\n",
      "iteration:  35 loss: 0.96697813\n",
      "iteration:  36 loss: 0.09601334\n",
      "iteration:  37 loss: 0.43177366\n",
      "iteration:  38 loss: 1.11572373\n",
      "iteration:  39 loss: 3.37421799\n",
      "iteration:  40 loss: 1.06289148\n",
      "iteration:  41 loss: 0.60543615\n",
      "iteration:  42 loss: 0.99282128\n",
      "iteration:  43 loss: 2.77160501\n",
      "iteration:  44 loss: 0.46151960\n",
      "iteration:  45 loss: 0.17924182\n",
      "iteration:  46 loss: 1.55411994\n",
      "iteration:  47 loss: 1.01834965\n",
      "iteration:  48 loss: 0.80983448\n",
      "iteration:  49 loss: 0.51794451\n",
      "iteration:  50 loss: 1.96794510\n",
      "iteration:  51 loss: 0.29396066\n",
      "iteration:  52 loss: 0.57148236\n",
      "iteration:  53 loss: 0.72048128\n",
      "iteration:  54 loss: 0.36902767\n",
      "iteration:  55 loss: 4.34213686\n",
      "iteration:  56 loss: 0.76234508\n",
      "iteration:  57 loss: 2.86609483\n",
      "iteration:  58 loss: 2.49513459\n",
      "iteration:  59 loss: 1.51825953\n",
      "iteration:  60 loss: 0.24269724\n",
      "iteration:  61 loss: 0.17104407\n",
      "iteration:  62 loss: 0.40855327\n",
      "iteration:  63 loss: 0.69302541\n",
      "iteration:  64 loss: 0.11913404\n",
      "iteration:  65 loss: 0.68475688\n",
      "iteration:  66 loss: 0.15741889\n",
      "iteration:  67 loss: 0.12539704\n",
      "iteration:  68 loss: 0.13284944\n",
      "iteration:  69 loss: 0.33015499\n",
      "iteration:  70 loss: 0.36126676\n",
      "iteration:  71 loss: 0.55411112\n",
      "iteration:  72 loss: 0.57580549\n",
      "iteration:  73 loss: 2.22335720\n",
      "iteration:  74 loss: 0.30147657\n",
      "iteration:  75 loss: 0.82833672\n",
      "iteration:  76 loss: 0.72004032\n",
      "iteration:  77 loss: 0.44980800\n",
      "iteration:  78 loss: 1.51633787\n",
      "iteration:  79 loss: 0.60909832\n",
      "iteration:  80 loss: 1.29779458\n",
      "iteration:  81 loss: 0.10026371\n",
      "iteration:  82 loss: 0.33427578\n",
      "iteration:  83 loss: 0.14226541\n",
      "iteration:  84 loss: 3.12960029\n",
      "iteration:  85 loss: 0.15762293\n",
      "iteration:  86 loss: 0.43362024\n",
      "iteration:  87 loss: 2.91935730\n",
      "iteration:  88 loss: 0.14015229\n",
      "iteration:  89 loss: 2.02579570\n",
      "iteration:  90 loss: 2.08886194\n",
      "iteration:  91 loss: 1.97513938\n",
      "iteration:  92 loss: 0.07693868\n",
      "iteration:  93 loss: 1.82606041\n",
      "iteration:  94 loss: 1.56145370\n",
      "iteration:  95 loss: 0.69467103\n",
      "iteration:  96 loss: 3.79537797\n",
      "iteration:  97 loss: 1.05805957\n",
      "iteration:  98 loss: 0.29628980\n",
      "iteration:  99 loss: 0.19752778\n",
      "iteration: 100 loss: 0.23159015\n",
      "iteration: 101 loss: 0.17023794\n",
      "iteration: 102 loss: 0.69294274\n",
      "iteration: 103 loss: 2.74842954\n",
      "iteration: 104 loss: 0.57460600\n",
      "iteration: 105 loss: 0.60951674\n",
      "iteration: 106 loss: 0.31907946\n",
      "iteration: 107 loss: 0.21740648\n",
      "iteration: 108 loss: 0.82432377\n",
      "iteration: 109 loss: 0.24919234\n",
      "iteration: 110 loss: 1.93864632\n",
      "iteration: 111 loss: 3.10519981\n",
      "iteration: 112 loss: 2.56074405\n",
      "iteration: 113 loss: 0.27939186\n",
      "iteration: 114 loss: 0.60845453\n",
      "iteration: 115 loss: 0.83328748\n",
      "iteration: 116 loss: 1.98767340\n",
      "iteration: 117 loss: 0.59423840\n",
      "iteration: 118 loss: 0.24307998\n",
      "iteration: 119 loss: 0.38709956\n",
      "iteration: 120 loss: 0.23468304\n",
      "iteration: 121 loss: 0.63282764\n",
      "iteration: 122 loss: 0.11809195\n",
      "iteration: 123 loss: 2.24265027\n",
      "iteration: 124 loss: 1.35339200\n",
      "iteration: 125 loss: 1.78803027\n",
      "iteration: 126 loss: 3.34291863\n",
      "iteration: 127 loss: 0.21577050\n",
      "iteration: 128 loss: 2.07940888\n",
      "iteration: 129 loss: 0.15621753\n",
      "iteration: 130 loss: 2.57152653\n",
      "iteration: 131 loss: 2.80041051\n",
      "iteration: 132 loss: 1.25877738\n",
      "iteration: 133 loss: 0.75026339\n",
      "iteration: 134 loss: 0.34999350\n",
      "iteration: 135 loss: 2.08859015\n",
      "iteration: 136 loss: 0.84358126\n",
      "iteration: 137 loss: 1.02671015\n",
      "iteration: 138 loss: 0.83724761\n",
      "iteration: 139 loss: 1.14404035\n",
      "iteration: 140 loss: 0.89070171\n",
      "iteration: 141 loss: 0.77255964\n",
      "iteration: 142 loss: 0.87988073\n",
      "iteration: 143 loss: 0.67628503\n",
      "iteration: 144 loss: 3.54054284\n",
      "iteration: 145 loss: 0.88383645\n",
      "iteration: 146 loss: 0.20237668\n",
      "iteration: 147 loss: 0.91049027\n",
      "iteration: 148 loss: 0.93847340\n",
      "iteration: 149 loss: 0.65989524\n",
      "iteration: 150 loss: 1.82674026\n",
      "iteration: 151 loss: 2.11389065\n",
      "iteration: 152 loss: 3.02581382\n",
      "iteration: 153 loss: 3.17154193\n",
      "iteration: 154 loss: 1.45837748\n",
      "iteration: 155 loss: 2.56255221\n",
      "iteration: 156 loss: 0.31648409\n",
      "iteration: 157 loss: 0.77102351\n",
      "iteration: 158 loss: 0.70338935\n",
      "iteration: 159 loss: 0.22291759\n",
      "iteration: 160 loss: 0.87510294\n",
      "iteration: 161 loss: 0.92291111\n",
      "iteration: 162 loss: 0.24247150\n",
      "iteration: 163 loss: 1.32317686\n",
      "iteration: 164 loss: 3.14084148\n",
      "iteration: 165 loss: 0.66626894\n",
      "iteration: 166 loss: 0.15728517\n",
      "iteration: 167 loss: 0.17531246\n",
      "iteration: 168 loss: 0.86851001\n",
      "iteration: 169 loss: 0.18256925\n",
      "iteration: 170 loss: 2.92814136\n",
      "iteration: 171 loss: 0.47211149\n",
      "iteration: 172 loss: 0.57916462\n",
      "iteration: 173 loss: 0.11080822\n",
      "iteration: 174 loss: 3.46612930\n",
      "iteration: 175 loss: 2.15929532\n",
      "iteration: 176 loss: 0.78536057\n",
      "iteration: 177 loss: 0.57748967\n",
      "iteration: 178 loss: 0.24861760\n",
      "iteration: 179 loss: 0.24644376\n",
      "iteration: 180 loss: 0.50718510\n",
      "iteration: 181 loss: 1.48368168\n",
      "iteration: 182 loss: 0.14151783\n",
      "iteration: 183 loss: 1.50022733\n",
      "iteration: 184 loss: 0.59020698\n",
      "iteration: 185 loss: 1.54063106\n",
      "iteration: 186 loss: 2.00662160\n",
      "iteration: 187 loss: 0.62015969\n",
      "iteration: 188 loss: 3.19158816\n",
      "iteration: 189 loss: 2.23511529\n",
      "iteration: 190 loss: 1.89988554\n",
      "iteration: 191 loss: 0.34416729\n",
      "iteration: 192 loss: 1.12947571\n",
      "iteration: 193 loss: 1.21562874\n",
      "iteration: 194 loss: 0.13939445\n",
      "iteration: 195 loss: 2.76633215\n",
      "iteration: 196 loss: 3.47775412\n",
      "iteration: 197 loss: 0.64285809\n",
      "iteration: 198 loss: 0.84972048\n",
      "iteration: 199 loss: 0.63774806\n",
      "epoch:  93 mean loss training: 1.10300112\n",
      "epoch:  93 mean loss validation: 1.41242588\n",
      "iteration:   0 loss: 2.79626179\n",
      "iteration:   1 loss: 0.55035269\n",
      "iteration:   2 loss: 0.87597466\n",
      "iteration:   3 loss: 0.78967100\n",
      "iteration:   4 loss: 1.39544284\n",
      "iteration:   5 loss: 0.81186807\n",
      "iteration:   6 loss: 1.34016752\n",
      "iteration:   7 loss: 0.83569902\n",
      "iteration:   8 loss: 2.49291658\n",
      "iteration:   9 loss: 3.83657932\n",
      "iteration:  10 loss: 0.44411334\n",
      "iteration:  11 loss: 0.39184964\n",
      "iteration:  12 loss: 0.70711637\n",
      "iteration:  13 loss: 1.25743783\n",
      "iteration:  14 loss: 1.00580752\n",
      "iteration:  15 loss: 0.30889234\n",
      "iteration:  16 loss: 0.75952303\n",
      "iteration:  17 loss: 0.65819639\n",
      "iteration:  18 loss: 0.99458879\n",
      "iteration:  19 loss: 1.44705355\n",
      "iteration:  20 loss: 0.33813995\n",
      "iteration:  21 loss: 0.78197140\n",
      "iteration:  22 loss: 0.18464577\n",
      "iteration:  23 loss: 0.97739643\n",
      "iteration:  24 loss: 0.23415855\n",
      "iteration:  25 loss: 1.74799550\n",
      "iteration:  26 loss: 0.89209151\n",
      "iteration:  27 loss: 3.07084012\n",
      "iteration:  28 loss: 0.26298904\n",
      "iteration:  29 loss: 0.38088545\n",
      "iteration:  30 loss: 0.56001085\n",
      "iteration:  31 loss: 0.39758524\n",
      "iteration:  32 loss: 0.30796954\n",
      "iteration:  33 loss: 0.15262039\n",
      "iteration:  34 loss: 1.03290927\n",
      "iteration:  35 loss: 1.08467317\n",
      "iteration:  36 loss: 0.11281129\n",
      "iteration:  37 loss: 0.43283156\n",
      "iteration:  38 loss: 1.11351848\n",
      "iteration:  39 loss: 3.43503952\n",
      "iteration:  40 loss: 1.06535554\n",
      "iteration:  41 loss: 0.51739705\n",
      "iteration:  42 loss: 0.94988710\n",
      "iteration:  43 loss: 2.76468587\n",
      "iteration:  44 loss: 0.38202834\n",
      "iteration:  45 loss: 0.18706943\n",
      "iteration:  46 loss: 1.84690011\n",
      "iteration:  47 loss: 1.08377445\n",
      "iteration:  48 loss: 0.83838022\n",
      "iteration:  49 loss: 0.51853418\n",
      "iteration:  50 loss: 2.10136914\n",
      "iteration:  51 loss: 0.34297562\n",
      "iteration:  52 loss: 0.62496322\n",
      "iteration:  53 loss: 0.67730176\n",
      "iteration:  54 loss: 0.38549685\n",
      "iteration:  55 loss: 4.36414480\n",
      "iteration:  56 loss: 0.84526753\n",
      "iteration:  57 loss: 2.87649107\n",
      "iteration:  58 loss: 2.47771311\n",
      "iteration:  59 loss: 1.60806715\n",
      "iteration:  60 loss: 0.28010073\n",
      "iteration:  61 loss: 0.17673726\n",
      "iteration:  62 loss: 1.16256976\n",
      "iteration:  63 loss: 0.84397393\n",
      "iteration:  64 loss: 0.11468092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  65 loss: 0.79844648\n",
      "iteration:  66 loss: 0.14667115\n",
      "iteration:  67 loss: 0.12291300\n",
      "iteration:  68 loss: 0.14800522\n",
      "iteration:  69 loss: 0.62578666\n",
      "iteration:  70 loss: 0.51001173\n",
      "iteration:  71 loss: 0.45174176\n",
      "iteration:  72 loss: 0.59972560\n",
      "iteration:  73 loss: 2.27469921\n",
      "iteration:  74 loss: 0.29604569\n",
      "iteration:  75 loss: 0.82216799\n",
      "iteration:  76 loss: 0.74732596\n",
      "iteration:  77 loss: 0.55530339\n",
      "iteration:  78 loss: 1.52528024\n",
      "iteration:  79 loss: 0.61797673\n",
      "iteration:  80 loss: 1.34339392\n",
      "iteration:  81 loss: 0.10677745\n",
      "iteration:  82 loss: 0.31764406\n",
      "iteration:  83 loss: 0.43515640\n",
      "iteration:  84 loss: 3.19586205\n",
      "iteration:  85 loss: 0.15666252\n",
      "iteration:  86 loss: 0.45083320\n",
      "iteration:  87 loss: 2.91447973\n",
      "iteration:  88 loss: 0.20533875\n",
      "iteration:  89 loss: 2.03533292\n",
      "iteration:  90 loss: 2.06895018\n",
      "iteration:  91 loss: 2.01598787\n",
      "iteration:  92 loss: 0.17993513\n",
      "iteration:  93 loss: 0.24541813\n",
      "iteration:  94 loss: 1.26136839\n",
      "iteration:  95 loss: 0.73486334\n",
      "iteration:  96 loss: 4.19404697\n",
      "iteration:  97 loss: 1.10610557\n",
      "iteration:  98 loss: 0.25654104\n",
      "iteration:  99 loss: 0.14052379\n",
      "iteration: 100 loss: 0.18987174\n",
      "iteration: 101 loss: 0.17193602\n",
      "iteration: 102 loss: 0.64111376\n",
      "iteration: 103 loss: 2.90098333\n",
      "iteration: 104 loss: 0.42977071\n",
      "iteration: 105 loss: 0.58702034\n",
      "iteration: 106 loss: 0.15480192\n",
      "iteration: 107 loss: 0.27097651\n",
      "iteration: 108 loss: 0.82415426\n",
      "iteration: 109 loss: 0.25585398\n",
      "iteration: 110 loss: 2.04664493\n",
      "iteration: 111 loss: 3.11339307\n",
      "iteration: 112 loss: 2.48572493\n",
      "iteration: 113 loss: 0.31666872\n",
      "iteration: 114 loss: 0.73828501\n",
      "iteration: 115 loss: 0.92546844\n",
      "iteration: 116 loss: 1.87000120\n",
      "iteration: 117 loss: 0.12094767\n",
      "iteration: 118 loss: 0.19232127\n",
      "iteration: 119 loss: 0.38897535\n",
      "iteration: 120 loss: 0.14861557\n",
      "iteration: 121 loss: 0.64378452\n",
      "iteration: 122 loss: 0.13665046\n",
      "iteration: 123 loss: 2.24130034\n",
      "iteration: 124 loss: 0.99731076\n",
      "iteration: 125 loss: 2.90907550\n",
      "iteration: 126 loss: 3.78482866\n",
      "iteration: 127 loss: 0.21634363\n",
      "iteration: 128 loss: 1.68788421\n",
      "iteration: 129 loss: 0.16010164\n",
      "iteration: 130 loss: 2.72331953\n",
      "iteration: 131 loss: 2.94240570\n",
      "iteration: 132 loss: 1.29083622\n",
      "iteration: 133 loss: 0.58665836\n",
      "iteration: 134 loss: 0.37040472\n",
      "iteration: 135 loss: 2.08140302\n",
      "iteration: 136 loss: 0.92938173\n",
      "iteration: 137 loss: 0.63978535\n",
      "iteration: 138 loss: 0.99385816\n",
      "iteration: 139 loss: 1.16189253\n",
      "iteration: 140 loss: 0.93456215\n",
      "iteration: 141 loss: 0.80085969\n",
      "iteration: 142 loss: 0.92292428\n",
      "iteration: 143 loss: 1.01564670\n",
      "iteration: 144 loss: 3.47259450\n",
      "iteration: 145 loss: 1.03926182\n",
      "iteration: 146 loss: 0.26970053\n",
      "iteration: 147 loss: 1.07985544\n",
      "iteration: 148 loss: 0.91482365\n",
      "iteration: 149 loss: 0.61112678\n",
      "iteration: 150 loss: 1.85236943\n",
      "iteration: 151 loss: 2.18903136\n",
      "iteration: 152 loss: 3.19629169\n",
      "iteration: 153 loss: 2.99253464\n",
      "iteration: 154 loss: 1.95368826\n",
      "iteration: 155 loss: 2.73411274\n",
      "iteration: 156 loss: 0.53312355\n",
      "iteration: 157 loss: 0.80717540\n",
      "iteration: 158 loss: 0.75933808\n",
      "iteration: 159 loss: 0.38386121\n",
      "iteration: 160 loss: 0.91656095\n",
      "iteration: 161 loss: 0.84947461\n",
      "iteration: 162 loss: 0.24245583\n",
      "iteration: 163 loss: 1.71585619\n",
      "iteration: 164 loss: 3.11090755\n",
      "iteration: 165 loss: 1.23186362\n",
      "iteration: 166 loss: 0.16602908\n",
      "iteration: 167 loss: 0.17737694\n",
      "iteration: 168 loss: 1.07560027\n",
      "iteration: 169 loss: 0.18001288\n",
      "iteration: 170 loss: 2.93519306\n",
      "iteration: 171 loss: 0.45764557\n",
      "iteration: 172 loss: 0.47172332\n",
      "iteration: 173 loss: 0.11474963\n",
      "iteration: 174 loss: 3.46342683\n",
      "iteration: 175 loss: 2.58094525\n",
      "iteration: 176 loss: 0.82254595\n",
      "iteration: 177 loss: 0.58271545\n",
      "iteration: 178 loss: 0.22650331\n",
      "iteration: 179 loss: 0.27192855\n",
      "iteration: 180 loss: 0.53020394\n",
      "iteration: 181 loss: 1.24656677\n",
      "iteration: 182 loss: 0.13298286\n",
      "iteration: 183 loss: 1.32496333\n",
      "iteration: 184 loss: 0.61070883\n",
      "iteration: 185 loss: 1.56601334\n",
      "iteration: 186 loss: 1.69613481\n",
      "iteration: 187 loss: 0.50879717\n",
      "iteration: 188 loss: 3.35602760\n",
      "iteration: 189 loss: 2.16580939\n",
      "iteration: 190 loss: 2.37164211\n",
      "iteration: 191 loss: 0.27816311\n",
      "iteration: 192 loss: 0.93615419\n",
      "iteration: 193 loss: 1.47153032\n",
      "iteration: 194 loss: 0.13692029\n",
      "iteration: 195 loss: 2.51733351\n",
      "iteration: 196 loss: 3.31538749\n",
      "iteration: 197 loss: 0.76521045\n",
      "iteration: 198 loss: 0.79114878\n",
      "iteration: 199 loss: 0.63379520\n",
      "epoch:  94 mean loss training: 1.12763202\n",
      "epoch:  94 mean loss validation: 1.38450360\n",
      "iteration:   0 loss: 2.97157788\n",
      "iteration:   1 loss: 0.53306431\n",
      "iteration:   2 loss: 0.78776938\n",
      "iteration:   3 loss: 0.80061066\n",
      "iteration:   4 loss: 1.41880989\n",
      "iteration:   5 loss: 1.35567498\n",
      "iteration:   6 loss: 1.14136159\n",
      "iteration:   7 loss: 0.95717084\n",
      "iteration:   8 loss: 2.36256552\n",
      "iteration:   9 loss: 3.87960029\n",
      "iteration:  10 loss: 0.38709000\n",
      "iteration:  11 loss: 0.40457219\n",
      "iteration:  12 loss: 0.82369405\n",
      "iteration:  13 loss: 1.12421286\n",
      "iteration:  14 loss: 1.02045858\n",
      "iteration:  15 loss: 0.30497971\n",
      "iteration:  16 loss: 0.39567459\n",
      "iteration:  17 loss: 0.66525602\n",
      "iteration:  18 loss: 1.08295238\n",
      "iteration:  19 loss: 1.37438142\n",
      "iteration:  20 loss: 0.47156620\n",
      "iteration:  21 loss: 0.74625838\n",
      "iteration:  22 loss: 0.18977135\n",
      "iteration:  23 loss: 0.92128235\n",
      "iteration:  24 loss: 0.23379251\n",
      "iteration:  25 loss: 1.84169626\n",
      "iteration:  26 loss: 0.90580040\n",
      "iteration:  27 loss: 2.94669437\n",
      "iteration:  28 loss: 0.28981307\n",
      "iteration:  29 loss: 0.24737270\n",
      "iteration:  30 loss: 0.54985219\n",
      "iteration:  31 loss: 0.38864988\n",
      "iteration:  32 loss: 0.34694532\n",
      "iteration:  33 loss: 0.14130272\n",
      "iteration:  34 loss: 1.01989746\n",
      "iteration:  35 loss: 1.08734441\n",
      "iteration:  36 loss: 0.09598123\n",
      "iteration:  37 loss: 0.41705450\n",
      "iteration:  38 loss: 1.11245775\n",
      "iteration:  39 loss: 3.44098210\n",
      "iteration:  40 loss: 1.23128510\n",
      "iteration:  41 loss: 0.51341969\n",
      "iteration:  42 loss: 0.81299102\n",
      "iteration:  43 loss: 2.75709820\n",
      "iteration:  44 loss: 0.54211158\n",
      "iteration:  45 loss: 0.21590979\n",
      "iteration:  46 loss: 1.07037568\n",
      "iteration:  47 loss: 0.76396853\n",
      "iteration:  48 loss: 0.84833747\n",
      "iteration:  49 loss: 0.53058124\n",
      "iteration:  50 loss: 2.08239412\n",
      "iteration:  51 loss: 0.34481049\n",
      "iteration:  52 loss: 0.56168371\n",
      "iteration:  53 loss: 0.69145024\n",
      "iteration:  54 loss: 0.34439576\n",
      "iteration:  55 loss: 4.56047773\n",
      "iteration:  56 loss: 0.61321950\n",
      "iteration:  57 loss: 2.87333798\n",
      "iteration:  58 loss: 2.31279373\n",
      "iteration:  59 loss: 1.55641913\n",
      "iteration:  60 loss: 0.19401518\n",
      "iteration:  61 loss: 0.16827911\n",
      "iteration:  62 loss: 0.22964962\n",
      "iteration:  63 loss: 0.79510695\n",
      "iteration:  64 loss: 0.12117738\n",
      "iteration:  65 loss: 0.58331710\n",
      "iteration:  66 loss: 0.12622538\n",
      "iteration:  67 loss: 0.10076408\n",
      "iteration:  68 loss: 0.12615247\n",
      "iteration:  69 loss: 0.68899179\n",
      "iteration:  70 loss: 0.54273576\n",
      "iteration:  71 loss: 0.50805116\n",
      "iteration:  72 loss: 0.61381596\n",
      "iteration:  73 loss: 2.14083552\n",
      "iteration:  74 loss: 0.21745884\n",
      "iteration:  75 loss: 0.73042715\n",
      "iteration:  76 loss: 0.72429764\n",
      "iteration:  77 loss: 0.57587087\n",
      "iteration:  78 loss: 1.21728873\n",
      "iteration:  79 loss: 0.52776223\n",
      "iteration:  80 loss: 1.27330399\n",
      "iteration:  81 loss: 0.09909520\n",
      "iteration:  82 loss: 0.22726744\n",
      "iteration:  83 loss: 0.93298680\n",
      "iteration:  84 loss: 3.12401676\n",
      "iteration:  85 loss: 0.14502913\n",
      "iteration:  86 loss: 0.44221044\n",
      "iteration:  87 loss: 2.74797249\n",
      "iteration:  88 loss: 0.22015218\n",
      "iteration:  89 loss: 2.38836503\n",
      "iteration:  90 loss: 2.58163357\n",
      "iteration:  91 loss: 1.97999108\n",
      "iteration:  92 loss: 0.13188441\n",
      "iteration:  93 loss: 1.20550668\n",
      "iteration:  94 loss: 0.81704116\n",
      "iteration:  95 loss: 0.68419898\n",
      "iteration:  96 loss: 3.54786873\n",
      "iteration:  97 loss: 1.09404194\n",
      "iteration:  98 loss: 0.31033981\n",
      "iteration:  99 loss: 0.21450360\n",
      "iteration: 100 loss: 0.30132446\n",
      "iteration: 101 loss: 0.18079896\n",
      "iteration: 102 loss: 0.66688603\n",
      "iteration: 103 loss: 2.72693515\n",
      "iteration: 104 loss: 0.50894552\n",
      "iteration: 105 loss: 0.63180923\n",
      "iteration: 106 loss: 0.41917592\n",
      "iteration: 107 loss: 0.19974667\n",
      "iteration: 108 loss: 0.81310970\n",
      "iteration: 109 loss: 0.23116900\n",
      "iteration: 110 loss: 1.92807400\n",
      "iteration: 111 loss: 3.10444903\n",
      "iteration: 112 loss: 2.77320337\n",
      "iteration: 113 loss: 0.26993284\n",
      "iteration: 114 loss: 0.74876249\n",
      "iteration: 115 loss: 0.86075783\n",
      "iteration: 116 loss: 2.55105710\n",
      "iteration: 117 loss: 0.47641340\n",
      "iteration: 118 loss: 0.31408587\n",
      "iteration: 119 loss: 0.44155326\n",
      "iteration: 120 loss: 0.18562333\n",
      "iteration: 121 loss: 0.65886652\n",
      "iteration: 122 loss: 0.11566282\n",
      "iteration: 123 loss: 2.36308670\n",
      "iteration: 124 loss: 0.94872570\n",
      "iteration: 125 loss: 1.36970496\n",
      "iteration: 126 loss: 3.09492564\n",
      "iteration: 127 loss: 0.18484773\n",
      "iteration: 128 loss: 1.70225668\n",
      "iteration: 129 loss: 0.15246840\n",
      "iteration: 130 loss: 2.64687800\n",
      "iteration: 131 loss: 2.90402746\n",
      "iteration: 132 loss: 1.14419663\n",
      "iteration: 133 loss: 0.57708204\n",
      "iteration: 134 loss: 0.35687259\n",
      "iteration: 135 loss: 2.07482076\n",
      "iteration: 136 loss: 0.87906396\n",
      "iteration: 137 loss: 0.89008969\n",
      "iteration: 138 loss: 0.84551048\n",
      "iteration: 139 loss: 1.06661177\n",
      "iteration: 140 loss: 0.92702717\n",
      "iteration: 141 loss: 0.73933744\n",
      "iteration: 142 loss: 0.75328726\n",
      "iteration: 143 loss: 0.68452501\n",
      "iteration: 144 loss: 3.50449562\n",
      "iteration: 145 loss: 1.15114820\n",
      "iteration: 146 loss: 0.31485838\n",
      "iteration: 147 loss: 0.85096961\n",
      "iteration: 148 loss: 0.92180854\n",
      "iteration: 149 loss: 0.61271250\n",
      "iteration: 150 loss: 1.72411585\n",
      "iteration: 151 loss: 2.13921380\n",
      "iteration: 152 loss: 2.88065124\n",
      "iteration: 153 loss: 3.13130164\n",
      "iteration: 154 loss: 1.22155762\n",
      "iteration: 155 loss: 2.53414965\n",
      "iteration: 156 loss: 0.19199175\n",
      "iteration: 157 loss: 0.76088417\n",
      "iteration: 158 loss: 0.64995831\n",
      "iteration: 159 loss: 0.22310477\n",
      "iteration: 160 loss: 0.90332150\n",
      "iteration: 161 loss: 0.88673186\n",
      "iteration: 162 loss: 0.22659643\n",
      "iteration: 163 loss: 1.94308650\n",
      "iteration: 164 loss: 3.49026299\n",
      "iteration: 165 loss: 1.51211631\n",
      "iteration: 166 loss: 0.18799423\n",
      "iteration: 167 loss: 0.17238276\n",
      "iteration: 168 loss: 0.82299906\n",
      "iteration: 169 loss: 0.17326273\n",
      "iteration: 170 loss: 2.93016768\n",
      "iteration: 171 loss: 0.46999708\n",
      "iteration: 172 loss: 0.55247605\n",
      "iteration: 173 loss: 0.11154433\n",
      "iteration: 174 loss: 3.45648766\n",
      "iteration: 175 loss: 2.56033731\n",
      "iteration: 176 loss: 1.41177881\n",
      "iteration: 177 loss: 0.60214537\n",
      "iteration: 178 loss: 0.24237765\n",
      "iteration: 179 loss: 0.27388969\n",
      "iteration: 180 loss: 0.56181359\n",
      "iteration: 181 loss: 1.44502938\n",
      "iteration: 182 loss: 0.15290776\n",
      "iteration: 183 loss: 1.44725597\n",
      "iteration: 184 loss: 0.58556706\n",
      "iteration: 185 loss: 1.52474046\n",
      "iteration: 186 loss: 1.87234092\n",
      "iteration: 187 loss: 0.62312984\n",
      "iteration: 188 loss: 2.88986111\n",
      "iteration: 189 loss: 2.23957348\n",
      "iteration: 190 loss: 1.53783524\n",
      "iteration: 191 loss: 0.40952960\n",
      "iteration: 192 loss: 1.29423475\n",
      "iteration: 193 loss: 1.35417509\n",
      "iteration: 194 loss: 0.13933377\n",
      "iteration: 195 loss: 2.82149005\n",
      "iteration: 196 loss: 3.45574236\n",
      "iteration: 197 loss: 0.83646744\n",
      "iteration: 198 loss: 0.64813942\n",
      "iteration: 199 loss: 0.68789482\n",
      "epoch:  95 mean loss training: 1.10598147\n",
      "epoch:  95 mean loss validation: 1.44916332\n",
      "iteration:   0 loss: 3.08441854\n",
      "iteration:   1 loss: 0.59453976\n",
      "iteration:   2 loss: 0.98423547\n",
      "iteration:   3 loss: 0.85584462\n",
      "iteration:   4 loss: 1.68361199\n",
      "iteration:   5 loss: 0.59677875\n",
      "iteration:   6 loss: 1.32563710\n",
      "iteration:   7 loss: 0.63379979\n",
      "iteration:   8 loss: 2.43007874\n",
      "iteration:   9 loss: 3.82234383\n",
      "iteration:  10 loss: 0.47035852\n",
      "iteration:  11 loss: 0.41109887\n",
      "iteration:  12 loss: 0.71107054\n",
      "iteration:  13 loss: 1.21766496\n",
      "iteration:  14 loss: 1.00654650\n",
      "iteration:  15 loss: 0.34411439\n",
      "iteration:  16 loss: 0.65444070\n",
      "iteration:  17 loss: 0.66764426\n",
      "iteration:  18 loss: 0.98446345\n",
      "iteration:  19 loss: 1.38890135\n",
      "iteration:  20 loss: 0.31519461\n",
      "iteration:  21 loss: 0.77267778\n",
      "iteration:  22 loss: 0.20048858\n",
      "iteration:  23 loss: 0.94775677\n",
      "iteration:  24 loss: 0.23793316\n",
      "iteration:  25 loss: 1.74892914\n",
      "iteration:  26 loss: 0.89896423\n",
      "iteration:  27 loss: 2.97517681\n",
      "iteration:  28 loss: 0.28890517\n",
      "iteration:  29 loss: 0.35351145\n",
      "iteration:  30 loss: 0.53751045\n",
      "iteration:  31 loss: 0.36329162\n",
      "iteration:  32 loss: 0.30704948\n",
      "iteration:  33 loss: 0.14352626\n",
      "iteration:  34 loss: 1.14355814\n",
      "iteration:  35 loss: 1.09903193\n",
      "iteration:  36 loss: 0.10165726\n",
      "iteration:  37 loss: 0.41386929\n",
      "iteration:  38 loss: 1.11521316\n",
      "iteration:  39 loss: 3.42450953\n",
      "iteration:  40 loss: 1.16457415\n",
      "iteration:  41 loss: 0.51068211\n",
      "iteration:  42 loss: 0.96855998\n",
      "iteration:  43 loss: 2.76466990\n",
      "iteration:  44 loss: 0.50041109\n",
      "iteration:  45 loss: 0.21733105\n",
      "iteration:  46 loss: 1.49813962\n",
      "iteration:  47 loss: 0.93605876\n",
      "iteration:  48 loss: 0.84680247\n",
      "iteration:  49 loss: 0.52258819\n",
      "iteration:  50 loss: 2.11451292\n",
      "iteration:  51 loss: 0.33378834\n",
      "iteration:  52 loss: 0.67802644\n",
      "iteration:  53 loss: 0.88448286\n",
      "iteration:  54 loss: 0.39574063\n",
      "iteration:  55 loss: 4.36229277\n",
      "iteration:  56 loss: 0.84543854\n",
      "iteration:  57 loss: 2.86037970\n",
      "iteration:  58 loss: 2.39488745\n",
      "iteration:  59 loss: 1.58150208\n",
      "iteration:  60 loss: 0.23697160\n",
      "iteration:  61 loss: 0.16429512\n",
      "iteration:  62 loss: 0.42523846\n",
      "iteration:  63 loss: 0.81139398\n",
      "iteration:  64 loss: 0.12201776\n",
      "iteration:  65 loss: 0.80567265\n",
      "iteration:  66 loss: 0.13537689\n",
      "iteration:  67 loss: 0.13218583\n",
      "iteration:  68 loss: 0.15301125\n",
      "iteration:  69 loss: 0.42799753\n",
      "iteration:  70 loss: 0.54982227\n",
      "iteration:  71 loss: 0.55342823\n",
      "iteration:  72 loss: 0.39912969\n",
      "iteration:  73 loss: 2.19231629\n",
      "iteration:  74 loss: 0.56208378\n",
      "iteration:  75 loss: 0.73363739\n",
      "iteration:  76 loss: 0.73911691\n",
      "iteration:  77 loss: 0.42069373\n",
      "iteration:  78 loss: 1.46853781\n",
      "iteration:  79 loss: 0.61867428\n",
      "iteration:  80 loss: 1.28933752\n",
      "iteration:  81 loss: 0.10907161\n",
      "iteration:  82 loss: 0.71366578\n",
      "iteration:  83 loss: 0.13285193\n",
      "iteration:  84 loss: 3.12093520\n",
      "iteration:  85 loss: 0.15408844\n",
      "iteration:  86 loss: 0.47912180\n",
      "iteration:  87 loss: 2.84719849\n",
      "iteration:  88 loss: 0.17569017\n",
      "iteration:  89 loss: 2.60249257\n",
      "iteration:  90 loss: 2.03659940\n",
      "iteration:  91 loss: 1.98331642\n",
      "iteration:  92 loss: 0.13812029\n",
      "iteration:  93 loss: 2.20019364\n",
      "iteration:  94 loss: 0.96251494\n",
      "iteration:  95 loss: 0.67394716\n",
      "iteration:  96 loss: 4.03055096\n",
      "iteration:  97 loss: 1.22525632\n",
      "iteration:  98 loss: 0.28488770\n",
      "iteration:  99 loss: 0.16401279\n",
      "iteration: 100 loss: 0.25868663\n",
      "iteration: 101 loss: 0.17371306\n",
      "iteration: 102 loss: 0.59910476\n",
      "iteration: 103 loss: 2.66447425\n",
      "iteration: 104 loss: 0.70362353\n",
      "iteration: 105 loss: 0.60472876\n",
      "iteration: 106 loss: 0.25669757\n",
      "iteration: 107 loss: 0.20968975\n",
      "iteration: 108 loss: 0.88116473\n",
      "iteration: 109 loss: 0.27631000\n",
      "iteration: 110 loss: 2.07178235\n",
      "iteration: 111 loss: 3.09969640\n",
      "iteration: 112 loss: 2.51668119\n",
      "iteration: 113 loss: 0.23862104\n",
      "iteration: 114 loss: 0.71362329\n",
      "iteration: 115 loss: 0.76606584\n",
      "iteration: 116 loss: 2.05357480\n",
      "iteration: 117 loss: 0.51068455\n",
      "iteration: 118 loss: 0.35281539\n",
      "iteration: 119 loss: 0.42596409\n",
      "iteration: 120 loss: 0.18422535\n",
      "iteration: 121 loss: 0.66022122\n",
      "iteration: 122 loss: 0.12560441\n",
      "iteration: 123 loss: 2.29501247\n",
      "iteration: 124 loss: 1.18206716\n",
      "iteration: 125 loss: 1.56025565\n",
      "iteration: 126 loss: 3.21498370\n",
      "iteration: 127 loss: 0.20139064\n",
      "iteration: 128 loss: 1.94154108\n",
      "iteration: 129 loss: 0.15923439\n",
      "iteration: 130 loss: 2.67898488\n",
      "iteration: 131 loss: 2.42393446\n",
      "iteration: 132 loss: 1.02034318\n",
      "iteration: 133 loss: 0.43120202\n",
      "iteration: 134 loss: 0.34868902\n",
      "iteration: 135 loss: 2.07639170\n",
      "iteration: 136 loss: 0.95825517\n",
      "iteration: 137 loss: 0.79564643\n",
      "iteration: 138 loss: 0.96345764\n",
      "iteration: 139 loss: 1.13749778\n",
      "iteration: 140 loss: 0.78718287\n",
      "iteration: 141 loss: 0.75232583\n",
      "iteration: 142 loss: 0.81018990\n",
      "iteration: 143 loss: 1.41947198\n",
      "iteration: 144 loss: 3.41898417\n",
      "iteration: 145 loss: 0.92334580\n",
      "iteration: 146 loss: 0.32859081\n",
      "iteration: 147 loss: 0.97134978\n",
      "iteration: 148 loss: 1.00623083\n",
      "iteration: 149 loss: 0.72512561\n",
      "iteration: 150 loss: 1.71928275\n",
      "iteration: 151 loss: 2.12722039\n",
      "iteration: 152 loss: 3.02793217\n",
      "iteration: 153 loss: 3.12557077\n",
      "iteration: 154 loss: 1.36549580\n",
      "iteration: 155 loss: 2.62297988\n",
      "iteration: 156 loss: 0.47405753\n",
      "iteration: 157 loss: 0.81180847\n",
      "iteration: 158 loss: 0.73337603\n",
      "iteration: 159 loss: 0.42644900\n",
      "iteration: 160 loss: 0.99539018\n",
      "iteration: 161 loss: 0.86216563\n",
      "iteration: 162 loss: 0.29786822\n",
      "iteration: 163 loss: 1.64282596\n",
      "iteration: 164 loss: 3.12441778\n",
      "iteration: 165 loss: 0.82129282\n",
      "iteration: 166 loss: 0.20402381\n",
      "iteration: 167 loss: 0.17540282\n",
      "iteration: 168 loss: 1.29028356\n",
      "iteration: 169 loss: 0.17202698\n",
      "iteration: 170 loss: 2.94463134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 171 loss: 0.47313398\n",
      "iteration: 172 loss: 0.70354819\n",
      "iteration: 173 loss: 0.10989997\n",
      "iteration: 174 loss: 3.46205521\n",
      "iteration: 175 loss: 2.24037123\n",
      "iteration: 176 loss: 0.94959229\n",
      "iteration: 177 loss: 0.60213184\n",
      "iteration: 178 loss: 0.24017239\n",
      "iteration: 179 loss: 0.29482538\n",
      "iteration: 180 loss: 0.57524353\n",
      "iteration: 181 loss: 1.39682913\n",
      "iteration: 182 loss: 0.21333569\n",
      "iteration: 183 loss: 1.56378639\n",
      "iteration: 184 loss: 0.66927558\n",
      "iteration: 185 loss: 1.54504681\n",
      "iteration: 186 loss: 1.17639959\n",
      "iteration: 187 loss: 0.78906888\n",
      "iteration: 188 loss: 3.39003205\n",
      "iteration: 189 loss: 2.23874164\n",
      "iteration: 190 loss: 1.45404971\n",
      "iteration: 191 loss: 0.33772761\n",
      "iteration: 192 loss: 1.05511153\n",
      "iteration: 193 loss: 1.37916696\n",
      "iteration: 194 loss: 0.13825156\n",
      "iteration: 195 loss: 2.83345461\n",
      "iteration: 196 loss: 3.48064065\n",
      "iteration: 197 loss: 0.71746206\n",
      "iteration: 198 loss: 0.88781011\n",
      "iteration: 199 loss: 0.63847995\n",
      "epoch:  96 mean loss training: 1.12006116\n",
      "epoch:  96 mean loss validation: 1.44606161\n",
      "iteration:   0 loss: 3.01224065\n",
      "iteration:   1 loss: 0.56732470\n",
      "iteration:   2 loss: 0.99548870\n",
      "iteration:   3 loss: 0.81293905\n",
      "iteration:   4 loss: 1.31028163\n",
      "iteration:   5 loss: 0.74384695\n",
      "iteration:   6 loss: 1.36902225\n",
      "iteration:   7 loss: 0.77653223\n",
      "iteration:   8 loss: 2.39604616\n",
      "iteration:   9 loss: 3.69487381\n",
      "iteration:  10 loss: 0.44435510\n",
      "iteration:  11 loss: 0.39900535\n",
      "iteration:  12 loss: 0.77418745\n",
      "iteration:  13 loss: 1.06228280\n",
      "iteration:  14 loss: 1.00729966\n",
      "iteration:  15 loss: 0.28924298\n",
      "iteration:  16 loss: 0.48565564\n",
      "iteration:  17 loss: 0.66640294\n",
      "iteration:  18 loss: 0.91100192\n",
      "iteration:  19 loss: 1.27871799\n",
      "iteration:  20 loss: 0.20912784\n",
      "iteration:  21 loss: 0.80196363\n",
      "iteration:  22 loss: 0.14512305\n",
      "iteration:  23 loss: 0.76940185\n",
      "iteration:  24 loss: 0.33056128\n",
      "iteration:  25 loss: 3.18220830\n",
      "iteration:  26 loss: 0.84276927\n",
      "iteration:  27 loss: 3.03138423\n",
      "iteration:  28 loss: 0.18006995\n",
      "iteration:  29 loss: 0.32607660\n",
      "iteration:  30 loss: 0.52193516\n",
      "iteration:  31 loss: 0.31242612\n",
      "iteration:  32 loss: 0.15054442\n",
      "iteration:  33 loss: 0.13694081\n",
      "iteration:  34 loss: 1.09617102\n",
      "iteration:  35 loss: 0.93732560\n",
      "iteration:  36 loss: 0.09654248\n",
      "iteration:  37 loss: 0.39524204\n",
      "iteration:  38 loss: 1.09680116\n",
      "iteration:  39 loss: 3.40348148\n",
      "iteration:  40 loss: 1.01780164\n",
      "iteration:  41 loss: 0.51192862\n",
      "iteration:  42 loss: 0.80961299\n",
      "iteration:  43 loss: 2.73384881\n",
      "iteration:  44 loss: 0.26459107\n",
      "iteration:  45 loss: 0.20425692\n",
      "iteration:  46 loss: 1.37091076\n",
      "iteration:  47 loss: 1.25460374\n",
      "iteration:  48 loss: 0.80658555\n",
      "iteration:  49 loss: 0.51256424\n",
      "iteration:  50 loss: 1.85647404\n",
      "iteration:  51 loss: 0.29812652\n",
      "iteration:  52 loss: 0.49138817\n",
      "iteration:  53 loss: 0.41890454\n",
      "iteration:  54 loss: 0.46172813\n",
      "iteration:  55 loss: 4.25377655\n",
      "iteration:  56 loss: 0.73747140\n",
      "iteration:  57 loss: 2.81033492\n",
      "iteration:  58 loss: 2.31854725\n",
      "iteration:  59 loss: 1.38331532\n",
      "iteration:  60 loss: 0.16887364\n",
      "iteration:  61 loss: 0.17952742\n",
      "iteration:  62 loss: 0.42064461\n",
      "iteration:  63 loss: 0.52538514\n",
      "iteration:  64 loss: 0.10788931\n",
      "iteration:  65 loss: 0.63369459\n",
      "iteration:  66 loss: 0.14528005\n",
      "iteration:  67 loss: 0.12797680\n",
      "iteration:  68 loss: 0.14500262\n",
      "iteration:  69 loss: 0.31318295\n",
      "iteration:  70 loss: 0.19726595\n",
      "iteration:  71 loss: 0.56091803\n",
      "iteration:  72 loss: 0.44637853\n",
      "iteration:  73 loss: 2.75915384\n",
      "iteration:  74 loss: 0.63498974\n",
      "iteration:  75 loss: 0.61707312\n",
      "iteration:  76 loss: 0.72164309\n",
      "iteration:  77 loss: 0.35082760\n",
      "iteration:  78 loss: 1.48494065\n",
      "iteration:  79 loss: 0.80681562\n",
      "iteration:  80 loss: 0.94780415\n",
      "iteration:  81 loss: 0.09124674\n",
      "iteration:  82 loss: 0.69289356\n",
      "iteration:  83 loss: 0.15342081\n",
      "iteration:  84 loss: 3.36389303\n",
      "iteration:  85 loss: 0.15072070\n",
      "iteration:  86 loss: 0.45408201\n",
      "iteration:  87 loss: 2.94756055\n",
      "iteration:  88 loss: 0.16253090\n",
      "iteration:  89 loss: 2.07585645\n",
      "iteration:  90 loss: 2.05418205\n",
      "iteration:  91 loss: 1.96965063\n",
      "iteration:  92 loss: 0.11483625\n",
      "iteration:  93 loss: 0.25574163\n",
      "iteration:  94 loss: 1.57454467\n",
      "iteration:  95 loss: 0.69583404\n",
      "iteration:  96 loss: 3.46079350\n",
      "iteration:  97 loss: 1.16282403\n",
      "iteration:  98 loss: 0.34465683\n",
      "iteration:  99 loss: 0.27045968\n",
      "iteration: 100 loss: 0.29350680\n",
      "iteration: 101 loss: 0.17547907\n",
      "iteration: 102 loss: 0.77284151\n",
      "iteration: 103 loss: 2.73645592\n",
      "iteration: 104 loss: 0.85260975\n",
      "iteration: 105 loss: 0.65754437\n",
      "iteration: 106 loss: 0.37649524\n",
      "iteration: 107 loss: 0.25246337\n",
      "iteration: 108 loss: 0.87511104\n",
      "iteration: 109 loss: 0.28651005\n",
      "iteration: 110 loss: 1.93086803\n",
      "iteration: 111 loss: 3.11136174\n",
      "iteration: 112 loss: 2.63324451\n",
      "iteration: 113 loss: 0.26621941\n",
      "iteration: 114 loss: 0.80409223\n",
      "iteration: 115 loss: 0.85616672\n",
      "iteration: 116 loss: 2.51848507\n",
      "iteration: 117 loss: 0.43864420\n",
      "iteration: 118 loss: 0.30652130\n",
      "iteration: 119 loss: 0.47712776\n",
      "iteration: 120 loss: 0.17437685\n",
      "iteration: 121 loss: 0.65110093\n",
      "iteration: 122 loss: 0.13286422\n",
      "iteration: 123 loss: 2.42533398\n",
      "iteration: 124 loss: 0.97731131\n",
      "iteration: 125 loss: 1.21004665\n",
      "iteration: 126 loss: 2.93117666\n",
      "iteration: 127 loss: 0.22314258\n",
      "iteration: 128 loss: 1.71403897\n",
      "iteration: 129 loss: 0.16286282\n",
      "iteration: 130 loss: 2.65235615\n",
      "iteration: 131 loss: 2.89842367\n",
      "iteration: 132 loss: 0.72808021\n",
      "iteration: 133 loss: 0.48814902\n",
      "iteration: 134 loss: 0.31991565\n",
      "iteration: 135 loss: 2.07873130\n",
      "iteration: 136 loss: 0.92008793\n",
      "iteration: 137 loss: 0.77551490\n",
      "iteration: 138 loss: 1.01058495\n",
      "iteration: 139 loss: 1.25718260\n",
      "iteration: 140 loss: 0.76278675\n",
      "iteration: 141 loss: 0.81334805\n",
      "iteration: 142 loss: 0.62708795\n",
      "iteration: 143 loss: 0.69667733\n",
      "iteration: 144 loss: 3.38843274\n",
      "iteration: 145 loss: 1.09786975\n",
      "iteration: 146 loss: 0.38943073\n",
      "iteration: 147 loss: 0.91637510\n",
      "iteration: 148 loss: 1.05179107\n",
      "iteration: 149 loss: 0.62077719\n",
      "iteration: 150 loss: 1.64355445\n",
      "iteration: 151 loss: 2.13570023\n",
      "iteration: 152 loss: 2.80408192\n",
      "iteration: 153 loss: 3.02224874\n",
      "iteration: 154 loss: 1.37420106\n",
      "iteration: 155 loss: 2.62846112\n",
      "iteration: 156 loss: 0.49234545\n",
      "iteration: 157 loss: 0.78449184\n",
      "iteration: 158 loss: 0.73477650\n",
      "iteration: 159 loss: 0.38246381\n",
      "iteration: 160 loss: 0.98028117\n",
      "iteration: 161 loss: 0.89782542\n",
      "iteration: 162 loss: 0.25443271\n",
      "iteration: 163 loss: 1.82761776\n",
      "iteration: 164 loss: 3.11868787\n",
      "iteration: 165 loss: 0.36647525\n",
      "iteration: 166 loss: 0.17986096\n",
      "iteration: 167 loss: 0.17001656\n",
      "iteration: 168 loss: 1.29293203\n",
      "iteration: 169 loss: 0.14448944\n",
      "iteration: 170 loss: 2.86991477\n",
      "iteration: 171 loss: 0.45823425\n",
      "iteration: 172 loss: 0.67998970\n",
      "iteration: 173 loss: 0.08624139\n",
      "iteration: 174 loss: 3.50999856\n",
      "iteration: 175 loss: 2.10887027\n",
      "iteration: 176 loss: 0.70881003\n",
      "iteration: 177 loss: 0.53575957\n",
      "iteration: 178 loss: 0.23476882\n",
      "iteration: 179 loss: 0.28737274\n",
      "iteration: 180 loss: 0.78020859\n",
      "iteration: 181 loss: 1.33132637\n",
      "iteration: 182 loss: 0.20055588\n",
      "iteration: 183 loss: 1.53280103\n",
      "iteration: 184 loss: 0.49449840\n",
      "iteration: 185 loss: 1.65976393\n",
      "iteration: 186 loss: 1.02879703\n",
      "iteration: 187 loss: 0.68575448\n",
      "iteration: 188 loss: 3.22137785\n",
      "iteration: 189 loss: 2.23514104\n",
      "iteration: 190 loss: 1.63890970\n",
      "iteration: 191 loss: 0.27208719\n",
      "iteration: 192 loss: 0.90870804\n",
      "iteration: 193 loss: 1.65154183\n",
      "iteration: 194 loss: 0.13941349\n",
      "iteration: 195 loss: 2.62156940\n",
      "iteration: 196 loss: 3.28734732\n",
      "iteration: 197 loss: 0.83915395\n",
      "iteration: 198 loss: 0.71315414\n",
      "iteration: 199 loss: 0.61096275\n",
      "epoch:  97 mean loss training: 1.08528090\n",
      "epoch:  97 mean loss validation: 1.37651527\n",
      "iteration:   0 loss: 2.84986997\n",
      "iteration:   1 loss: 0.57794750\n",
      "iteration:   2 loss: 0.79764265\n",
      "iteration:   3 loss: 0.80222493\n",
      "iteration:   4 loss: 2.43697023\n",
      "iteration:   5 loss: 0.76273340\n",
      "iteration:   6 loss: 1.16479993\n",
      "iteration:   7 loss: 0.81767613\n",
      "iteration:   8 loss: 2.35513711\n",
      "iteration:   9 loss: 3.82997680\n",
      "iteration:  10 loss: 0.49773195\n",
      "iteration:  11 loss: 0.52801341\n",
      "iteration:  12 loss: 0.54943573\n",
      "iteration:  13 loss: 1.34209394\n",
      "iteration:  14 loss: 0.97441304\n",
      "iteration:  15 loss: 0.20258220\n",
      "iteration:  16 loss: 0.44092795\n",
      "iteration:  17 loss: 0.67839265\n",
      "iteration:  18 loss: 0.94716060\n",
      "iteration:  19 loss: 1.21060956\n",
      "iteration:  20 loss: 0.33276239\n",
      "iteration:  21 loss: 0.73738718\n",
      "iteration:  22 loss: 0.17792118\n",
      "iteration:  23 loss: 0.84085625\n",
      "iteration:  24 loss: 0.37925026\n",
      "iteration:  25 loss: 2.17102742\n",
      "iteration:  26 loss: 0.76241899\n",
      "iteration:  27 loss: 2.77805185\n",
      "iteration:  28 loss: 0.23844005\n",
      "iteration:  29 loss: 0.22098990\n",
      "iteration:  30 loss: 0.51535195\n",
      "iteration:  31 loss: 0.33353248\n",
      "iteration:  32 loss: 0.17720881\n",
      "iteration:  33 loss: 0.13708234\n",
      "iteration:  34 loss: 1.15217185\n",
      "iteration:  35 loss: 1.09175217\n",
      "iteration:  36 loss: 0.08597816\n",
      "iteration:  37 loss: 0.38179207\n",
      "iteration:  38 loss: 1.11115575\n",
      "iteration:  39 loss: 3.40069652\n",
      "iteration:  40 loss: 1.03805411\n",
      "iteration:  41 loss: 0.55295992\n",
      "iteration:  42 loss: 0.82026744\n",
      "iteration:  43 loss: 2.71871138\n",
      "iteration:  44 loss: 0.68036091\n",
      "iteration:  45 loss: 0.12898347\n",
      "iteration:  46 loss: 1.63810420\n",
      "iteration:  47 loss: 0.93910658\n",
      "iteration:  48 loss: 0.82720762\n",
      "iteration:  49 loss: 0.50975400\n",
      "iteration:  50 loss: 2.13342595\n",
      "iteration:  51 loss: 0.34398448\n",
      "iteration:  52 loss: 0.59501129\n",
      "iteration:  53 loss: 0.78678799\n",
      "iteration:  54 loss: 0.37507817\n",
      "iteration:  55 loss: 4.30154705\n",
      "iteration:  56 loss: 0.75424892\n",
      "iteration:  57 loss: 2.87584710\n",
      "iteration:  58 loss: 2.34686923\n",
      "iteration:  59 loss: 1.57237601\n",
      "iteration:  60 loss: 0.20542805\n",
      "iteration:  61 loss: 0.17020011\n",
      "iteration:  62 loss: 0.36106524\n",
      "iteration:  63 loss: 0.76040685\n",
      "iteration:  64 loss: 0.11056101\n",
      "iteration:  65 loss: 0.69992715\n",
      "iteration:  66 loss: 0.16903715\n",
      "iteration:  67 loss: 0.15786973\n",
      "iteration:  68 loss: 0.13344325\n",
      "iteration:  69 loss: 0.38833895\n",
      "iteration:  70 loss: 0.27576524\n",
      "iteration:  71 loss: 0.75465274\n",
      "iteration:  72 loss: 0.34330344\n",
      "iteration:  73 loss: 2.23038530\n",
      "iteration:  74 loss: 0.29790518\n",
      "iteration:  75 loss: 0.83274412\n",
      "iteration:  76 loss: 0.67079258\n",
      "iteration:  77 loss: 0.48269048\n",
      "iteration:  78 loss: 1.37899351\n",
      "iteration:  79 loss: 0.62403625\n",
      "iteration:  80 loss: 1.56978130\n",
      "iteration:  81 loss: 0.10167014\n",
      "iteration:  82 loss: 0.30698100\n",
      "iteration:  83 loss: 0.22150539\n",
      "iteration:  84 loss: 3.10765433\n",
      "iteration:  85 loss: 0.15693611\n",
      "iteration:  86 loss: 0.43399781\n",
      "iteration:  87 loss: 3.01805401\n",
      "iteration:  88 loss: 0.15201224\n",
      "iteration:  89 loss: 2.01324034\n",
      "iteration:  90 loss: 1.96731448\n",
      "iteration:  91 loss: 1.49339652\n",
      "iteration:  92 loss: 0.06893965\n",
      "iteration:  93 loss: 1.54814959\n",
      "iteration:  94 loss: 2.06200266\n",
      "iteration:  95 loss: 0.69929636\n",
      "iteration:  96 loss: 4.18495417\n",
      "iteration:  97 loss: 0.44398057\n",
      "iteration:  98 loss: 0.52417505\n",
      "iteration:  99 loss: 0.13541676\n",
      "iteration: 100 loss: 0.20259328\n",
      "iteration: 101 loss: 0.37479272\n",
      "iteration: 102 loss: 0.99327236\n",
      "iteration: 103 loss: 2.79697585\n",
      "iteration: 104 loss: 0.49100271\n",
      "iteration: 105 loss: 0.73723233\n",
      "iteration: 106 loss: 0.20342442\n",
      "iteration: 107 loss: 0.36349505\n",
      "iteration: 108 loss: 2.28607750\n",
      "iteration: 109 loss: 0.83227450\n",
      "iteration: 110 loss: 1.93316185\n",
      "iteration: 111 loss: 3.10692453\n",
      "iteration: 112 loss: 2.49710751\n",
      "iteration: 113 loss: 0.33557361\n",
      "iteration: 114 loss: 0.81131840\n",
      "iteration: 115 loss: 0.88109493\n",
      "iteration: 116 loss: 2.31206965\n",
      "iteration: 117 loss: 0.11579860\n",
      "iteration: 118 loss: 0.19238237\n",
      "iteration: 119 loss: 0.40033516\n",
      "iteration: 120 loss: 0.12600617\n",
      "iteration: 121 loss: 0.69016755\n",
      "iteration: 122 loss: 0.12486735\n",
      "iteration: 123 loss: 2.31268907\n",
      "iteration: 124 loss: 1.25284779\n",
      "iteration: 125 loss: 3.12702632\n",
      "iteration: 126 loss: 3.12724853\n",
      "iteration: 127 loss: 0.21606530\n",
      "iteration: 128 loss: 1.82420576\n",
      "iteration: 129 loss: 0.17849001\n",
      "iteration: 130 loss: 2.60594821\n",
      "iteration: 131 loss: 2.77521157\n",
      "iteration: 132 loss: 0.83235228\n",
      "iteration: 133 loss: 0.39206338\n",
      "iteration: 134 loss: 0.35182184\n",
      "iteration: 135 loss: 2.08228421\n",
      "iteration: 136 loss: 0.90828121\n",
      "iteration: 137 loss: 0.68094659\n",
      "iteration: 138 loss: 1.12203789\n",
      "iteration: 139 loss: 1.05470955\n",
      "iteration: 140 loss: 0.85577536\n",
      "iteration: 141 loss: 0.89736551\n",
      "iteration: 142 loss: 0.71080881\n",
      "iteration: 143 loss: 0.71032941\n",
      "iteration: 144 loss: 3.54598045\n",
      "iteration: 145 loss: 2.61948633\n",
      "iteration: 146 loss: 0.46749797\n",
      "iteration: 147 loss: 0.83200526\n",
      "iteration: 148 loss: 0.57897770\n",
      "iteration: 149 loss: 0.61473882\n",
      "iteration: 150 loss: 1.33526194\n",
      "iteration: 151 loss: 2.02906418\n",
      "iteration: 152 loss: 3.03621340\n",
      "iteration: 153 loss: 2.92649961\n",
      "iteration: 154 loss: 1.21149826\n",
      "iteration: 155 loss: 2.63959861\n",
      "iteration: 156 loss: 0.16776845\n",
      "iteration: 157 loss: 0.73157752\n",
      "iteration: 158 loss: 0.63042367\n",
      "iteration: 159 loss: 0.35605547\n",
      "iteration: 160 loss: 0.95314240\n",
      "iteration: 161 loss: 1.04807925\n",
      "iteration: 162 loss: 0.24272406\n",
      "iteration: 163 loss: 1.75188339\n",
      "iteration: 164 loss: 3.10920620\n",
      "iteration: 165 loss: 1.17902160\n",
      "iteration: 166 loss: 0.17987369\n",
      "iteration: 167 loss: 0.17061839\n",
      "iteration: 168 loss: 1.27970600\n",
      "iteration: 169 loss: 0.15323554\n",
      "iteration: 170 loss: 2.91617179\n",
      "iteration: 171 loss: 0.45478666\n",
      "iteration: 172 loss: 0.33956730\n",
      "iteration: 173 loss: 0.08741978\n",
      "iteration: 174 loss: 3.49410963\n",
      "iteration: 175 loss: 2.76920128\n",
      "iteration: 176 loss: 0.73231292\n",
      "iteration: 177 loss: 0.58560693\n",
      "iteration: 178 loss: 0.22770959\n",
      "iteration: 179 loss: 0.30295530\n",
      "iteration: 180 loss: 0.58441299\n",
      "iteration: 181 loss: 1.27804565\n",
      "iteration: 182 loss: 0.14401451\n",
      "iteration: 183 loss: 1.21852756\n",
      "iteration: 184 loss: 0.55001587\n",
      "iteration: 185 loss: 1.62581992\n",
      "iteration: 186 loss: 1.75547528\n",
      "iteration: 187 loss: 0.48394087\n",
      "iteration: 188 loss: 3.45342350\n",
      "iteration: 189 loss: 2.14419866\n",
      "iteration: 190 loss: 2.43993449\n",
      "iteration: 191 loss: 0.29748181\n",
      "iteration: 192 loss: 0.97576374\n",
      "iteration: 193 loss: 1.53184998\n",
      "iteration: 194 loss: 0.14046596\n",
      "iteration: 195 loss: 2.55367780\n",
      "iteration: 196 loss: 3.36162567\n",
      "iteration: 197 loss: 1.03041220\n",
      "iteration: 198 loss: 0.91053796\n",
      "iteration: 199 loss: 0.67583644\n",
      "epoch:  98 mean loss training: 1.12834883\n",
      "epoch:  98 mean loss validation: 1.41093099\n",
      "iteration:   0 loss: 2.90047884\n",
      "iteration:   1 loss: 0.54012573\n",
      "iteration:   2 loss: 0.88555163\n",
      "iteration:   3 loss: 0.81093031\n",
      "iteration:   4 loss: 1.34150338\n",
      "iteration:   5 loss: 1.05515182\n",
      "iteration:   6 loss: 1.30743766\n",
      "iteration:   7 loss: 0.91644204\n",
      "iteration:   8 loss: 2.28130221\n",
      "iteration:   9 loss: 3.70459795\n",
      "iteration:  10 loss: 0.41505602\n",
      "iteration:  11 loss: 0.41062036\n",
      "iteration:  12 loss: 1.04441500\n",
      "iteration:  13 loss: 1.13677847\n",
      "iteration:  14 loss: 0.95269525\n",
      "iteration:  15 loss: 0.15168627\n",
      "iteration:  16 loss: 0.31781289\n",
      "iteration:  17 loss: 0.67989856\n",
      "iteration:  18 loss: 1.00362873\n",
      "iteration:  19 loss: 1.39802134\n",
      "iteration:  20 loss: 0.36683327\n",
      "iteration:  21 loss: 0.75651073\n",
      "iteration:  22 loss: 0.15254064\n",
      "iteration:  23 loss: 1.01005864\n",
      "iteration:  24 loss: 0.26128691\n",
      "iteration:  25 loss: 2.18929338\n",
      "iteration:  26 loss: 0.76058227\n",
      "iteration:  27 loss: 3.41356039\n",
      "iteration:  28 loss: 0.17216071\n",
      "iteration:  29 loss: 0.13535608\n",
      "iteration:  30 loss: 0.51013207\n",
      "iteration:  31 loss: 0.30142117\n",
      "iteration:  32 loss: 0.16362362\n",
      "iteration:  33 loss: 0.15355164\n",
      "iteration:  34 loss: 0.97761834\n",
      "iteration:  35 loss: 1.08165562\n",
      "iteration:  36 loss: 0.10509600\n",
      "iteration:  37 loss: 0.39965948\n",
      "iteration:  38 loss: 1.10993564\n",
      "iteration:  39 loss: 3.39861298\n",
      "iteration:  40 loss: 1.08289409\n",
      "iteration:  41 loss: 0.51881236\n",
      "iteration:  42 loss: 0.79215431\n",
      "iteration:  43 loss: 2.72048950\n",
      "iteration:  44 loss: 0.45741603\n",
      "iteration:  45 loss: 0.16249052\n",
      "iteration:  46 loss: 1.44599319\n",
      "iteration:  47 loss: 0.78973824\n",
      "iteration:  48 loss: 0.83055210\n",
      "iteration:  49 loss: 0.52678353\n",
      "iteration:  50 loss: 1.99967659\n",
      "iteration:  51 loss: 0.36481106\n",
      "iteration:  52 loss: 0.64669764\n",
      "iteration:  53 loss: 0.89618176\n",
      "iteration:  54 loss: 0.39858267\n",
      "iteration:  55 loss: 4.29372883\n",
      "iteration:  56 loss: 0.82198751\n",
      "iteration:  57 loss: 2.87386489\n",
      "iteration:  58 loss: 2.31329417\n",
      "iteration:  59 loss: 1.47412741\n",
      "iteration:  60 loss: 0.24800992\n",
      "iteration:  61 loss: 0.16395092\n",
      "iteration:  62 loss: 0.28384712\n",
      "iteration:  63 loss: 0.86302447\n",
      "iteration:  64 loss: 0.11088109\n",
      "iteration:  65 loss: 0.71549237\n",
      "iteration:  66 loss: 0.13890293\n",
      "iteration:  67 loss: 0.10242900\n",
      "iteration:  68 loss: 0.13786897\n",
      "iteration:  69 loss: 0.63469660\n",
      "iteration:  70 loss: 0.53226459\n",
      "iteration:  71 loss: 0.52086174\n",
      "iteration:  72 loss: 0.42486522\n",
      "iteration:  73 loss: 2.13273144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  74 loss: 0.20600528\n",
      "iteration:  75 loss: 0.67983830\n",
      "iteration:  76 loss: 0.73911399\n",
      "iteration:  77 loss: 0.50358379\n",
      "iteration:  78 loss: 1.43077588\n",
      "iteration:  79 loss: 0.62360883\n",
      "iteration:  80 loss: 1.35944045\n",
      "iteration:  81 loss: 0.10651046\n",
      "iteration:  82 loss: 0.35220051\n",
      "iteration:  83 loss: 0.14757450\n",
      "iteration:  84 loss: 3.11056709\n",
      "iteration:  85 loss: 0.15592405\n",
      "iteration:  86 loss: 0.43899444\n",
      "iteration:  87 loss: 2.82178831\n",
      "iteration:  88 loss: 0.19992962\n",
      "iteration:  89 loss: 2.05645776\n",
      "iteration:  90 loss: 1.99517941\n",
      "iteration:  91 loss: 1.97264111\n",
      "iteration:  92 loss: 0.13041574\n",
      "iteration:  93 loss: 1.52815723\n",
      "iteration:  94 loss: 1.03553331\n",
      "iteration:  95 loss: 0.70187926\n",
      "iteration:  96 loss: 4.11178970\n",
      "iteration:  97 loss: 1.36013019\n",
      "iteration:  98 loss: 0.23068549\n",
      "iteration:  99 loss: 0.12658370\n",
      "iteration: 100 loss: 0.22858047\n",
      "iteration: 101 loss: 0.18251255\n",
      "iteration: 102 loss: 0.57088083\n",
      "iteration: 103 loss: 2.74612761\n",
      "iteration: 104 loss: 0.49963713\n",
      "iteration: 105 loss: 0.61497551\n",
      "iteration: 106 loss: 0.16985297\n",
      "iteration: 107 loss: 0.23916778\n",
      "iteration: 108 loss: 0.92512566\n",
      "iteration: 109 loss: 0.29777613\n",
      "iteration: 110 loss: 1.91755211\n",
      "iteration: 111 loss: 3.09617639\n",
      "iteration: 112 loss: 2.55055237\n",
      "iteration: 113 loss: 0.33565566\n",
      "iteration: 114 loss: 0.81930530\n",
      "iteration: 115 loss: 0.83284301\n",
      "iteration: 116 loss: 2.12961149\n",
      "iteration: 117 loss: 0.13486676\n",
      "iteration: 118 loss: 0.15408319\n",
      "iteration: 119 loss: 0.40339935\n",
      "iteration: 120 loss: 0.12926592\n",
      "iteration: 121 loss: 0.65610892\n",
      "iteration: 122 loss: 0.12704788\n",
      "iteration: 123 loss: 2.23279428\n",
      "iteration: 124 loss: 1.04890966\n",
      "iteration: 125 loss: 3.12014723\n",
      "iteration: 126 loss: 3.12367392\n",
      "iteration: 127 loss: 0.16817142\n",
      "iteration: 128 loss: 1.67584515\n",
      "iteration: 129 loss: 0.14082287\n",
      "iteration: 130 loss: 2.60110021\n",
      "iteration: 131 loss: 2.79012728\n",
      "iteration: 132 loss: 1.04398799\n",
      "iteration: 133 loss: 0.55763930\n",
      "iteration: 134 loss: 0.31974733\n",
      "iteration: 135 loss: 2.07427502\n",
      "iteration: 136 loss: 0.88945419\n",
      "iteration: 137 loss: 0.58811647\n",
      "iteration: 138 loss: 1.09675241\n",
      "iteration: 139 loss: 1.04090214\n",
      "iteration: 140 loss: 0.72726196\n",
      "iteration: 141 loss: 0.91980439\n",
      "iteration: 142 loss: 0.71940821\n",
      "iteration: 143 loss: 0.71435469\n",
      "iteration: 144 loss: 3.55092192\n",
      "iteration: 145 loss: 2.87029886\n",
      "iteration: 146 loss: 0.49748334\n",
      "iteration: 147 loss: 0.83629614\n",
      "iteration: 148 loss: 0.55222589\n",
      "iteration: 149 loss: 0.59623712\n",
      "iteration: 150 loss: 1.34056664\n",
      "iteration: 151 loss: 2.32098222\n",
      "iteration: 152 loss: 3.07627773\n",
      "iteration: 153 loss: 2.89517546\n",
      "iteration: 154 loss: 1.24629259\n",
      "iteration: 155 loss: 2.65124702\n",
      "iteration: 156 loss: 0.18716635\n",
      "iteration: 157 loss: 0.74448210\n",
      "iteration: 158 loss: 0.63324869\n",
      "iteration: 159 loss: 0.40783322\n",
      "iteration: 160 loss: 0.59330058\n",
      "iteration: 161 loss: 0.98802167\n",
      "iteration: 162 loss: 0.25180677\n",
      "iteration: 163 loss: 1.46846914\n",
      "iteration: 164 loss: 3.10991764\n",
      "iteration: 165 loss: 0.71716803\n",
      "iteration: 166 loss: 0.22912602\n",
      "iteration: 167 loss: 0.16948792\n",
      "iteration: 168 loss: 1.32921088\n",
      "iteration: 169 loss: 0.16890062\n",
      "iteration: 170 loss: 2.80713677\n",
      "iteration: 171 loss: 0.64174283\n",
      "iteration: 172 loss: 0.28426677\n",
      "iteration: 173 loss: 0.08728441\n",
      "iteration: 174 loss: 3.49364805\n",
      "iteration: 175 loss: 3.19792509\n",
      "iteration: 176 loss: 0.80117875\n",
      "iteration: 177 loss: 0.48928884\n",
      "iteration: 178 loss: 0.23157220\n",
      "iteration: 179 loss: 0.35686669\n",
      "iteration: 180 loss: 1.45491123\n",
      "iteration: 181 loss: 1.48913729\n",
      "iteration: 182 loss: 0.16534366\n",
      "iteration: 183 loss: 1.35020792\n",
      "iteration: 184 loss: 0.57082617\n",
      "iteration: 185 loss: 1.56359255\n",
      "iteration: 186 loss: 1.67310107\n",
      "iteration: 187 loss: 0.50807256\n",
      "iteration: 188 loss: 3.41113710\n",
      "iteration: 189 loss: 2.14322686\n",
      "iteration: 190 loss: 2.25454569\n",
      "iteration: 191 loss: 0.31466526\n",
      "iteration: 192 loss: 1.13490868\n",
      "iteration: 193 loss: 1.49436402\n",
      "iteration: 194 loss: 0.16870317\n",
      "iteration: 195 loss: 2.62528014\n",
      "iteration: 196 loss: 3.37703419\n",
      "iteration: 197 loss: 0.91876942\n",
      "iteration: 198 loss: 0.87919003\n",
      "iteration: 199 loss: 0.66305530\n",
      "epoch:  99 mean loss training: 1.11278272\n",
      "epoch:  99 mean loss validation: 1.42612994\n",
      "iteration:   0 loss: 3.17127419\n",
      "iteration:   1 loss: 0.53452492\n",
      "iteration:   2 loss: 0.88235730\n",
      "iteration:   3 loss: 0.87046337\n",
      "iteration:   4 loss: 1.28680551\n",
      "iteration:   5 loss: 0.96562785\n",
      "iteration:   6 loss: 1.48900533\n",
      "iteration:   7 loss: 0.86489606\n",
      "iteration:   8 loss: 2.29452658\n",
      "iteration:   9 loss: 3.33247805\n",
      "iteration:  10 loss: 0.42698458\n",
      "iteration:  11 loss: 0.40783647\n",
      "iteration:  12 loss: 1.12188208\n",
      "iteration:  13 loss: 1.09386027\n",
      "iteration:  14 loss: 1.02628613\n",
      "iteration:  15 loss: 0.17628895\n",
      "iteration:  16 loss: 0.48387453\n",
      "iteration:  17 loss: 0.64572585\n",
      "iteration:  18 loss: 0.82630402\n",
      "iteration:  19 loss: 1.13298309\n",
      "iteration:  20 loss: 0.26700789\n",
      "iteration:  21 loss: 1.23448694\n",
      "iteration:  22 loss: 0.15095441\n",
      "iteration:  23 loss: 0.98044753\n",
      "iteration:  24 loss: 0.24893427\n",
      "iteration:  25 loss: 2.27055216\n",
      "iteration:  26 loss: 0.76754385\n",
      "iteration:  27 loss: 3.35355067\n",
      "iteration:  28 loss: 0.53193992\n",
      "iteration:  29 loss: 0.66461504\n",
      "iteration:  30 loss: 0.61954474\n",
      "iteration:  31 loss: 0.45860684\n",
      "iteration:  32 loss: 0.30952725\n",
      "iteration:  33 loss: 0.16415995\n",
      "iteration:  34 loss: 1.19402277\n",
      "iteration:  35 loss: 1.10323822\n",
      "iteration:  36 loss: 0.08878304\n",
      "iteration:  37 loss: 0.37164399\n",
      "iteration:  38 loss: 1.11165750\n",
      "iteration:  39 loss: 3.28267789\n",
      "iteration:  40 loss: 1.25619078\n",
      "iteration:  41 loss: 0.66625220\n",
      "iteration:  42 loss: 1.00599730\n",
      "iteration:  43 loss: 2.70131540\n",
      "iteration:  44 loss: 0.65053082\n",
      "iteration:  45 loss: 0.19171947\n",
      "iteration:  46 loss: 2.08633256\n",
      "iteration:  47 loss: 1.06237733\n",
      "iteration:  48 loss: 0.86309606\n",
      "iteration:  49 loss: 0.56982982\n",
      "iteration:  50 loss: 2.14381194\n",
      "iteration:  51 loss: 0.33858326\n",
      "iteration:  52 loss: 0.69587046\n",
      "iteration:  53 loss: 0.25293961\n",
      "iteration:  54 loss: 0.37629575\n",
      "iteration:  55 loss: 5.06197071\n",
      "iteration:  56 loss: 0.74072164\n",
      "iteration:  57 loss: 2.84383512\n",
      "iteration:  58 loss: 2.23860717\n",
      "iteration:  59 loss: 0.44225371\n",
      "iteration:  60 loss: 0.17175901\n",
      "iteration:  61 loss: 0.17295450\n",
      "iteration:  62 loss: 0.18809931\n",
      "iteration:  63 loss: 0.72593045\n",
      "iteration:  64 loss: 0.10933977\n",
      "iteration:  65 loss: 1.08828175\n",
      "iteration:  66 loss: 0.15997609\n",
      "iteration:  67 loss: 0.15139295\n",
      "iteration:  68 loss: 0.13482791\n",
      "iteration:  69 loss: 0.36904174\n",
      "iteration:  70 loss: 0.16021711\n",
      "iteration:  71 loss: 0.54877001\n",
      "iteration:  72 loss: 0.38721076\n",
      "iteration:  73 loss: 2.07601881\n",
      "iteration:  74 loss: 0.22626871\n",
      "iteration:  75 loss: 0.29813793\n",
      "iteration:  76 loss: 0.73080713\n",
      "iteration:  77 loss: 0.62199545\n",
      "iteration:  78 loss: 1.56955254\n",
      "iteration:  79 loss: 0.62138462\n",
      "iteration:  80 loss: 1.24867690\n",
      "iteration:  81 loss: 0.11773231\n",
      "iteration:  82 loss: 0.55919009\n",
      "iteration:  83 loss: 0.11887585\n",
      "iteration:  84 loss: 3.09411168\n",
      "iteration:  85 loss: 0.15822648\n",
      "iteration:  86 loss: 0.46172479\n",
      "iteration:  87 loss: 2.88727593\n",
      "iteration:  88 loss: 0.16192636\n",
      "iteration:  89 loss: 2.16971707\n",
      "iteration:  90 loss: 1.94491529\n",
      "iteration:  91 loss: 1.95220613\n",
      "iteration:  92 loss: 0.14899872\n",
      "iteration:  93 loss: 1.20590663\n",
      "iteration:  94 loss: 0.98777217\n",
      "iteration:  95 loss: 0.69739079\n",
      "iteration:  96 loss: 4.13370943\n",
      "iteration:  97 loss: 1.05624521\n",
      "iteration:  98 loss: 0.24153553\n",
      "iteration:  99 loss: 0.12591276\n",
      "iteration: 100 loss: 0.23827378\n",
      "iteration: 101 loss: 0.16785768\n",
      "iteration: 102 loss: 0.54482257\n",
      "iteration: 103 loss: 2.74032736\n",
      "iteration: 104 loss: 0.49582541\n",
      "iteration: 105 loss: 0.55977648\n",
      "iteration: 106 loss: 0.17147869\n",
      "iteration: 107 loss: 0.25461251\n",
      "iteration: 108 loss: 0.80589187\n",
      "iteration: 109 loss: 0.19166040\n",
      "iteration: 110 loss: 1.90438616\n",
      "iteration: 111 loss: 3.08738112\n",
      "iteration: 112 loss: 2.53461480\n",
      "iteration: 113 loss: 0.41591451\n",
      "iteration: 114 loss: 0.89456189\n",
      "iteration: 115 loss: 0.97621709\n",
      "iteration: 116 loss: 2.06241250\n",
      "iteration: 117 loss: 0.12672099\n",
      "iteration: 118 loss: 0.17767972\n",
      "iteration: 119 loss: 0.41271588\n",
      "iteration: 120 loss: 0.15092844\n",
      "iteration: 121 loss: 0.66692048\n",
      "iteration: 122 loss: 0.12835515\n",
      "iteration: 123 loss: 2.20851207\n",
      "iteration: 124 loss: 1.06380033\n",
      "iteration: 125 loss: 3.23024011\n",
      "iteration: 126 loss: 2.82402587\n",
      "iteration: 127 loss: 0.17089611\n",
      "iteration: 128 loss: 1.69948757\n",
      "iteration: 129 loss: 0.15407638\n",
      "iteration: 130 loss: 2.58696032\n",
      "iteration: 131 loss: 2.46983528\n",
      "iteration: 132 loss: 0.27957931\n",
      "iteration: 133 loss: 0.38311380\n",
      "iteration: 134 loss: 0.40201223\n",
      "iteration: 135 loss: 2.06694961\n",
      "iteration: 136 loss: 0.97179681\n",
      "iteration: 137 loss: 0.70034605\n",
      "iteration: 138 loss: 1.21886361\n",
      "iteration: 139 loss: 1.16650558\n",
      "iteration: 140 loss: 0.48107204\n",
      "iteration: 141 loss: 1.08368576\n",
      "iteration: 142 loss: 0.67447788\n",
      "iteration: 143 loss: 0.71825123\n",
      "iteration: 144 loss: 3.49670649\n",
      "iteration: 145 loss: 2.79847240\n",
      "iteration: 146 loss: 0.57210755\n",
      "iteration: 147 loss: 0.89128137\n",
      "iteration: 148 loss: 0.79607773\n",
      "iteration: 149 loss: 0.64791006\n",
      "iteration: 150 loss: 1.39495683\n",
      "iteration: 151 loss: 2.27399206\n",
      "iteration: 152 loss: 2.78959537\n",
      "iteration: 153 loss: 2.93012786\n",
      "iteration: 154 loss: 1.21478474\n",
      "iteration: 155 loss: 2.57937121\n",
      "iteration: 156 loss: 0.18740590\n",
      "iteration: 157 loss: 0.73996425\n",
      "iteration: 158 loss: 0.64172548\n",
      "iteration: 159 loss: 0.41519606\n",
      "iteration: 160 loss: 0.54979175\n",
      "iteration: 161 loss: 1.34108281\n",
      "iteration: 162 loss: 0.22867054\n",
      "iteration: 163 loss: 1.74923086\n",
      "iteration: 164 loss: 3.09737229\n",
      "iteration: 165 loss: 0.24868946\n",
      "iteration: 166 loss: 0.25009093\n",
      "iteration: 167 loss: 0.17049323\n",
      "iteration: 168 loss: 1.47907937\n",
      "iteration: 169 loss: 0.16242193\n",
      "iteration: 170 loss: 2.78959298\n",
      "iteration: 171 loss: 0.66562849\n",
      "iteration: 172 loss: 0.33444625\n",
      "iteration: 173 loss: 0.09802322\n",
      "iteration: 174 loss: 3.48398423\n",
      "iteration: 175 loss: 2.92894721\n",
      "iteration: 176 loss: 0.54223055\n",
      "iteration: 177 loss: 0.50714827\n",
      "iteration: 178 loss: 0.22969367\n",
      "iteration: 179 loss: 0.33831167\n",
      "iteration: 180 loss: 0.81397974\n",
      "iteration: 181 loss: 1.27479279\n",
      "iteration: 182 loss: 0.14943004\n",
      "iteration: 183 loss: 1.20212674\n",
      "iteration: 184 loss: 0.41272479\n",
      "iteration: 185 loss: 1.90225255\n",
      "iteration: 186 loss: 1.66708350\n",
      "iteration: 187 loss: 0.52511132\n",
      "iteration: 188 loss: 3.24766374\n",
      "iteration: 189 loss: 2.15450191\n",
      "iteration: 190 loss: 2.35600281\n",
      "iteration: 191 loss: 0.28084609\n",
      "iteration: 192 loss: 1.05145657\n",
      "iteration: 193 loss: 1.48907745\n",
      "iteration: 194 loss: 0.15363063\n",
      "iteration: 195 loss: 2.52962923\n",
      "iteration: 196 loss: 3.15182614\n",
      "iteration: 197 loss: 0.88015461\n",
      "iteration: 198 loss: 0.75546759\n",
      "iteration: 199 loss: 0.63596034\n",
      "epoch: 100 mean loss training: 1.10421276\n",
      "epoch: 100 mean loss validation: 1.39658272\n"
     ]
    }
   ],
   "source": [
    "#Training the data\n",
    "epochs = 100 #JUMLAH BERAPA KALI DATASET DIGUNAKAN UNTUK TRAIN MODEL\n",
    "batch_size = 10\n",
    "mean_losses_train = []\n",
    "mean_losses_valid = []\n",
    "best_loss_valid = np.inf\n",
    "\n",
    "for i in range(epochs):\n",
    "    aggregated_losses_train = []\n",
    "    aggregated_losses_valid = []\n",
    "    i += 1\n",
    "    for j in range((train_records//batch_size)+1):\n",
    "        start_train = j*batch_size\n",
    "        end_train = start_train+10\n",
    "        train = model(numerical_train_data[start_train:end_train], categorical_train_data[start_train:end_train]) # yg error 1\n",
    "        logits, mlogits = lmcl_loss(train, train_outputs[start_train:end_train])\n",
    "        train_loss = loss_function(mlogits, train_outputs[start_train:end_train])\n",
    "        aggregated_losses_train.append(train_loss)\n",
    "\n",
    "        print(f'iteration: {j:3} loss: {train_loss.item():10.8f}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_lmcl.zero_grad()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_lmcl.step()\n",
    "        \n",
    "        mean_loss_train = torch.mean(torch.stack(aggregated_losses_train))\n",
    "        \n",
    "    print(f'epoch: {i:3} mean loss training: {mean_loss_train.item():10.8f}')\n",
    "    mean_losses_train.append(mean_loss_train)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for k in range((valid_records//batch_size)+1):\n",
    "            start_valid = k*batch_size\n",
    "            end_valid = start_valid+batch_size\n",
    "            valid = model(numerical_valid_data[start_valid:end_valid], categorical_valid_data[start_valid:end_valid]) # kenapa disini harus ada 2 variabel prefix\n",
    "            logits, mlogits = lmcl_loss(valid, valid_outputs[start_valid:end_valid])\n",
    "            valid_loss = loss_function(mlogits, valid_outputs[start_valid:end_valid])\n",
    "            aggregated_losses_valid.append(valid_loss)\n",
    "    mean_loss_valid = torch.mean(torch.stack(aggregated_losses_valid))\n",
    "    print(f'epoch: {i:3} mean loss validation: {mean_loss_valid:.8f}')\n",
    "    if mean_loss_valid.cpu().numpy()[()] < best_loss_valid:\n",
    "        best_loss_valid = mean_loss_valid\n",
    "        torch.save(model.state_dict(), \"model_train_lmcl.pth\")\n",
    "        torch.save(lmcl_loss.state_dict(), \"lmcloss.pth\")\n",
    "        \n",
    "    \n",
    "    mean_losses_valid.append(mean_loss_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_train_lmcl.pth\"))\n",
    "lmcl_loss.load_state_dict(torch.load(\"lmcloss.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.34115207\n"
     ]
    }
   ],
   "source": [
    "#Creating predictions\n",
    "with torch.no_grad():\n",
    "    valid = model(numerical_valid_data, categorical_valid_data)\n",
    "    logits, mlogits = lmcl_loss(valid, valid_outputs)\n",
    "    valid_loss = loss_function(mlogits, valid_outputs)\n",
    "    total_valid_loss = valid_loss\n",
    "print(f'Loss: {total_valid_loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[485  79]\n",
      " [ 23  79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90       564\n",
      "           1       0.50      0.77      0.61       102\n",
      "\n",
      "    accuracy                           0.85       666\n",
      "   macro avg       0.73      0.82      0.76       666\n",
      "weighted avg       0.89      0.85      0.86       666\n",
      "\n",
      "Accuracy:  0.8468468468468469\n",
      "F1 Score:  0.7562715269804823\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================================\n",
    "# [26 Mei 2020]: \n",
    "# hasil utama yang dilihat adalah F1 Score, karena metrik akurasi misleading digunakan untuk \n",
    "# data imbalance\n",
    "# =============================================================================================\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "valid_val = np.argmax(logits.data, axis=1)\n",
    "print(confusion_matrix(valid_outputs, valid_val))\n",
    "print(classification_report(valid_outputs, valid_val))\n",
    "print(\"Accuracy: \", accuracy_score(valid_outputs, valid_val))\n",
    "print(\"F1 Score: \", f1_score(valid_outputs, valid_val, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(2.3558, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1,epochs+1), mean_losses_train, label = 'train')\n",
    "plt.plot(range(1,epochs+1), mean_losses_valid, label = 'validation')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('loss graph')\n",
    "plt.show()\n",
    "max_y = max(mean_losses_train)  # Find the maximum y value\n",
    "max_x = range(1,epochs+1)[mean_losses_train.index(max_y)]  # Find the x value corresponding to the maximum y value\n",
    "print(max_x, max_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
